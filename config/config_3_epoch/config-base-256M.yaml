Train:
  quantization: True # True for QLoRA, relevant for training only
  lora: True # Use True for lora, False for full tuning
  r: 8
  lora_alpha: 8
  lora_dropout: 0.1
  target_modules: ["down_proj", "o_proj", "k_proj", "q_proj", "gate_proj", "up_proj", "v_proj"]
  max_seq_length: 1024
  per_device_train_batch_size: 4 # Fix this
  gradient_accumulation_steps: 4  # Fix this
  optim: "adamw_torch_fused" # Fix this
  num_train_epochs: 3
  learning_rate: 0.0001
  weight_decay: 0.01
  warmup_ratio: 0.1
  lr_scheduler_type: "cosine"
  logging_steps: 25
  save_strategy: "epoch"

Model:
  model_type: "HuggingFaceTB/SmolVLM-256M-Base" # Huggingface model ID
  output_dir: "molvlm-orig-base-256M"

Data:
  data_path: "./data/" # location of data pickle files
  data_type: "long" # "long", "middle", "short", "other"