Train:
  quantization: True # True for QLoRA, relevant for training only
  lora: True # Use True for lora, False for full tuning
  r: 512 # default:8 LoRA config
  lora_alpha: 512 # default:8 LoRA config
  lora_dropout: 0.1 # default:0.1 LoRA config
  target_modules: ["q_proj","k_proj","v_proj","o_proj","gate_proj", "up_proj","down_proj","lm_head"] # default:["down_proj", "o_proj", "k_proj", "q_proj", "gate_proj", "up_proj", "v_proj"] LoRA config
  max_seq_length: 2048 # Fix this
  per_device_train_batch_size: 4 # Fix this
  gradient_accumulation_steps: 4  # Fix this
  optim: "adamw_torch_fused" # Fix this
  num_train_epochs: 5
  learning_rate: 0.0001
  weight_decay: 0.01
  warmup_ratio: 0.1
  lr_scheduler_type: "cosine"
  logging_steps: 25
  save_strategy: "epoch"

Model:
  model_type: "HuggingFaceTB/SmolVLM-256M-Instruct" # Huggingface model ID
  output_dir: "256M-instruct-comp-ti-5epoch-17"

Data:
  data_path: "./data/" # location of data pickle files
  data_type: "long" # "long", "middle", "short", "other"