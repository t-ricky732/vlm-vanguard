---------------------------------------
Begin Slurm Prolog: Apr-27-2025 06:14:48
Job ID:    2539200
User ID:   ryoshida7
Account:   coc
Job name:  256M-instruct-comp-ti-5epoch-45
Partition: coc-gpu
QOS:       coc-ice
---------------------------------------
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
r 8
lora_alpha 256
lora_dropout 0.1
target_modules ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj', 'lm_head']
  0%|          | 0/245 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.
  0%|          | 1/245 [00:10<44:02, 10.83s/it]  1%|          | 2/245 [00:19<37:58,  9.38s/it]  1%|          | 3/245 [00:27<36:06,  8.95s/it]  2%|▏         | 4/245 [00:36<35:18,  8.79s/it]  2%|▏         | 5/245 [00:44<34:46,  8.69s/it]  2%|▏         | 6/245 [00:53<34:36,  8.69s/it]  3%|▎         | 7/245 [01:01<33:59,  8.57s/it]  3%|▎         | 8/245 [01:10<33:46,  8.55s/it]  4%|▎         | 9/245 [01:18<33:32,  8.53s/it]  4%|▍         | 10/245 [01:27<33:20,  8.51s/it]  4%|▍         | 11/245 [01:35<33:02,  8.47s/it]  5%|▍         | 12/245 [01:44<33:03,  8.51s/it]  5%|▌         | 13/245 [01:52<32:54,  8.51s/it]  6%|▌         | 14/245 [02:01<32:46,  8.51s/it]  6%|▌         | 15/245 [02:09<32:32,  8.49s/it]  7%|▋         | 16/245 [02:18<32:22,  8.48s/it]  7%|▋         | 17/245 [02:26<32:15,  8.49s/it]  7%|▋         | 18/245 [02:35<32:32,  8.60s/it]  8%|▊         | 19/245 [02:44<32:23,  8.60s/it]  8%|▊         | 20/245 [02:52<32:17,  8.61s/it]  9%|▊         | 21/245 [03:01<31:58,  8.57s/it]  9%|▉         | 22/245 [03:09<31:47,  8.56s/it]  9%|▉         | 23/245 [03:18<31:33,  8.53s/it] 10%|▉         | 24/245 [03:26<31:25,  8.53s/it] 10%|█         | 25/245 [03:35<31:08,  8.49s/it]                                                 10%|█         | 25/245 [03:35<31:08,  8.49s/it] 11%|█         | 26/245 [03:43<31:00,  8.49s/it] 11%|█         | 27/245 [03:51<30:44,  8.46s/it] 11%|█▏        | 28/245 [04:00<30:51,  8.53s/it] 12%|█▏        | 29/245 [04:09<30:48,  8.56s/it] 12%|█▏        | 30/245 [04:17<30:24,  8.49s/it] 13%|█▎        | 31/245 [04:25<30:03,  8.43s/it] 13%|█▎        | 32/245 [04:34<30:08,  8.49s/it] 13%|█▎        | 33/245 [04:42<29:53,  8.46s/it] 14%|█▍        | 34/245 [04:51<29:57,  8.52s/it] 14%|█▍        | 35/245 [05:00<29:54,  8.55s/it] 15%|█▍        | 36/245 [05:08<29:38,  8.51s/it] 15%|█▌        | 37/245 [05:17<29:39,  8.55s/it] 16%|█▌        | 38/245 [05:25<29:24,  8.52s/it] 16%|█▌        | 39/245 [05:34<29:20,  8.55s/it] 16%|█▋        | 40/245 [05:42<29:15,  8.56s/it] 17%|█▋        | 41/245 [05:51<29:20,  8.63s/it] 17%|█▋        | 42/245 [06:00<29:04,  8.60s/it] 18%|█▊        | 43/245 [06:08<28:46,  8.55s/it] 18%|█▊        | 44/245 [06:17<28:29,  8.51s/it] 18%|█▊        | 45/245 [06:25<28:31,  8.56s/it] 19%|█▉        | 46/245 [06:34<28:21,  8.55s/it] 19%|█▉        | 47/245 [06:42<28:23,  8.60s/it] 20%|█▉        | 48/245 [06:51<28:22,  8.64s/it] 20%|██        | 49/245 [06:59<27:00,  8.27s/it]{'loss': 1.9761, 'grad_norm': 3.374769926071167, 'learning_rate': 9.6e-05, 'num_tokens': 402507.0, 'mean_token_accuracy': 0.6325247538089752, 'epoch': 0.51}

  0%|          | 0/25 [00:00<?, ?it/s][A
  8%|▊         | 2/25 [00:02<00:31,  1.35s/it][A
 12%|█▏        | 3/25 [00:05<00:40,  1.84s/it][A
 16%|█▌        | 4/25 [00:08<00:47,  2.25s/it][A
 20%|██        | 5/25 [00:10<00:47,  2.38s/it][A
 24%|██▍       | 6/25 [00:13<00:47,  2.49s/it][A
 28%|██▊       | 7/25 [00:16<00:45,  2.54s/it][A
 32%|███▏      | 8/25 [00:18<00:43,  2.56s/it][A
 36%|███▌      | 9/25 [00:21<00:41,  2.57s/it][A
 40%|████      | 10/25 [00:24<00:39,  2.63s/it][A
 44%|████▍     | 11/25 [00:26<00:37,  2.65s/it][A
 48%|████▊     | 12/25 [00:29<00:34,  2.65s/it][A
 52%|█████▏    | 13/25 [00:32<00:31,  2.62s/it][A
 56%|█████▌    | 14/25 [00:34<00:28,  2.61s/it][A
 60%|██████    | 15/25 [00:38<00:29,  3.00s/it][A
 64%|██████▍   | 16/25 [00:41<00:26,  2.90s/it][A
 68%|██████▊   | 17/25 [00:43<00:22,  2.85s/it][A
 72%|███████▏  | 18/25 [00:46<00:20,  2.88s/it][A
 76%|███████▌  | 19/25 [00:49<00:16,  2.82s/it][A
 80%|████████  | 20/25 [00:52<00:13,  2.80s/it][A
 84%|████████▍ | 21/25 [00:54<00:11,  2.75s/it][A
 88%|████████▊ | 22/25 [00:57<00:08,  2.73s/it][A
 92%|█████████▏| 23/25 [01:00<00:05,  2.69s/it][A
 96%|█████████▌| 24/25 [01:01<00:02,  2.37s/it][A
100%|██████████| 25/25 [01:02<00:00,  1.75s/it][A                                                
                                               [A 20%|██        | 49/245 [08:06<27:00,  8.27s/it]
100%|██████████| 25/25 [01:02<00:00,  1.75s/it][A
                                               [A/home/hice1/ryoshida7/scratch/py10_vlm/lib/python3.10/site-packages/peft/utils/save_and_load.py:220: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.
  warnings.warn("Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.")
 20%|██        | 50/245 [08:16<1:34:35, 29.10s/it]                                                   20%|██        | 50/245 [08:16<1:34:35, 29.10s/it] 21%|██        | 51/245 [08:25<1:14:09, 22.94s/it] 21%|██        | 52/245 [08:33<59:47, 18.59s/it]   22%|██▏       | 53/245 [08:42<49:59, 15.62s/it] 22%|██▏       | 54/245 [08:51<43:06, 13.54s/it] 22%|██▏       | 55/245 [08:59<38:02, 12.02s/it] 23%|██▎       | 56/245 [09:08<34:51, 11.06s/it] 23%|██▎       | 57/245 [09:17<32:25, 10.35s/it] 24%|██▎       | 58/245 [09:25<30:44,  9.86s/it] 24%|██▍       | 59/245 [09:34<29:09,  9.41s/it] 24%|██▍       | 60/245 [09:43<28:23,  9.21s/it] 25%|██▍       | 61/245 [09:51<27:31,  8.97s/it] 25%|██▌       | 62/245 [10:00<27:10,  8.91s/it] 26%|██▌       | 63/245 [10:08<26:42,  8.80s/it] 26%|██▌       | 64/245 [10:17<26:31,  8.79s/it] 27%|██▋       | 65/245 [10:26<26:21,  8.79s/it] 27%|██▋       | 66/245 [10:34<25:54,  8.68s/it] 27%|██▋       | 67/245 [10:43<25:37,  8.64s/it] 28%|██▊       | 68/245 [10:51<25:24,  8.62s/it] 28%|██▊       | 69/245 [11:00<25:14,  8.61s/it] 29%|██▊       | 70/245 [11:08<25:00,  8.57s/it] 29%|██▉       | 71/245 [11:17<24:55,  8.59s/it] 29%|██▉       | 72/245 [11:26<24:46,  8.59s/it] 30%|██▉       | 73/245 [11:34<24:44,  8.63s/it] 30%|███       | 74/245 [11:43<24:24,  8.56s/it] 31%|███       | 75/245 [11:51<24:20,  8.59s/it]                                                 31%|███       | 75/245 [11:51<24:20,  8.59s/it] 31%|███       | 76/245 [12:00<24:23,  8.66s/it] 31%|███▏      | 77/245 [12:09<24:06,  8.61s/it] 32%|███▏      | 78/245 [12:17<23:57,  8.61s/it] 32%|███▏      | 79/245 [12:26<23:38,  8.55s/it] 33%|███▎      | 80/245 [12:34<23:27,  8.53s/it] 33%|███▎      | 81/245 [12:43<23:21,  8.55s/it] 33%|███▎      | 82/245 [12:51<23:05,  8.50s/it] 34%|███▍      | 83/245 [13:00<23:01,  8.53s/it] 34%|███▍      | 84/245 [13:08<22:48,  8.50s/it] 35%|███▍      | 85/245 [13:17<22:32,  8.45s/it] 35%|███▌      | 86/245 [13:25<22:25,  8.46s/it] 36%|███▌      | 87/245 [13:34<22:16,  8.46s/it] 36%|███▌      | 88/245 [13:42<22:21,  8.54s/it] 36%|███▋      | 89/245 [13:51<22:13,  8.55s/it] 37%|███▋      | 90/245 [14:00<22:25,  8.68s/it] 37%|███▋      | 91/245 [14:09<22:20,  8.70s/it] 38%|███▊      | 92/245 [14:17<22:09,  8.69s/it] 38%|███▊      | 93/245 [14:26<21:54,  8.65s/it] 38%|███▊      | 94/245 [14:34<21:48,  8.66s/it] 39%|███▉      | 95/245 [14:43<21:39,  8.66s/it] 39%|███▉      | 96/245 [14:52<21:26,  8.63s/it] 40%|███▉      | 97/245 [15:00<21:09,  8.58s/it] 40%|████      | 98/245 [15:08<20:18,  8.29s/it]{'eval_loss': 0.21529033780097961, 'eval_runtime': 66.9814, 'eval_samples_per_second': 2.926, 'eval_steps_per_second': 0.373, 'eval_num_tokens': 790263.0, 'eval_mean_token_accuracy': 0.9469871830940246, 'epoch': 1.0}
{'loss': 0.2764, 'grad_norm': 2.0637471675872803, 'learning_rate': 9.709221818197624e-05, 'num_tokens': 806179.0, 'mean_token_accuracy': 0.9438922548294068, 'epoch': 1.02}
{'loss': 0.1704, 'grad_norm': 1.8146361112594604, 'learning_rate': 8.825117959999116e-05, 'num_tokens': 1211058.0, 'mean_token_accuracy': 0.9557362145185471, 'epoch': 1.53}

  0%|          | 0/25 [00:00<?, ?it/s][A
  8%|▊         | 2/25 [00:02<00:31,  1.36s/it][A
 12%|█▏        | 3/25 [00:05<00:40,  1.86s/it][A
 16%|█▌        | 4/25 [00:08<00:46,  2.22s/it][A
 20%|██        | 5/25 [00:10<00:47,  2.37s/it][A
 24%|██▍       | 6/25 [00:13<00:47,  2.49s/it][A
 28%|██▊       | 7/25 [00:16<00:45,  2.54s/it][A
 32%|███▏      | 8/25 [00:18<00:43,  2.57s/it][A
 36%|███▌      | 9/25 [00:21<00:41,  2.60s/it][A
 40%|████      | 10/25 [00:24<00:39,  2.65s/it][A
 44%|████▍     | 11/25 [00:26<00:37,  2.68s/it][A
 48%|████▊     | 12/25 [00:29<00:34,  2.67s/it][A
 52%|█████▏    | 13/25 [00:32<00:31,  2.64s/it][A
 56%|█████▌    | 14/25 [00:34<00:28,  2.63s/it][A
 60%|██████    | 15/25 [00:37<00:26,  2.64s/it][A
 64%|██████▍   | 16/25 [00:40<00:23,  2.64s/it][A
 68%|██████▊   | 17/25 [00:42<00:21,  2.68s/it][A
 72%|███████▏  | 18/25 [00:45<00:19,  2.74s/it][A
 76%|███████▌  | 19/25 [00:48<00:16,  2.72s/it][A
 80%|████████  | 20/25 [00:51<00:13,  2.73s/it][A
 84%|████████▍ | 21/25 [00:53<00:10,  2.71s/it][A
 88%|████████▊ | 22/25 [00:56<00:08,  2.70s/it][A
 92%|█████████▏| 23/25 [00:59<00:05,  2.66s/it][A
 96%|█████████▌| 24/25 [01:00<00:02,  2.35s/it][A
100%|██████████| 25/25 [01:01<00:00,  1.74s/it][A                                                
                                               [A 40%|████      | 98/245 [16:14<20:18,  8.29s/it]
100%|██████████| 25/25 [01:01<00:00,  1.74s/it][A
                                               [A 40%|████      | 99/245 [16:23<1:09:20, 28.49s/it] 41%|████      | 100/245 [16:32<54:12, 22.43s/it]                                                   41%|████      | 100/245 [16:32<54:12, 22.43s/it] 41%|████      | 101/245 [16:40<43:55, 18.30s/it] 42%|████▏     | 102/245 [16:49<36:35, 15.36s/it] 42%|████▏     | 103/245 [16:57<31:31, 13.32s/it] 42%|████▏     | 104/245 [17:06<27:59, 11.91s/it] 43%|████▎     | 105/245 [17:15<25:25, 10.89s/it] 43%|████▎     | 106/245 [17:23<23:38, 10.20s/it] 44%|████▎     | 107/245 [17:32<22:19,  9.70s/it] 44%|████▍     | 108/245 [17:40<21:23,  9.37s/it] 44%|████▍     | 109/245 [17:49<20:47,  9.17s/it] 45%|████▍     | 110/245 [17:58<20:12,  8.98s/it] 45%|████▌     | 111/245 [18:06<19:48,  8.87s/it] 46%|████▌     | 112/245 [18:15<19:26,  8.77s/it] 46%|████▌     | 113/245 [18:23<19:03,  8.66s/it] 47%|████▋     | 114/245 [18:32<19:03,  8.73s/it] 47%|████▋     | 115/245 [18:41<18:49,  8.69s/it] 47%|████▋     | 116/245 [18:49<18:45,  8.73s/it] 48%|████▊     | 117/245 [18:58<18:28,  8.66s/it] 48%|████▊     | 118/245 [19:06<18:10,  8.59s/it] 49%|████▊     | 119/245 [19:15<18:04,  8.61s/it] 49%|████▉     | 120/245 [19:24<17:56,  8.61s/it] 49%|████▉     | 121/245 [19:32<17:41,  8.56s/it] 50%|████▉     | 122/245 [19:41<17:46,  8.67s/it] 50%|█████     | 123/245 [19:50<17:35,  8.65s/it] 51%|█████     | 124/245 [19:58<17:26,  8.65s/it] 51%|█████     | 125/245 [20:07<17:14,  8.62s/it]                                                  51%|█████     | 125/245 [20:07<17:14,  8.62s/it] 51%|█████▏    | 126/245 [20:16<17:13,  8.68s/it] 52%|█████▏    | 127/245 [20:24<17:02,  8.67s/it] 52%|█████▏    | 128/245 [20:33<16:55,  8.68s/it] 53%|█████▎    | 129/245 [20:41<16:36,  8.59s/it] 53%|█████▎    | 130/245 [20:50<16:31,  8.62s/it] 53%|█████▎    | 131/245 [20:58<16:16,  8.57s/it] 54%|█████▍    | 132/245 [21:07<16:16,  8.64s/it] 54%|█████▍    | 133/245 [21:16<15:59,  8.57s/it] 55%|█████▍    | 134/245 [21:24<15:49,  8.55s/it] 55%|█████▌    | 135/245 [21:33<15:47,  8.61s/it] 56%|█████▌    | 136/245 [21:42<15:40,  8.63s/it] 56%|█████▌    | 137/245 [21:50<15:32,  8.64s/it] 56%|█████▋    | 138/245 [21:59<15:22,  8.62s/it] 57%|█████▋    | 139/245 [22:07<15:14,  8.63s/it] 57%|█████▋    | 140/245 [22:16<15:11,  8.68s/it] 58%|█████▊    | 141/245 [22:25<15:01,  8.67s/it] 58%|█████▊    | 142/245 [22:34<14:55,  8.70s/it] 58%|█████▊    | 143/245 [22:42<14:41,  8.64s/it] 59%|█████▉    | 144/245 [22:51<14:28,  8.60s/it] 59%|█████▉    | 145/245 [22:59<14:20,  8.60s/it] 60%|█████▉    | 146/245 [23:08<14:18,  8.67s/it] 60%|██████    | 147/245 [23:16<13:38,  8.35s/it]{'eval_loss': 0.1723308116197586, 'eval_runtime': 65.7449, 'eval_samples_per_second': 2.981, 'eval_steps_per_second': 0.38, 'eval_num_tokens': 1580526.0, 'eval_mean_token_accuracy': 0.9573317623138428, 'epoch': 2.0}
{'loss': 0.1547, 'grad_norm': 1.4701868295669556, 'learning_rate': 7.45866462322802e-05, 'num_tokens': 1612624.0, 'mean_token_accuracy': 0.958586436510086, 'epoch': 2.04}
{'loss': 0.124, 'grad_norm': 1.7777053117752075, 'learning_rate': 5.782172325201155e-05, 'num_tokens': 2015349.0, 'mean_token_accuracy': 0.9659706526994705, 'epoch': 2.55}

  0%|          | 0/25 [00:00<?, ?it/s][A
  8%|▊         | 2/25 [00:02<00:31,  1.38s/it][A
 12%|█▏        | 3/25 [00:05<00:41,  1.87s/it][A
 16%|█▌        | 4/25 [00:08<00:46,  2.23s/it][A
 20%|██        | 5/25 [00:10<00:47,  2.39s/it][A
 24%|██▍       | 6/25 [00:13<00:47,  2.52s/it][A
 28%|██▊       | 7/25 [00:16<00:46,  2.57s/it][A
 32%|███▏      | 8/25 [00:18<00:44,  2.59s/it][A
 36%|███▌      | 9/25 [00:21<00:41,  2.61s/it][A
 40%|████      | 10/25 [00:24<00:39,  2.66s/it][A
 44%|████▍     | 11/25 [00:27<00:37,  2.69s/it][A
 48%|████▊     | 12/25 [00:29<00:34,  2.68s/it][A
 52%|█████▏    | 13/25 [00:32<00:31,  2.66s/it][A
 56%|█████▌    | 14/25 [00:34<00:29,  2.64s/it][A
 60%|██████    | 15/25 [00:37<00:26,  2.66s/it][A
 64%|██████▍   | 16/25 [00:40<00:23,  2.66s/it][A
 68%|██████▊   | 17/25 [00:43<00:21,  2.70s/it][A
 72%|███████▏  | 18/25 [00:46<00:19,  2.76s/it][A
 76%|███████▌  | 19/25 [00:48<00:16,  2.74s/it][A
 80%|████████  | 20/25 [00:51<00:13,  2.75s/it][A
 84%|████████▍ | 21/25 [00:54<00:10,  2.72s/it][A
 88%|████████▊ | 22/25 [00:56<00:08,  2.72s/it][A
 92%|█████████▏| 23/25 [00:59<00:05,  2.68s/it][A
 96%|█████████▌| 24/25 [01:01<00:02,  2.36s/it][A
100%|██████████| 25/25 [01:01<00:00,  1.75s/it][A                                                 
                                               [A 60%|██████    | 147/245 [24:22<13:38,  8.35s/it]
100%|██████████| 25/25 [01:01<00:00,  1.75s/it][A
                                               [A 60%|██████    | 148/245 [24:32<46:21, 28.67s/it] 61%|██████    | 149/245 [24:40<36:14, 22.65s/it] 61%|██████    | 150/245 [24:49<29:16, 18.49s/it]                                                  61%|██████    | 150/245 [24:49<29:16, 18.49s/it] 62%|██████▏   | 151/245 [24:58<24:17, 15.50s/it] 62%|██████▏   | 152/245 [25:06<20:51, 13.46s/it] 62%|██████▏   | 153/245 [25:15<18:23, 11.99s/it] 63%|██████▎   | 154/245 [25:23<16:35, 10.94s/it] 63%|██████▎   | 155/245 [25:32<15:26, 10.29s/it] 64%|██████▎   | 156/245 [25:41<14:31,  9.79s/it] 64%|██████▍   | 157/245 [25:50<13:55,  9.50s/it] 64%|██████▍   | 158/245 [26:04<16:02, 11.06s/it] 65%|██████▍   | 159/245 [26:13<14:47, 10.32s/it] 65%|██████▌   | 160/245 [26:22<13:52,  9.80s/it] 66%|██████▌   | 161/245 [26:30<13:07,  9.37s/it] 66%|██████▌   | 162/245 [26:39<12:42,  9.18s/it] 67%|██████▋   | 163/245 [26:47<12:20,  9.03s/it] 67%|██████▋   | 164/245 [26:56<11:55,  8.84s/it] 67%|██████▋   | 165/245 [27:04<11:41,  8.77s/it] 68%|██████▊   | 166/245 [27:13<11:25,  8.68s/it] 68%|██████▊   | 167/245 [27:21<11:10,  8.59s/it] 69%|██████▊   | 168/245 [27:30<11:06,  8.66s/it] 69%|██████▉   | 169/245 [27:38<10:53,  8.60s/it] 69%|██████▉   | 170/245 [27:47<10:47,  8.63s/it] 70%|██████▉   | 171/245 [27:56<10:41,  8.67s/it] 70%|███████   | 172/245 [28:04<10:25,  8.57s/it] 71%|███████   | 173/245 [28:13<10:19,  8.61s/it] 71%|███████   | 174/245 [28:22<10:12,  8.63s/it] 71%|███████▏  | 175/245 [28:30<10:05,  8.64s/it]                                                  71%|███████▏  | 175/245 [28:30<10:05,  8.64s/it] 72%|███████▏  | 176/245 [28:39<09:55,  8.63s/it] 72%|███████▏  | 177/245 [28:47<09:44,  8.60s/it] 73%|███████▎  | 178/245 [28:56<09:35,  8.59s/it] 73%|███████▎  | 179/245 [29:05<09:29,  8.63s/it] 73%|███████▎  | 180/245 [29:13<09:20,  8.62s/it] 74%|███████▍  | 181/245 [29:22<09:15,  8.68s/it] 74%|███████▍  | 182/245 [29:31<09:03,  8.63s/it] 75%|███████▍  | 183/245 [29:39<08:58,  8.68s/it] 75%|███████▌  | 184/245 [29:48<08:46,  8.62s/it] 76%|███████▌  | 185/245 [29:56<08:35,  8.59s/it] 76%|███████▌  | 186/245 [30:05<08:23,  8.54s/it] 76%|███████▋  | 187/245 [30:14<08:17,  8.57s/it] 77%|███████▋  | 188/245 [30:22<08:06,  8.54s/it] 77%|███████▋  | 189/245 [30:31<07:59,  8.57s/it] 78%|███████▊  | 190/245 [30:39<07:52,  8.60s/it] 78%|███████▊  | 191/245 [30:48<07:46,  8.64s/it] 78%|███████▊  | 192/245 [30:56<07:35,  8.59s/it] 79%|███████▉  | 193/245 [31:05<07:27,  8.60s/it] 79%|███████▉  | 194/245 [31:13<07:15,  8.54s/it] 80%|███████▉  | 195/245 [31:22<07:05,  8.51s/it] 80%|████████  | 196/245 [31:29<06:40,  8.16s/it]{'eval_loss': 0.17180252075195312, 'eval_runtime': 66.1946, 'eval_samples_per_second': 2.961, 'eval_steps_per_second': 0.378, 'eval_num_tokens': 2370789.0, 'eval_mean_token_accuracy': 0.9576327276229858, 'epoch': 3.0}
{'loss': 0.1221, 'grad_norm': 1.4708726406097412, 'learning_rate': 4.007047666771274e-05, 'num_tokens': 2419236.0, 'mean_token_accuracy': 0.9649372535943985, 'epoch': 3.06}
{'loss': 0.1038, 'grad_norm': 2.113333225250244, 'learning_rate': 2.3571348436857904e-05, 'num_tokens': 2823361.0, 'mean_token_accuracy': 0.9696773320436478, 'epoch': 3.57}

  0%|          | 0/25 [00:00<?, ?it/s][A
  8%|▊         | 2/25 [00:02<00:31,  1.36s/it][A
 12%|█▏        | 3/25 [00:05<00:40,  1.85s/it][A
 16%|█▌        | 4/25 [00:08<00:46,  2.21s/it][A
 20%|██        | 5/25 [00:10<00:47,  2.37s/it][A
 24%|██▍       | 6/25 [00:13<00:47,  2.49s/it][A
 28%|██▊       | 7/25 [00:16<00:45,  2.54s/it][A
 32%|███▏      | 8/25 [00:18<00:43,  2.57s/it][A
 36%|███▌      | 9/25 [00:21<00:41,  2.58s/it][A
 40%|████      | 10/25 [00:24<00:39,  2.64s/it][A
 44%|████▍     | 11/25 [00:26<00:37,  2.67s/it][A
 48%|████▊     | 12/25 [00:29<00:34,  2.67s/it][A
 52%|█████▏    | 13/25 [00:32<00:31,  2.64s/it][A
 56%|█████▌    | 14/25 [00:34<00:28,  2.63s/it][A
 60%|██████    | 15/25 [00:37<00:26,  2.64s/it][A
 64%|██████▍   | 16/25 [00:40<00:23,  2.65s/it][A
 68%|██████▊   | 17/25 [00:42<00:21,  2.68s/it][A
 72%|███████▏  | 18/25 [00:45<00:19,  2.74s/it][A
 76%|███████▌  | 19/25 [00:48<00:16,  2.72s/it][A
 80%|████████  | 20/25 [00:51<00:13,  2.73s/it][A
 84%|████████▍ | 21/25 [00:53<00:10,  2.71s/it][A
 88%|████████▊ | 22/25 [00:56<00:08,  2.70s/it][A
 92%|█████████▏| 23/25 [00:59<00:05,  2.67s/it][A
 96%|█████████▌| 24/25 [01:00<00:02,  2.35s/it][A
100%|██████████| 25/25 [01:00<00:00,  1.74s/it][A                                                 
                                               [A 80%|████████  | 196/245 [32:35<06:40,  8.16s/it]
100%|██████████| 25/25 [01:00<00:00,  1.74s/it][A
                                               [A 80%|████████  | 197/245 [32:45<22:48, 28.51s/it] 81%|████████  | 198/245 [32:54<17:37, 22.50s/it] 81%|████████  | 199/245 [33:02<14:00, 18.27s/it] 82%|████████▏ | 200/245 [33:11<11:34, 15.44s/it]                                                  82%|████████▏ | 200/245 [33:11<11:34, 15.44s/it] 82%|████████▏ | 201/245 [33:19<09:46, 13.32s/it] 82%|████████▏ | 202/245 [33:28<08:32, 11.91s/it] 83%|████████▎ | 203/245 [33:37<07:39, 10.95s/it] 83%|████████▎ | 204/245 [33:45<06:59, 10.22s/it] 84%|████████▎ | 205/245 [33:54<06:31,  9.79s/it] 84%|████████▍ | 206/245 [34:03<06:07,  9.42s/it] 84%|████████▍ | 207/245 [34:11<05:46,  9.12s/it] 85%|████████▍ | 208/245 [34:20<05:33,  9.01s/it] 85%|████████▌ | 209/245 [34:28<05:19,  8.89s/it] 86%|████████▌ | 210/245 [34:37<05:06,  8.76s/it] 86%|████████▌ | 211/245 [34:45<04:55,  8.69s/it] 87%|████████▋ | 212/245 [34:54<04:43,  8.59s/it] 87%|████████▋ | 213/245 [35:02<04:35,  8.61s/it] 87%|████████▋ | 214/245 [35:11<04:25,  8.57s/it] 88%|████████▊ | 215/245 [35:20<04:18,  8.61s/it] 88%|████████▊ | 216/245 [35:28<04:11,  8.67s/it] 89%|████████▊ | 217/245 [35:37<04:01,  8.63s/it] 89%|████████▉ | 218/245 [35:45<03:52,  8.60s/it] 89%|████████▉ | 219/245 [35:54<03:42,  8.56s/it] 90%|████████▉ | 220/245 [36:02<03:32,  8.49s/it] 90%|█████████ | 221/245 [36:11<03:25,  8.55s/it] 91%|█████████ | 222/245 [36:19<03:15,  8.50s/it] 91%|█████████ | 223/245 [36:28<03:06,  8.48s/it] 91%|█████████▏| 224/245 [36:36<02:58,  8.49s/it] 92%|█████████▏| 225/245 [36:45<02:49,  8.46s/it]                                                  92%|█████████▏| 225/245 [36:45<02:49,  8.46s/it] 92%|█████████▏| 226/245 [36:53<02:41,  8.48s/it] 93%|█████████▎| 227/245 [37:02<02:32,  8.48s/it] 93%|█████████▎| 228/245 [37:10<02:25,  8.55s/it] 93%|█████████▎| 229/245 [37:19<02:18,  8.65s/it] 94%|█████████▍| 230/245 [37:28<02:08,  8.57s/it] 94%|█████████▍| 231/245 [37:36<01:59,  8.56s/it] 95%|█████████▍| 232/245 [37:45<01:51,  8.61s/it] 95%|█████████▌| 233/245 [37:53<01:42,  8.58s/it] 96%|█████████▌| 234/245 [38:02<01:33,  8.53s/it] 96%|█████████▌| 235/245 [38:10<01:25,  8.53s/it] 96%|█████████▋| 236/245 [38:19<01:17,  8.58s/it] 97%|█████████▋| 237/245 [38:28<01:08,  8.56s/it] 97%|█████████▋| 238/245 [38:36<01:00,  8.60s/it] 98%|█████████▊| 239/245 [38:45<00:51,  8.54s/it] 98%|█████████▊| 240/245 [38:53<00:43,  8.61s/it] 98%|█████████▊| 241/245 [39:02<00:34,  8.63s/it] 99%|█████████▉| 242/245 [39:11<00:26,  8.67s/it] 99%|█████████▉| 243/245 [39:19<00:17,  8.67s/it]100%|█████████▉| 244/245 [39:28<00:08,  8.66s/it]100%|██████████| 245/245 [39:36<00:00,  8.30s/it]{'eval_loss': 0.17721520364284515, 'eval_runtime': 65.7546, 'eval_samples_per_second': 2.981, 'eval_steps_per_second': 0.38, 'eval_num_tokens': 3161052.0, 'eval_mean_token_accuracy': 0.9573995471000671, 'epoch': 4.0}
{'loss': 0.0923, 'grad_norm': 1.3617806434631348, 'learning_rate': 1.0404887703886251e-05, 'num_tokens': 3225748.0, 'mean_token_accuracy': 0.972602728009224, 'epoch': 4.08}
{'loss': 0.0826, 'grad_norm': 1.6227940320968628, 'learning_rate': 2.2313924087851656e-06, 'num_tokens': 3627880.0, 'mean_token_accuracy': 0.9760012716054917, 'epoch': 4.59}

  0%|          | 0/25 [00:00<?, ?it/s][A
  8%|▊         | 2/25 [00:02<00:31,  1.37s/it][A
 12%|█▏        | 3/25 [00:05<00:40,  1.86s/it][A
 16%|█▌        | 4/25 [00:08<00:46,  2.22s/it][A
 20%|██        | 5/25 [00:10<00:47,  2.38s/it][A
 24%|██▍       | 6/25 [00:13<00:47,  2.50s/it][A
 28%|██▊       | 7/25 [00:16<00:45,  2.55s/it][A
 32%|███▏      | 8/25 [00:18<00:43,  2.58s/it][A
 36%|███▌      | 9/25 [00:21<00:41,  2.60s/it][A
 40%|████      | 10/25 [00:24<00:39,  2.66s/it][A
 44%|████▍     | 11/25 [00:27<00:37,  2.69s/it][A
 48%|████▊     | 12/25 [00:29<00:34,  2.68s/it][A
 52%|█████▏    | 13/25 [00:32<00:31,  2.65s/it][A
 56%|█████▌    | 14/25 [00:34<00:29,  2.64s/it][A
 60%|██████    | 15/25 [00:37<00:26,  2.65s/it][A
 64%|██████▍   | 16/25 [00:40<00:23,  2.65s/it][A
 68%|██████▊   | 17/25 [00:42<00:21,  2.68s/it][A
 72%|███████▏  | 18/25 [00:45<00:19,  2.75s/it][A
 76%|███████▌  | 19/25 [00:48<00:16,  2.73s/it][A
 80%|████████  | 20/25 [00:51<00:13,  2.74s/it][A
 84%|████████▍ | 21/25 [00:54<00:10,  2.72s/it][A
 88%|████████▊ | 22/25 [00:56<00:08,  2.71s/it][A
 92%|█████████▏| 23/25 [00:59<00:05,  2.67s/it][A
 96%|█████████▌| 24/25 [01:00<00:02,  2.36s/it][A
100%|██████████| 25/25 [01:01<00:00,  1.74s/it][A                                                 
                                               [A100%|██████████| 245/245 [40:42<00:00,  8.30s/it]
100%|██████████| 25/25 [01:01<00:00,  1.74s/it][A
                                               [A{'eval_loss': 0.18041415512561798, 'eval_runtime': 65.9603, 'eval_samples_per_second': 2.971, 'eval_steps_per_second': 0.379, 'eval_num_tokens': 3951315.0, 'eval_mean_token_accuracy': 0.9574733138084411, 'epoch': 5.0}
Traceback (most recent call last):
  File "/storage/ice1/2/0/ryoshida7/vlm_train.py", line 206, in <module>
    instance.main()
  File "/storage/ice1/2/0/ryoshida7/vlm_train.py", line 48, in main
    config_file, time_taken, log_history = self.run()
  File "/storage/ice1/2/0/ryoshida7/vlm_train.py", line 185, in run
    train_result = trainer.train()
  File "/home/hice1/ryoshida7/scratch/py10_vlm/lib/python3.10/site-packages/transformers/trainer.py", line 2245, in train
    return inner_training_loop(
  File "/home/hice1/ryoshida7/scratch/py10_vlm/lib/python3.10/site-packages/transformers/trainer.py", line 2691, in _inner_training_loop
    self._load_best_model()
  File "/home/hice1/ryoshida7/scratch/py10_vlm/lib/python3.10/site-packages/transformers/trainer.py", line 2975, in _load_best_model
    model.load_adapter(self.state.best_model_checkpoint, active_adapter)
  File "/home/hice1/ryoshida7/scratch/py10_vlm/lib/python3.10/site-packages/peft/peft_model.py", line 1276, in load_adapter
    load_result = set_peft_model_state_dict(
  File "/home/hice1/ryoshida7/scratch/py10_vlm/lib/python3.10/site-packages/peft/utils/save_and_load.py", line 448, in set_peft_model_state_dict
    load_result = model.load_state_dict(peft_model_state_dict, strict=False)
  File "/home/hice1/ryoshida7/scratch/py10_vlm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 2581, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for PeftModelForCausalLM:
	size mismatch for base_model.model.model.vision_model.encoder.layers.0.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 768]) from checkpoint, the shape in current model is torch.Size([8, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.0.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 512]) from checkpoint, the shape in current model is torch.Size([768, 8]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.0.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 768]) from checkpoint, the shape in current model is torch.Size([8, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.0.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 512]) from checkpoint, the shape in current model is torch.Size([768, 8]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 768]) from checkpoint, the shape in current model is torch.Size([8, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.0.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 512]) from checkpoint, the shape in current model is torch.Size([768, 8]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.1.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 768]) from checkpoint, the shape in current model is torch.Size([8, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.1.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 512]) from checkpoint, the shape in current model is torch.Size([768, 8]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.1.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 768]) from checkpoint, the shape in current model is torch.Size([8, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.1.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 512]) from checkpoint, the shape in current model is torch.Size([768, 8]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.1.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 768]) from checkpoint, the shape in current model is torch.Size([8, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.1.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 512]) from checkpoint, the shape in current model is torch.Size([768, 8]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.2.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 768]) from checkpoint, the shape in current model is torch.Size([8, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.2.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 512]) from checkpoint, the shape in current model is torch.Size([768, 8]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.2.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 768]) from checkpoint, the shape in current model is torch.Size([8, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.2.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 512]) from checkpoint, the shape in current model is torch.Size([768, 8]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.2.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 768]) from checkpoint, the shape in current model is torch.Size([8, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.2.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 512]) from checkpoint, the shape in current model is torch.Size([768, 8]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.3.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 768]) from checkpoint, the shape in current model is torch.Size([8, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.3.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 512]) from checkpoint, the shape in current model is torch.Size([768, 8]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.3.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 768]) from checkpoint, the shape in current model is torch.Size([8, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.3.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 512]) from checkpoint, the shape in current model is torch.Size([768, 8]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.3.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 768]) from checkpoint, the shape in current model is torch.Size([8, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.3.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 512]) from checkpoint, the shape in current model is torch.Size([768, 8]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.4.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 768]) from checkpoint, the shape in current model is torch.Size([8, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.4.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 512]) from checkpoint, the shape in current model is torch.Size([768, 8]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.4.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 768]) from checkpoint, the shape in current model is torch.Size([8, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.4.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 512]) from checkpoint, the shape in current model is torch.Size([768, 8]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.4.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 768]) from checkpoint, the shape in current model is torch.Size([8, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.4.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 512]) from checkpoint, the shape in current model is torch.Size([768, 8]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.5.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 768]) from checkpoint, the shape in current model is torch.Size([8, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.5.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 512]) from checkpoint, the shape in current model is torch.Size([768, 8]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.5.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 768]) from checkpoint, the shape in current model is torch.Size([8, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.5.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 512]) from checkpoint, the shape in current model is torch.Size([768, 8]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.5.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 768]) from checkpoint, the shape in current model is torch.Size([8, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.5.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 512]) from checkpoint, the shape in current model is torch.Size([768, 8]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.6.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 768]) from checkpoint, the shape in current model is torch.Size([8, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.6.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 512]) from checkpoint, the shape in current model is torch.Size([768, 8]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.6.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 768]) from checkpoint, the shape in current model is torch.Size([8, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.6.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 512]) from checkpoint, the shape in current model is torch.Size([768, 8]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.6.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 768]) from checkpoint, the shape in current model is torch.Size([8, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.6.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 512]) from checkpoint, the shape in current model is torch.Size([768, 8]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.7.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 768]) from checkpoint, the shape in current model is torch.Size([8, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.7.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 512]) from checkpoint, the shape in current model is torch.Size([768, 8]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.7.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 768]) from checkpoint, the shape in current model is torch.Size([8, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.7.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 512]) from checkpoint, the shape in current model is torch.Size([768, 8]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.7.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 768]) from checkpoint, the shape in current model is torch.Size([8, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.7.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 512]) from checkpoint, the shape in current model is torch.Size([768, 8]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.8.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 768]) from checkpoint, the shape in current model is torch.Size([8, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.8.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 512]) from checkpoint, the shape in current model is torch.Size([768, 8]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.8.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 768]) from checkpoint, the shape in current model is torch.Size([8, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.8.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 512]) from checkpoint, the shape in current model is torch.Size([768, 8]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.8.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 768]) from checkpoint, the shape in current model is torch.Size([8, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.8.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 512]) from checkpoint, the shape in current model is torch.Size([768, 8]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.9.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 768]) from checkpoint, the shape in current model is torch.Size([8, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.9.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 512]) from checkpoint, the shape in current model is torch.Size([768, 8]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.9.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 768]) from checkpoint, the shape in current model is torch.Size([8, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.9.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 512]) from checkpoint, the shape in current model is torch.Size([768, 8]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.9.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 768]) from checkpoint, the shape in current model is torch.Size([8, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.9.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 512]) from checkpoint, the shape in current model is torch.Size([768, 8]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.10.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 768]) from checkpoint, the shape in current model is torch.Size([8, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.10.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 512]) from checkpoint, the shape in current model is torch.Size([768, 8]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.10.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 768]) from checkpoint, the shape in current model is torch.Size([8, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.10.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 512]) from checkpoint, the shape in current model is torch.Size([768, 8]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.10.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 768]) from checkpoint, the shape in current model is torch.Size([8, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.10.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 512]) from checkpoint, the shape in current model is torch.Size([768, 8]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.11.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 768]) from checkpoint, the shape in current model is torch.Size([8, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.11.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 512]) from checkpoint, the shape in current model is torch.Size([768, 8]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.11.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 768]) from checkpoint, the shape in current model is torch.Size([8, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.11.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 512]) from checkpoint, the shape in current model is torch.Size([768, 8]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.11.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 768]) from checkpoint, the shape in current model is torch.Size([8, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.11.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 512]) from checkpoint, the shape in current model is torch.Size([768, 8]).
	size mismatch for base_model.model.model.text_model.layers.0.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.0.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.model.text_model.layers.0.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.0.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([192, 512]) from checkpoint, the shape in current model is torch.Size([192, 8]).
	size mismatch for base_model.model.model.text_model.layers.0.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.0.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([192, 512]) from checkpoint, the shape in current model is torch.Size([192, 8]).
	size mismatch for base_model.model.model.text_model.layers.0.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.0.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.model.text_model.layers.0.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.0.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([1536, 8]).
	size mismatch for base_model.model.model.text_model.layers.0.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.0.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([1536, 8]).
	size mismatch for base_model.model.model.text_model.layers.0.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 1536]) from checkpoint, the shape in current model is torch.Size([8, 1536]).
	size mismatch for base_model.model.model.text_model.layers.0.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.model.text_model.layers.1.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.1.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.model.text_model.layers.1.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.1.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([192, 512]) from checkpoint, the shape in current model is torch.Size([192, 8]).
	size mismatch for base_model.model.model.text_model.layers.1.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.1.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([192, 512]) from checkpoint, the shape in current model is torch.Size([192, 8]).
	size mismatch for base_model.model.model.text_model.layers.1.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.1.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.model.text_model.layers.1.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.1.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([1536, 8]).
	size mismatch for base_model.model.model.text_model.layers.1.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.1.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([1536, 8]).
	size mismatch for base_model.model.model.text_model.layers.1.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 1536]) from checkpoint, the shape in current model is torch.Size([8, 1536]).
	size mismatch for base_model.model.model.text_model.layers.1.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.model.text_model.layers.2.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.2.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.model.text_model.layers.2.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.2.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([192, 512]) from checkpoint, the shape in current model is torch.Size([192, 8]).
	size mismatch for base_model.model.model.text_model.layers.2.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.2.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([192, 512]) from checkpoint, the shape in current model is torch.Size([192, 8]).
	size mismatch for base_model.model.model.text_model.layers.2.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.2.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.model.text_model.layers.2.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.2.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([1536, 8]).
	size mismatch for base_model.model.model.text_model.layers.2.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.2.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([1536, 8]).
	size mismatch for base_model.model.model.text_model.layers.2.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 1536]) from checkpoint, the shape in current model is torch.Size([8, 1536]).
	size mismatch for base_model.model.model.text_model.layers.2.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.model.text_model.layers.3.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.3.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.model.text_model.layers.3.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.3.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([192, 512]) from checkpoint, the shape in current model is torch.Size([192, 8]).
	size mismatch for base_model.model.model.text_model.layers.3.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.3.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([192, 512]) from checkpoint, the shape in current model is torch.Size([192, 8]).
	size mismatch for base_model.model.model.text_model.layers.3.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.3.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.model.text_model.layers.3.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.3.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([1536, 8]).
	size mismatch for base_model.model.model.text_model.layers.3.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.3.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([1536, 8]).
	size mismatch for base_model.model.model.text_model.layers.3.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 1536]) from checkpoint, the shape in current model is torch.Size([8, 1536]).
	size mismatch for base_model.model.model.text_model.layers.3.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.model.text_model.layers.4.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.4.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.model.text_model.layers.4.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.4.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([192, 512]) from checkpoint, the shape in current model is torch.Size([192, 8]).
	size mismatch for base_model.model.model.text_model.layers.4.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.4.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([192, 512]) from checkpoint, the shape in current model is torch.Size([192, 8]).
	size mismatch for base_model.model.model.text_model.layers.4.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.4.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.model.text_model.layers.4.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.4.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([1536, 8]).
	size mismatch for base_model.model.model.text_model.layers.4.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.4.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([1536, 8]).
	size mismatch for base_model.model.model.text_model.layers.4.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 1536]) from checkpoint, the shape in current model is torch.Size([8, 1536]).
	size mismatch for base_model.model.model.text_model.layers.4.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.model.text_model.layers.5.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.5.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.model.text_model.layers.5.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.5.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([192, 512]) from checkpoint, the shape in current model is torch.Size([192, 8]).
	size mismatch for base_model.model.model.text_model.layers.5.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.5.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([192, 512]) from checkpoint, the shape in current model is torch.Size([192, 8]).
	size mismatch for base_model.model.model.text_model.layers.5.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.5.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.model.text_model.layers.5.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.5.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([1536, 8]).
	size mismatch for base_model.model.model.text_model.layers.5.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.5.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([1536, 8]).
	size mismatch for base_model.model.model.text_model.layers.5.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 1536]) from checkpoint, the shape in current model is torch.Size([8, 1536]).
	size mismatch for base_model.model.model.text_model.layers.5.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.model.text_model.layers.6.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.6.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.model.text_model.layers.6.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.6.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([192, 512]) from checkpoint, the shape in current model is torch.Size([192, 8]).
	size mismatch for base_model.model.model.text_model.layers.6.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.6.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([192, 512]) from checkpoint, the shape in current model is torch.Size([192, 8]).
	size mismatch for base_model.model.model.text_model.layers.6.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.6.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.model.text_model.layers.6.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.6.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([1536, 8]).
	size mismatch for base_model.model.model.text_model.layers.6.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.6.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([1536, 8]).
	size mismatch for base_model.model.model.text_model.layers.6.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 1536]) from checkpoint, the shape in current model is torch.Size([8, 1536]).
	size mismatch for base_model.model.model.text_model.layers.6.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.model.text_model.layers.7.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.7.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.model.text_model.layers.7.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.7.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([192, 512]) from checkpoint, the shape in current model is torch.Size([192, 8]).
	size mismatch for base_model.model.model.text_model.layers.7.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.7.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([192, 512]) from checkpoint, the shape in current model is torch.Size([192, 8]).
	size mismatch for base_model.model.model.text_model.layers.7.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.7.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.model.text_model.layers.7.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.7.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([1536, 8]).
	size mismatch for base_model.model.model.text_model.layers.7.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.7.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([1536, 8]).
	size mismatch for base_model.model.model.text_model.layers.7.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 1536]) from checkpoint, the shape in current model is torch.Size([8, 1536]).
	size mismatch for base_model.model.model.text_model.layers.7.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.model.text_model.layers.8.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.8.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.model.text_model.layers.8.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.8.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([192, 512]) from checkpoint, the shape in current model is torch.Size([192, 8]).
	size mismatch for base_model.model.model.text_model.layers.8.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.8.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([192, 512]) from checkpoint, the shape in current model is torch.Size([192, 8]).
	size mismatch for base_model.model.model.text_model.layers.8.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.8.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.model.text_model.layers.8.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.8.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([1536, 8]).
	size mismatch for base_model.model.model.text_model.layers.8.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.8.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([1536, 8]).
	size mismatch for base_model.model.model.text_model.layers.8.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 1536]) from checkpoint, the shape in current model is torch.Size([8, 1536]).
	size mismatch for base_model.model.model.text_model.layers.8.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.model.text_model.layers.9.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.9.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.model.text_model.layers.9.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.9.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([192, 512]) from checkpoint, the shape in current model is torch.Size([192, 8]).
	size mismatch for base_model.model.model.text_model.layers.9.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.9.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([192, 512]) from checkpoint, the shape in current model is torch.Size([192, 8]).
	size mismatch for base_model.model.model.text_model.layers.9.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.9.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.model.text_model.layers.9.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.9.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([1536, 8]).
	size mismatch for base_model.model.model.text_model.layers.9.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.9.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([1536, 8]).
	size mismatch for base_model.model.model.text_model.layers.9.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 1536]) from checkpoint, the shape in current model is torch.Size([8, 1536]).
	size mismatch for base_model.model.model.text_model.layers.9.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.model.text_model.layers.10.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.10.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.model.text_model.layers.10.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.10.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([192, 512]) from checkpoint, the shape in current model is torch.Size([192, 8]).
	size mismatch for base_model.model.model.text_model.layers.10.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.10.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([192, 512]) from checkpoint, the shape in current model is torch.Size([192, 8]).
	size mismatch for base_model.model.model.text_model.layers.10.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.10.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.model.text_model.layers.10.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.10.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([1536, 8]).
	size mismatch for base_model.model.model.text_model.layers.10.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.10.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([1536, 8]).
	size mismatch for base_model.model.model.text_model.layers.10.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 1536]) from checkpoint, the shape in current model is torch.Size([8, 1536]).
	size mismatch for base_model.model.model.text_model.layers.10.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.model.text_model.layers.11.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.11.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.model.text_model.layers.11.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.11.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([192, 512]) from checkpoint, the shape in current model is torch.Size([192, 8]).
	size mismatch for base_model.model.model.text_model.layers.11.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.11.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([192, 512]) from checkpoint, the shape in current model is torch.Size([192, 8]).
	size mismatch for base_model.model.model.text_model.layers.11.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.11.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.model.text_model.layers.11.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.11.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([1536, 8]).
	size mismatch for base_model.model.model.text_model.layers.11.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.11.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([1536, 8]).
	size mismatch for base_model.model.model.text_model.layers.11.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 1536]) from checkpoint, the shape in current model is torch.Size([8, 1536]).
	size mismatch for base_model.model.model.text_model.layers.11.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.model.text_model.layers.12.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.12.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.model.text_model.layers.12.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.12.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([192, 512]) from checkpoint, the shape in current model is torch.Size([192, 8]).
	size mismatch for base_model.model.model.text_model.layers.12.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.12.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([192, 512]) from checkpoint, the shape in current model is torch.Size([192, 8]).
	size mismatch for base_model.model.model.text_model.layers.12.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.12.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.model.text_model.layers.12.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.12.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([1536, 8]).
	size mismatch for base_model.model.model.text_model.layers.12.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.12.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([1536, 8]).
	size mismatch for base_model.model.model.text_model.layers.12.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 1536]) from checkpoint, the shape in current model is torch.Size([8, 1536]).
	size mismatch for base_model.model.model.text_model.layers.12.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.model.text_model.layers.13.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.13.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.model.text_model.layers.13.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.13.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([192, 512]) from checkpoint, the shape in current model is torch.Size([192, 8]).
	size mismatch for base_model.model.model.text_model.layers.13.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.13.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([192, 512]) from checkpoint, the shape in current model is torch.Size([192, 8]).
	size mismatch for base_model.model.model.text_model.layers.13.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.13.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.model.text_model.layers.13.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.13.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([1536, 8]).
	size mismatch for base_model.model.model.text_model.layers.13.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.13.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([1536, 8]).
	size mismatch for base_model.model.model.text_model.layers.13.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 1536]) from checkpoint, the shape in current model is torch.Size([8, 1536]).
	size mismatch for base_model.model.model.text_model.layers.13.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.model.text_model.layers.14.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.14.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.model.text_model.layers.14.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.14.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([192, 512]) from checkpoint, the shape in current model is torch.Size([192, 8]).
	size mismatch for base_model.model.model.text_model.layers.14.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.14.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([192, 512]) from checkpoint, the shape in current model is torch.Size([192, 8]).
	size mismatch for base_model.model.model.text_model.layers.14.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.14.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.model.text_model.layers.14.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.14.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([1536, 8]).
	size mismatch for base_model.model.model.text_model.layers.14.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.14.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([1536, 8]).
	size mismatch for base_model.model.model.text_model.layers.14.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 1536]) from checkpoint, the shape in current model is torch.Size([8, 1536]).
	size mismatch for base_model.model.model.text_model.layers.14.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.model.text_model.layers.15.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.15.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.model.text_model.layers.15.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.15.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([192, 512]) from checkpoint, the shape in current model is torch.Size([192, 8]).
	size mismatch for base_model.model.model.text_model.layers.15.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.15.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([192, 512]) from checkpoint, the shape in current model is torch.Size([192, 8]).
	size mismatch for base_model.model.model.text_model.layers.15.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.15.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.model.text_model.layers.15.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.15.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([1536, 8]).
	size mismatch for base_model.model.model.text_model.layers.15.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.15.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([1536, 8]).
	size mismatch for base_model.model.model.text_model.layers.15.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 1536]) from checkpoint, the shape in current model is torch.Size([8, 1536]).
	size mismatch for base_model.model.model.text_model.layers.15.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.model.text_model.layers.16.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.16.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.model.text_model.layers.16.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.16.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([192, 512]) from checkpoint, the shape in current model is torch.Size([192, 8]).
	size mismatch for base_model.model.model.text_model.layers.16.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.16.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([192, 512]) from checkpoint, the shape in current model is torch.Size([192, 8]).
	size mismatch for base_model.model.model.text_model.layers.16.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.16.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.model.text_model.layers.16.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.16.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([1536, 8]).
	size mismatch for base_model.model.model.text_model.layers.16.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.16.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([1536, 8]).
	size mismatch for base_model.model.model.text_model.layers.16.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 1536]) from checkpoint, the shape in current model is torch.Size([8, 1536]).
	size mismatch for base_model.model.model.text_model.layers.16.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.model.text_model.layers.17.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.17.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.model.text_model.layers.17.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.17.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([192, 512]) from checkpoint, the shape in current model is torch.Size([192, 8]).
	size mismatch for base_model.model.model.text_model.layers.17.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.17.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([192, 512]) from checkpoint, the shape in current model is torch.Size([192, 8]).
	size mismatch for base_model.model.model.text_model.layers.17.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.17.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.model.text_model.layers.17.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.17.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([1536, 8]).
	size mismatch for base_model.model.model.text_model.layers.17.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.17.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([1536, 8]).
	size mismatch for base_model.model.model.text_model.layers.17.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 1536]) from checkpoint, the shape in current model is torch.Size([8, 1536]).
	size mismatch for base_model.model.model.text_model.layers.17.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.model.text_model.layers.18.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.18.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.model.text_model.layers.18.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.18.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([192, 512]) from checkpoint, the shape in current model is torch.Size([192, 8]).
	size mismatch for base_model.model.model.text_model.layers.18.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.18.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([192, 512]) from checkpoint, the shape in current model is torch.Size([192, 8]).
	size mismatch for base_model.model.model.text_model.layers.18.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.18.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.model.text_model.layers.18.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.18.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([1536, 8]).
	size mismatch for base_model.model.model.text_model.layers.18.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.18.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([1536, 8]).
	size mismatch for base_model.model.model.text_model.layers.18.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 1536]) from checkpoint, the shape in current model is torch.Size([8, 1536]).
	size mismatch for base_model.model.model.text_model.layers.18.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.model.text_model.layers.19.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.19.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.model.text_model.layers.19.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.19.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([192, 512]) from checkpoint, the shape in current model is torch.Size([192, 8]).
	size mismatch for base_model.model.model.text_model.layers.19.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.19.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([192, 512]) from checkpoint, the shape in current model is torch.Size([192, 8]).
	size mismatch for base_model.model.model.text_model.layers.19.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.19.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.model.text_model.layers.19.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.19.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([1536, 8]).
	size mismatch for base_model.model.model.text_model.layers.19.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.19.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([1536, 8]).
	size mismatch for base_model.model.model.text_model.layers.19.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 1536]) from checkpoint, the shape in current model is torch.Size([8, 1536]).
	size mismatch for base_model.model.model.text_model.layers.19.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.model.text_model.layers.20.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.20.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.model.text_model.layers.20.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.20.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([192, 512]) from checkpoint, the shape in current model is torch.Size([192, 8]).
	size mismatch for base_model.model.model.text_model.layers.20.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.20.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([192, 512]) from checkpoint, the shape in current model is torch.Size([192, 8]).
	size mismatch for base_model.model.model.text_model.layers.20.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.20.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.model.text_model.layers.20.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.20.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([1536, 8]).
	size mismatch for base_model.model.model.text_model.layers.20.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.20.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([1536, 8]).
	size mismatch for base_model.model.model.text_model.layers.20.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 1536]) from checkpoint, the shape in current model is torch.Size([8, 1536]).
	size mismatch for base_model.model.model.text_model.layers.20.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.model.text_model.layers.21.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.21.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.model.text_model.layers.21.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.21.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([192, 512]) from checkpoint, the shape in current model is torch.Size([192, 8]).
	size mismatch for base_model.model.model.text_model.layers.21.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.21.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([192, 512]) from checkpoint, the shape in current model is torch.Size([192, 8]).
	size mismatch for base_model.model.model.text_model.layers.21.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.21.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.model.text_model.layers.21.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.21.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([1536, 8]).
	size mismatch for base_model.model.model.text_model.layers.21.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.21.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([1536, 8]).
	size mismatch for base_model.model.model.text_model.layers.21.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 1536]) from checkpoint, the shape in current model is torch.Size([8, 1536]).
	size mismatch for base_model.model.model.text_model.layers.21.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.model.text_model.layers.22.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.22.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.model.text_model.layers.22.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.22.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([192, 512]) from checkpoint, the shape in current model is torch.Size([192, 8]).
	size mismatch for base_model.model.model.text_model.layers.22.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.22.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([192, 512]) from checkpoint, the shape in current model is torch.Size([192, 8]).
	size mismatch for base_model.model.model.text_model.layers.22.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.22.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.model.text_model.layers.22.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.22.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([1536, 8]).
	size mismatch for base_model.model.model.text_model.layers.22.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.22.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([1536, 8]).
	size mismatch for base_model.model.model.text_model.layers.22.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 1536]) from checkpoint, the shape in current model is torch.Size([8, 1536]).
	size mismatch for base_model.model.model.text_model.layers.22.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.model.text_model.layers.23.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.23.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.model.text_model.layers.23.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.23.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([192, 512]) from checkpoint, the shape in current model is torch.Size([192, 8]).
	size mismatch for base_model.model.model.text_model.layers.23.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.23.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([192, 512]) from checkpoint, the shape in current model is torch.Size([192, 8]).
	size mismatch for base_model.model.model.text_model.layers.23.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.23.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.model.text_model.layers.23.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.23.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([1536, 8]).
	size mismatch for base_model.model.model.text_model.layers.23.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.23.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([1536, 8]).
	size mismatch for base_model.model.model.text_model.layers.23.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 1536]) from checkpoint, the shape in current model is torch.Size([8, 1536]).
	size mismatch for base_model.model.model.text_model.layers.23.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.model.text_model.layers.24.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.24.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.model.text_model.layers.24.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.24.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([192, 512]) from checkpoint, the shape in current model is torch.Size([192, 8]).
	size mismatch for base_model.model.model.text_model.layers.24.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.24.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([192, 512]) from checkpoint, the shape in current model is torch.Size([192, 8]).
	size mismatch for base_model.model.model.text_model.layers.24.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.24.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.model.text_model.layers.24.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.24.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([1536, 8]).
	size mismatch for base_model.model.model.text_model.layers.24.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.24.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([1536, 8]).
	size mismatch for base_model.model.model.text_model.layers.24.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 1536]) from checkpoint, the shape in current model is torch.Size([8, 1536]).
	size mismatch for base_model.model.model.text_model.layers.24.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.model.text_model.layers.25.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.25.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.model.text_model.layers.25.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.25.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([192, 512]) from checkpoint, the shape in current model is torch.Size([192, 8]).
	size mismatch for base_model.model.model.text_model.layers.25.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.25.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([192, 512]) from checkpoint, the shape in current model is torch.Size([192, 8]).
	size mismatch for base_model.model.model.text_model.layers.25.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.25.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.model.text_model.layers.25.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.25.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([1536, 8]).
	size mismatch for base_model.model.model.text_model.layers.25.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.25.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([1536, 8]).
	size mismatch for base_model.model.model.text_model.layers.25.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 1536]) from checkpoint, the shape in current model is torch.Size([8, 1536]).
	size mismatch for base_model.model.model.text_model.layers.25.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.model.text_model.layers.26.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.26.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.model.text_model.layers.26.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.26.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([192, 512]) from checkpoint, the shape in current model is torch.Size([192, 8]).
	size mismatch for base_model.model.model.text_model.layers.26.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.26.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([192, 512]) from checkpoint, the shape in current model is torch.Size([192, 8]).
	size mismatch for base_model.model.model.text_model.layers.26.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.26.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.model.text_model.layers.26.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.26.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([1536, 8]).
	size mismatch for base_model.model.model.text_model.layers.26.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.26.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([1536, 8]).
	size mismatch for base_model.model.model.text_model.layers.26.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 1536]) from checkpoint, the shape in current model is torch.Size([8, 1536]).
	size mismatch for base_model.model.model.text_model.layers.26.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.model.text_model.layers.27.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.27.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.model.text_model.layers.27.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.27.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([192, 512]) from checkpoint, the shape in current model is torch.Size([192, 8]).
	size mismatch for base_model.model.model.text_model.layers.27.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.27.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([192, 512]) from checkpoint, the shape in current model is torch.Size([192, 8]).
	size mismatch for base_model.model.model.text_model.layers.27.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.27.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.model.text_model.layers.27.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.27.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([1536, 8]).
	size mismatch for base_model.model.model.text_model.layers.27.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.27.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([1536, 8]).
	size mismatch for base_model.model.model.text_model.layers.27.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 1536]) from checkpoint, the shape in current model is torch.Size([8, 1536]).
	size mismatch for base_model.model.model.text_model.layers.27.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.model.text_model.layers.28.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.28.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.model.text_model.layers.28.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.28.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([192, 512]) from checkpoint, the shape in current model is torch.Size([192, 8]).
	size mismatch for base_model.model.model.text_model.layers.28.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.28.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([192, 512]) from checkpoint, the shape in current model is torch.Size([192, 8]).
	size mismatch for base_model.model.model.text_model.layers.28.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.28.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.model.text_model.layers.28.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.28.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([1536, 8]).
	size mismatch for base_model.model.model.text_model.layers.28.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.28.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([1536, 8]).
	size mismatch for base_model.model.model.text_model.layers.28.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 1536]) from checkpoint, the shape in current model is torch.Size([8, 1536]).
	size mismatch for base_model.model.model.text_model.layers.28.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.model.text_model.layers.29.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.29.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.model.text_model.layers.29.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.29.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([192, 512]) from checkpoint, the shape in current model is torch.Size([192, 8]).
	size mismatch for base_model.model.model.text_model.layers.29.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.29.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([192, 512]) from checkpoint, the shape in current model is torch.Size([192, 8]).
	size mismatch for base_model.model.model.text_model.layers.29.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.29.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.model.text_model.layers.29.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.29.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([1536, 8]).
	size mismatch for base_model.model.model.text_model.layers.29.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.model.text_model.layers.29.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([1536, 8]).
	size mismatch for base_model.model.model.text_model.layers.29.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([512, 1536]) from checkpoint, the shape in current model is torch.Size([8, 1536]).
	size mismatch for base_model.model.model.text_model.layers.29.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([576, 512]) from checkpoint, the shape in current model is torch.Size([576, 8]).
	size mismatch for base_model.model.lm_head.lora_A.default.weight: copying a param with shape torch.Size([512, 576]) from checkpoint, the shape in current model is torch.Size([8, 576]).
	size mismatch for base_model.model.lm_head.lora_B.default.weight: copying a param with shape torch.Size([49280, 512]) from checkpoint, the shape in current model is torch.Size([49280, 8]).
100%|██████████| 245/245 [40:46<00:00,  9.98s/it]
r 8
lora_alpha 256
lora_dropout 0.1
target_modules ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj', 'lm_head']
Traceback (most recent call last):
  File "/home/hice1/ryoshida7/scratch/py10_vlm/lib/python3.10/site-packages/huggingface_hub/utils/_http.py", line 409, in hf_raise_for_status
    response.raise_for_status()
  File "/home/hice1/ryoshida7/scratch/py10_vlm/lib/python3.10/site-packages/requests/models.py", line 1024, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/256M-instruct-comp-ti-5epoch-45/final_model/resolve/main/adapter_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/hice1/ryoshida7/scratch/py10_vlm/lib/python3.10/site-packages/transformers/utils/hub.py", line 424, in cached_files
    hf_hub_download(
  File "/home/hice1/ryoshida7/scratch/py10_vlm/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
  File "/home/hice1/ryoshida7/scratch/py10_vlm/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 961, in hf_hub_download
    return _hf_hub_download_to_cache_dir(
  File "/home/hice1/ryoshida7/scratch/py10_vlm/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1068, in _hf_hub_download_to_cache_dir
    _raise_on_head_call_error(head_call_error, force_download, local_files_only)
  File "/home/hice1/ryoshida7/scratch/py10_vlm/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1596, in _raise_on_head_call_error
    raise head_call_error
  File "/home/hice1/ryoshida7/scratch/py10_vlm/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1484, in _get_metadata_or_catch_error
    metadata = get_hf_file_metadata(
  File "/home/hice1/ryoshida7/scratch/py10_vlm/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
  File "/home/hice1/ryoshida7/scratch/py10_vlm/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1401, in get_hf_file_metadata
    r = _request_wrapper(
  File "/home/hice1/ryoshida7/scratch/py10_vlm/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 285, in _request_wrapper
    response = _request_wrapper(
  File "/home/hice1/ryoshida7/scratch/py10_vlm/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 309, in _request_wrapper
    hf_raise_for_status(response)
  File "/home/hice1/ryoshida7/scratch/py10_vlm/lib/python3.10/site-packages/huggingface_hub/utils/_http.py", line 459, in hf_raise_for_status
    raise _format(RepositoryNotFoundError, message, response) from e
huggingface_hub.errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-680e0d61-68c4925972192be347bdf1a8;78d5920e-5ddb-4393-b0f8-3b01b4dd234e)

Repository Not Found for url: https://huggingface.co/256M-instruct-comp-ti-5epoch-45/final_model/resolve/main/adapter_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated. For more details, see https://huggingface.co/docs/huggingface_hub/authentication
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/storage/ice1/2/0/ryoshida7/vlm_inference.py", line 249, in <module>
    instance.main()
  File "/storage/ice1/2/0/ryoshida7/vlm_inference.py", line 238, in main
    self.prepare_eval()
  File "/storage/ice1/2/0/ryoshida7/vlm_inference.py", line 83, in prepare_eval
    model, processor = model_instance.get_model()
  File "/storage/ice1/2/0/ryoshida7/vlm_model.py", line 75, in get_model
    model.load_adapter(self.adapter_path)
  File "/home/hice1/ryoshida7/scratch/py10_vlm/lib/python3.10/site-packages/transformers/integrations/peft.py", line 200, in load_adapter
    adapter_config_file = find_adapter_config_file(
  File "/home/hice1/ryoshida7/scratch/py10_vlm/lib/python3.10/site-packages/transformers/utils/peft_utils.py", line 88, in find_adapter_config_file
    adapter_cached_filename = cached_file(
  File "/home/hice1/ryoshida7/scratch/py10_vlm/lib/python3.10/site-packages/transformers/utils/hub.py", line 266, in cached_file
    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
  File "/home/hice1/ryoshida7/scratch/py10_vlm/lib/python3.10/site-packages/transformers/utils/hub.py", line 456, in cached_files
    raise OSError(
OSError: 256M-instruct-comp-ti-5epoch-45/final_model is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`
---------------------------------------
Begin Slurm Epilog: Apr-27-2025 06:56:34
Job ID:        2539200
User ID:       ryoshida7
Account:       coc
Job name:      256M-instruct-comp-ti-5epoch-45
Resources:     cpu=1,gres/gpu:a100=1,mem=32G,node=1
Rsrc Used:     cput=00:41:46,vmem=0,walltime=00:41:46,mem=13069020K,energy_used=0
Partition:     coc-gpu
QOS:           coc-ice
Nodes:         atl1-1-01-005-13-0
---------------------------------------
