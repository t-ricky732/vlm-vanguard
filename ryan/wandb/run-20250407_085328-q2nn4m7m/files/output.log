PyTorch version: 2.6.0+cu124
CUDA available: True
CUDA version: 12.4
GPU: NVIDIA A100-SXM4-40GB
GPU memory: 42.47 GB
Using dtype: torch.bfloat16
Checking data directory: /content/drive/MyDrive/code/chartllama_data
Does directory exist? True
Files in directory:
  - candlestick_chart_100examples_simplified_qa.json
  - funnel_chart_100examples_simplified_qa.json
  - gantt_chart_100examples_simplified_qa.json
  - polar_chart_100examples_simplified_qa.json
  - heatmap_chart_100examples_simplified_qa.json
  - box_chart_100examples_simplified_qa.json
  - .gitattributes
  - ours.zip
  - scatter_chart_100examples_simplified_qa.json
  - extracted_images
  - .cache
Loading dataset using custom ChartDataset...
Loading processor...
/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning:
The secret `HF_TOKEN` does not exist in your Colab secrets.
To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.
You will be able to reuse this secret in all of your notebooks.
Please note that authentication is recommended but still optional to access public models or datasets.
  warnings.warn(
Checking data directory: /content/drive/MyDrive/code/chartllama_data
Does directory exist? True
Files in directory:
  - candlestick_chart_100examples_simplified_qa.json
  - funnel_chart_100examples_simplified_qa.json
  - gantt_chart_100examples_simplified_qa.json
  - polar_chart_100examples_simplified_qa.json
  - heatmap_chart_100examples_simplified_qa.json
  - box_chart_100examples_simplified_qa.json
  - .gitattributes
  - ours.zip
  - scatter_chart_100examples_simplified_qa.json
  - extracted_images
  - .cache
Loading dataset using custom ChartDataset...
Reading JSONs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:14<00:00,  2.12s/it]
Total samples: 980
Training samples: 784
Validation samples: 98
Test samples: 98
Configuring model for standard LoRA (dtype: torch.bfloat16)...
Loading model 'HuggingFaceTB/SmolVLM-256M-Base'...
Model loaded in 1.35 seconds
Configured for Full Fine-tuning.
/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
Initializing Trainer...
Starting training...
[34m[1mwandb[0m: [33mWARNING[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
Training completed in 40.46 minutes
Saving training results and final model/adapter...
***** train metrics *****
  epoch                    =        3.0
  total_flos               =  3449970GF
  train_loss               =     2.6857
  train_runtime            = 0:40:27.26
  train_samples_per_second =      0.969
  train_steps_per_second   =      0.061
Model adapter and processor saved to ./smolvlm-chartllama-256-full-tuned
Loading processor...
Loading dataset using custom ChartDataset...
Reading JSONs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:14<00:00,  2.07s/it]
Total samples: 980
Training samples: 784
Validation samples: 98
Test samples: 98
Configuring model for QLoRA (8-bit quantization)...
Loading model 'HuggingFaceTB/SmolVLM-256M-Base'...
`low_cpu_mem_usage` was None, now default to True since model is quantized.
Model loaded in 1.59 seconds
Preparing model for LoRA training...
Applying prepare_model_for_kbit_training for QLoRA...
trainable params: 5,769,216 || all params: 262,254,144 || trainable%: 2.1999
/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
Initializing Trainer...
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Starting training...
/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Training completed in 55.74 minutes
Saving training results and final model/adapter...
***** train metrics *****
  epoch                    =        3.0
  total_flos               =  3537531GF
  train_loss               =      4.028
  train_runtime            = 0:55:43.88
  train_samples_per_second =      0.703
  train_steps_per_second   =      0.044
Model adapter and processor saved to ./smolvlm-chartllama-256-lora-tuned
Downloading ChartQA dataset...
Loaded ChartQA test split with 2500 examples.

First ChartQA test sample structure:
{'image': <PIL.PngImagePlugin.PngImageFile image mode=RGBA size=850x600 at 0x7E2999A85AD0>, 'query': 'How many food item is shown in the bar graph?', 'label': ['14'], 'human_or_machine': 0}
Using accuracy function: calculate_relaxed_accuracy

--- Loading Base Model for Evaluation ---
Base model 'HuggingFaceTB/SmolVLM-256M-Base' loaded successfully on cuda.

--- Loading Fine-tuned Model for Evaluation ---
Checking for model weights in: ./smolvlm-chartllama-256-full-tuned/final_adapter
Checking for processor in: ./smolvlm-chartllama-256-full-tuned/final_processor
Loading processor from: ./smolvlm-chartllama-256-full-tuned/final_processor
Loading model from: ./smolvlm-chartllama-256-full-tuned/final_adapter
Fine-tuned model and processor loaded successfully from subdirs on cuda.

--- Loading LoRA Fine-tuned Model for Evaluation ---
Adapter config found in subdirectory: ./smolvlm-chartllama-256-lora-tuned/final_adapter
Checking for adapter config in: ./smolvlm-chartllama-256-lora-tuned/final_adapter
Checking for processor config in: ./smolvlm-chartllama-256-lora-tuned/final_processor

Loading processor from: ./smolvlm-chartllama-256-lora-tuned/final_processor

Loading BASE model 'HuggingFaceTB/SmolVLM-256M-Base'...
Configuring base model for standard LoRA loading...
Base model loaded.
Loading LoRA adapter from: ./smolvlm-chartllama-256-lora-tuned/final_adapter
LoRA adapter loaded and applied. Final model ready on cuda:0.

--- Starting Evaluation on 100 ChartQA Samples (Base vs FFT vs LoRA) ---
/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(

--- Evaluation Results ---
Evaluated on: 100 samples
Metric: Relaxed Accuracy (calculate_relaxed_accuracy)

Base Model (HuggingFaceTB/SmolVLM-256M-Base): 0.0300
FFT Model (smolvlm-chartllama-256-full-tuned): 0.0600
LoRA Model (smolvlm-chartllama-256-lora-tuned): 0.0800

--- Example Predictions (First 5 Evaluated) ---

--------------------- Sample Index: 0 ---------------------
  Q: How many food item is shown in the bar graph?
  GT: 14
  Base Pred: 1000 (Incorrect)
  FFT Pred:  1000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000 (Incorrect)
  LoRA Pred: 1000 (Incorrect)

--------------------- Sample Index: 1 ---------------------
  Q: What is the difference in value between Lamb and Corn?
  GT: 0.57
  Base Pred: 1000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.00 (Incorrect)
  FFT Pred:  100.000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000 (Incorrect)
  LoRA Pred: 1000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000 (Incorrect)

--------------------- Sample Index: 2 ---------------------
  Q: How many bars are shown in the chart?
  GT: 3
  Base Pred: 0.1% 0.2% 0.3% 0.4% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0. (Incorrect)
  FFT Pred:  0.21% (Incorrect)
  LoRA Pred: 0.21% 0.38% 0.4% 0.3% 0.4% 0.3% 0.4% 0.3% 0.4% 0.3% 0.4% 0.3% 0.4% 0.3% 0.4% 0.3% 0.4% 0.3% 0.4% 0.3% 0.4% 0.3% 0.4% 0.3% 0.4% (Incorrect)

--------------------- Sample Index: 3 ---------------------
  Q: Is the sum value of Madagascar more then Fiji?
  GT: No
  Base Pred: 0.1% 0.2% 0.3% 0.4% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0. (Incorrect)
  FFT Pred:  Yes. (Incorrect)
  LoRA Pred: 0.21% 0.38% 0.4% 0.3% 0.4% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% (Incorrect)

--------------------- Sample Index: 4 ---------------------
  Q: What's the value of the lowest bar?
  GT: 23
  Base Pred: 0.000000 (Incorrect)
  FFT Pred:  100.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000. (Incorrect)
  LoRA Pred: 0.000000 (Incorrect)

----------------------------------------------------------

--- Evaluation DataFrame (First 10 rows) ---
