Training error: Invalid input type. Must be a single image, a list of images, or a list of batches of images.
Traceback (most recent call last):
  File "C:\Users\ignit\AppData\Local\Temp\ipykernel_32776\1050014336.py", line 4, in <module>
    trainer.train()
  File "c:\Users\ignit\AppData\Local\Programs\Python\Python311\Lib\site-packages\transformers\trainer.py", line 2245, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\ignit\AppData\Local\Programs\Python\Python311\Lib\site-packages\transformers\trainer.py", line 2508, in _inner_training_loop
    batch_samples, num_items_in_batch = self.get_batch_samples(epoch_iterator, num_batches, args.device)
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\ignit\AppData\Local\Programs\Python\Python311\Lib\site-packages\transformers\trainer.py", line 5224, in get_batch_samples
    batch_samples += [next(epoch_iterator)]
                      ^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\ignit\AppData\Local\Programs\Python\Python311\Lib\site-packages\accelerate\data_loader.py", line 566, in __iter__
    current_batch = next(dataloader_iter)
                    ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\ignit\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\utils\data\dataloader.py", line 701, in __next__
    data = self._next_data()
           ^^^^^^^^^^^^^^^^^
  File "c:\Users\ignit\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\utils\data\dataloader.py", line 757, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\ignit\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\utils\data\_utils\fetch.py", line 55, in fetch
    return self.collate_fn(data)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ignit\AppData\Local\Temp\ipykernel_32776\36619815.py", line 35, in collate_fn
    inputs = processor(
             ^^^^^^^^^^
  File "c:\Users\ignit\AppData\Local\Programs\Python\Python311\Lib\site-packages\transformers\models\idefics3\processing_idefics3.py", line 284, in __call__
    image_inputs = self.image_processor(images, **output_kwargs["images_kwargs"])
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\ignit\AppData\Local\Programs\Python\Python311\Lib\site-packages\transformers\image_processing_utils.py", line 42, in __call__
    return self.preprocess(images, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\ignit\AppData\Local\Programs\Python\Python311\Lib\site-packages\transformers\models\idefics3\image_processing_idefics3.py", line 689, in preprocess
    images_list = make_nested_list_of_images(images)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\ignit\AppData\Local\Programs\Python\Python311\Lib\site-packages\transformers\image_utils.py", line 298, in make_nested_list_of_images
    raise ValueError("Invalid input type. Must be a single image, a list of images, or a list of batches of images.")
ValueError: Invalid input type. Must be a single image, a list of images, or a list of batches of images.
Training time: 0.03 minutes
PyTorch version: 2.5.1+cu118
CUDA available: True
Using device: cuda
Loading processor...
Loading model...
2025-03-22 17:39:00,744 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Setting up LoRA...
Loading dataset...
2025-03-22 17:39:03,699 - ChartDataset - INFO - Loaded 50 examples for training
Successfully loaded 50 examples
Sample format:
- System message: system
- User question: What is the title of the chart?
- Assistant response: Analysis of smartphone usage patterns
Training on 40 samples, validating on 10 samples
Setting up training configuration...
Initializing trainer...
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Starting training...
Image type: <class 'PIL.Image.Image'>
Image mode: RGB, size: (384, 384)
Error in processor: The number of images in the text [0, 1] and images [1] should be the same.
Creating dummy inputs
Image type: <class 'PIL.Image.Image'>
Image mode: RGB, size: (384, 384)
Error in processor: The number of images in the text [0, 1] and images [1] should be the same.
Creating dummy inputs
Image type: <class 'PIL.Image.Image'>
Image mode: RGB, size: (384, 384)
Error in processor: The number of images in the text [0, 1] and images [1] should be the same.
Creating dummy inputs
Image type: <class 'PIL.Image.Image'>
Image mode: RGB, size: (384, 384)
Error in processor: The number of images in the text [0, 1] and images [1] should be the same.
Creating dummy inputs
Image type: <class 'PIL.Image.Image'>
Image mode: RGB, size: (384, 384)
Error in processor: The number of images in the text [0, 1] and images [1] should be the same.
Creating dummy inputs
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
Training error: not enough values to unpack (expected 5, got 4)
Traceback (most recent call last):
  File "C:\Users\ignit\AppData\Local\Temp\ipykernel_32776\1050014336.py", line 4, in <module>
    trainer.train()
  File "c:\Users\ignit\AppData\Local\Programs\Python\Python311\Lib\site-packages\transformers\trainer.py", line 2245, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\ignit\AppData\Local\Programs\Python\Python311\Lib\site-packages\transformers\trainer.py", line 2556, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\ignit\AppData\Local\Programs\Python\Python311\Lib\site-packages\transformers\trainer.py", line 3718, in training_step
    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ignit\AppData\Local\Temp\ipykernel_32776\2839524216.py", line 4, in compute_loss
    outputs = model(**inputs)
              ^^^^^^^^^^^^^^^
  File "c:\Users\ignit\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\nn\modules\module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\ignit\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\nn\modules\module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\ignit\AppData\Local\Programs\Python\Python311\Lib\site-packages\accelerate\utils\operations.py", line 819, in forward
    return model_forward(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\ignit\AppData\Local\Programs\Python\Python311\Lib\site-packages\accelerate\utils\operations.py", line 807, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\ignit\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\amp\autocast_mode.py", line 44, in decorate_autocast
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\ignit\AppData\Local\Programs\Python\Python311\Lib\site-packages\peft\peft_model.py", line 1719, in forward
    return self.base_model(
           ^^^^^^^^^^^^^^^^
  File "c:\Users\ignit\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\nn\modules\module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\ignit\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\nn\modules\module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\ignit\AppData\Local\Programs\Python\Python311\Lib\site-packages\peft\tuners\tuners_utils.py", line 197, in forward
    return self.model.forward(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\ignit\AppData\Local\Programs\Python\Python311\Lib\site-packages\transformers\models\idefics3\modeling_idefics3.py", line 1198, in forward
    outputs = self.model(
              ^^^^^^^^^^^
  File "c:\Users\ignit\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\nn\modules\module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\ignit\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\nn\modules\module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\ignit\AppData\Local\Programs\Python\Python311\Lib\site-packages\transformers\models\idefics3\modeling_idefics3.py", line 975, in forward
    batch_size, num_images, num_channels, height, width = pixel_values.shape
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: not enough values to unpack (expected 5, got 4)
Training time: 0.03 minutes
