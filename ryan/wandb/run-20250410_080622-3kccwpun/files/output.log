PyTorch version: 2.6.0+cu124
CUDA available: True
CUDA version: 12.4
GPU: NVIDIA A100-SXM4-40GB
GPU memory: 42.47 GB
Using dtype: torch.bfloat16
Checking data directory: /content/drive/MyDrive/code/chartllama_data
Does directory exist? True
Files in directory:
  - candlestick_chart_100examples_simplified_qa.json
  - funnel_chart_100examples_simplified_qa.json
  - gantt_chart_100examples_simplified_qa.json
  - polar_chart_100examples_simplified_qa.json
  - heatmap_chart_100examples_simplified_qa.json
  - box_chart_100examples_simplified_qa.json
  - .gitattributes
  - ours.zip
  - scatter_chart_100examples_simplified_qa.json
  - extracted_images
  - .cache
Downloading ChartQA dataset...
/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning:
The secret `HF_TOKEN` does not exist in your Colab secrets.
To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.
You will be able to reuse this secret in all of your notebooks.
Please note that authentication is recommended but still optional to access public models or datasets.
  warnings.warn(
Loaded ChartQA test split with 2500 examples.

First ChartQA test sample structure:
{'image': <PIL.PngImagePlugin.PngImageFile image mode=RGBA size=850x600 at 0x7B2082AF6CD0>, 'query': 'How many food item is shown in the bar graph?', 'label': ['14'], 'human_or_machine': 0}

--- Loading Base Model for Evaluation ---
Base model 'HuggingFaceTB/SmolVLM-500M-Base' loaded successfully on cuda.

--- Loading Fine-tuned Model for Evaluation ---

--- Loading Fine-tuned Model for Evaluation ---
Checking for model weights in: ./smolvlm-chartllama-500-full-tuned/final_adapter
Checking for processor in: ./smolvlm-chartllama-500-full-tuned/final_processor
Loading processor from: ./smolvlm-chartllama-500-full-tuned/final_processor
Loading model from: ./smolvlm-chartllama-500-full-tuned/final_adapter
Fine-tuned model and processor loaded successfully from subdirs on cuda.

--- Loading LoRA Fine-tuned Model for Evaluation ---
Error: adapter_config.json not found in ./smolvlm-chartllama-500-lora-tuned or ./smolvlm-chartllama-500-lora-tuned/final_adapter
Checking for adapter config in: None
Checking for processor config in: ./smolvlm-chartllama-500-lora-tuned/final_processor
Error: Could not find required adapter/processor directories or config files.

Evaluation skipped. Check if ChartQA dataset and ALL THREE models/processors (base, fft, lora) loaded correctly.

--- Loading LoRA Fine-tuned Model for Evaluation ---
Adapter config found in subdirectory: ./smolvlm-chartllama-lora-tuned/final_adapter
Checking for adapter config in: ./smolvlm-chartllama-lora-tuned/final_adapter
Checking for processor config in: ./smolvlm-chartllama-lora-tuned/final_processor

Loading processor from: ./smolvlm-chartllama-lora-tuned/final_processor

Loading BASE model 'HuggingFaceTB/SmolVLM-500M-Base'...
Configuring base model for standard LoRA loading...
Base model loaded.
Loading LoRA adapter from: ./smolvlm-chartllama-lora-tuned/final_adapter
Error loading LoRA fine-tuned model/processor: Error(s) in loading state_dict for PeftModelForCausalLM:
	size mismatch for base_model.model.model.vision_model.encoder.layers.0.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.0.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.0.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.0.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.0.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.1.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.1.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.1.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.1.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.1.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.1.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.2.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.2.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.2.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.2.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.2.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.2.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.3.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.3.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.3.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.3.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.3.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.3.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.4.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.4.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.4.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.4.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.4.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.4.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.5.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.5.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.5.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.5.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.5.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.5.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.6.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.6.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.6.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.6.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.6.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.6.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.7.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.7.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.7.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.7.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.7.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.7.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.8.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.8.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.8.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.8.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.8.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.8.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.9.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.9.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.9.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.9.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.9.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.9.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.10.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.10.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.10.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.10.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.10.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.10.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.11.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.11.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.11.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.11.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.11.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.11.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.text_model.layers.0.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.0.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.0.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.0.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.0.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.0.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.0.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.0.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.0.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.0.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.0.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.0.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.0.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 8192]) from checkpoint, the shape in current model is torch.Size([16, 2560]).
	size mismatch for base_model.model.model.text_model.layers.0.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.1.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.1.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.1.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.1.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.1.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.1.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.1.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.1.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.1.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.1.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.1.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.1.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.1.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 8192]) from checkpoint, the shape in current model is torch.Size([16, 2560]).
	size mismatch for base_model.model.model.text_model.layers.1.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.2.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.2.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.2.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.2.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.2.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.2.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.2.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.2.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.2.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.2.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.2.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.2.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.2.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 8192]) from checkpoint, the shape in current model is torch.Size([16, 2560]).
	size mismatch for base_model.model.model.text_model.layers.2.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.3.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.3.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.3.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.3.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.3.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.3.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.3.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.3.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.3.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.3.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.3.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.3.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.3.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 8192]) from checkpoint, the shape in current model is torch.Size([16, 2560]).
	size mismatch for base_model.model.model.text_model.layers.3.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.4.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.4.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.4.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.4.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.4.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.4.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.4.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.4.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.4.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.4.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.4.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.4.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.4.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 8192]) from checkpoint, the shape in current model is torch.Size([16, 2560]).
	size mismatch for base_model.model.model.text_model.layers.4.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.5.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.5.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.5.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.5.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.5.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.5.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.5.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.5.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.5.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.5.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.5.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.5.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.5.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 8192]) from checkpoint, the shape in current model is torch.Size([16, 2560]).
	size mismatch for base_model.model.model.text_model.layers.5.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.6.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.6.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.6.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.6.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.6.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.6.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.6.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.6.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.6.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.6.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.6.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.6.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.6.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 8192]) from checkpoint, the shape in current model is torch.Size([16, 2560]).
	size mismatch for base_model.model.model.text_model.layers.6.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.7.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.7.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.7.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.7.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.7.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.7.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.7.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.7.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.7.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.7.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.7.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.7.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.7.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 8192]) from checkpoint, the shape in current model is torch.Size([16, 2560]).
	size mismatch for base_model.model.model.text_model.layers.7.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.8.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.8.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.8.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.8.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.8.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.8.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.8.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.8.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.8.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.8.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.8.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.8.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.8.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 8192]) from checkpoint, the shape in current model is torch.Size([16, 2560]).
	size mismatch for base_model.model.model.text_model.layers.8.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.9.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.9.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.9.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.9.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.9.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.9.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.9.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.9.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.9.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.9.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.9.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.9.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.9.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 8192]) from checkpoint, the shape in current model is torch.Size([16, 2560]).
	size mismatch for base_model.model.model.text_model.layers.9.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.10.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.10.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.10.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.10.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.10.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.10.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.10.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.10.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.10.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.10.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.10.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.10.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.10.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 8192]) from checkpoint, the shape in current model is torch.Size([16, 2560]).
	size mismatch for base_model.model.model.text_model.layers.10.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.11.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.11.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.11.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.11.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.11.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.11.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.11.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.11.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.11.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.11.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.11.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.11.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.11.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 8192]) from checkpoint, the shape in current model is torch.Size([16, 2560]).
	size mismatch for base_model.model.model.text_model.layers.11.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.12.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.12.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.12.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.12.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.12.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.12.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.12.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.12.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.12.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.12.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.12.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.12.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.12.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 8192]) from checkpoint, the shape in current model is torch.Size([16, 2560]).
	size mismatch for base_model.model.model.text_model.layers.12.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.13.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.13.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.13.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.13.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.13.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.13.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.13.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.13.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.13.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.13.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.13.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.13.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.13.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 8192]) from checkpoint, the shape in current model is torch.Size([16, 2560]).
	size mismatch for base_model.model.model.text_model.layers.13.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.14.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.14.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.14.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.14.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.14.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.14.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.14.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.14.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.14.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.14.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.14.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.14.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.14.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 8192]) from checkpoint, the shape in current model is torch.Size([16, 2560]).
	size mismatch for base_model.model.model.text_model.layers.14.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.15.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.15.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.15.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.15.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.15.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.15.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.15.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.15.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.15.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.15.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.15.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.15.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.15.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 8192]) from checkpoint, the shape in current model is torch.Size([16, 2560]).
	size mismatch for base_model.model.model.text_model.layers.15.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.16.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.16.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.16.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.16.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.16.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.16.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.16.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.16.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.16.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.16.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.16.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.16.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.16.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 8192]) from checkpoint, the shape in current model is torch.Size([16, 2560]).
	size mismatch for base_model.model.model.text_model.layers.16.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.17.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.17.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.17.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.17.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.17.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.17.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.17.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.17.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.17.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.17.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.17.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.17.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.17.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 8192]) from checkpoint, the shape in current model is torch.Size([16, 2560]).
	size mismatch for base_model.model.model.text_model.layers.17.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.18.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.18.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.18.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.18.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.18.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.18.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.18.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.18.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.18.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.18.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.18.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.18.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.18.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 8192]) from checkpoint, the shape in current model is torch.Size([16, 2560]).
	size mismatch for base_model.model.model.text_model.layers.18.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.19.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.19.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.19.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.19.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.19.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.19.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.19.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.19.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.19.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.19.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.19.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.19.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.19.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 8192]) from checkpoint, the shape in current model is torch.Size([16, 2560]).
	size mismatch for base_model.model.model.text_model.layers.19.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.20.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.20.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.20.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.20.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.20.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.20.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.20.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.20.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.20.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.20.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.20.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.20.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.20.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 8192]) from checkpoint, the shape in current model is torch.Size([16, 2560]).
	size mismatch for base_model.model.model.text_model.layers.20.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.21.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.21.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.21.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.21.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.21.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.21.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.21.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.21.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.21.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.21.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.21.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.21.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.21.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 8192]) from checkpoint, the shape in current model is torch.Size([16, 2560]).
	size mismatch for base_model.model.model.text_model.layers.21.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.22.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.22.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.22.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.22.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.22.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.22.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.22.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.22.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.22.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.22.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.22.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.22.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.22.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 8192]) from checkpoint, the shape in current model is torch.Size([16, 2560]).
	size mismatch for base_model.model.model.text_model.layers.22.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.23.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.23.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.23.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.23.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.23.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.23.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.23.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.23.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.23.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.23.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.23.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.23.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.23.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 8192]) from checkpoint, the shape in current model is torch.Size([16, 2560]).
	size mismatch for base_model.model.model.text_model.layers.23.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).

--- Loading Fine-tuned Model for Evaluation ---
Checking for model weights in: ./smolvlm-chartllama-full-tuned/final_adapter
Checking for processor in: ./smolvlm-chartllama-full-tuned/final_processor
Error: Could not find required model/processor directories or config files.
  Reason: Model path ('./smolvlm-chartllama-full-tuned/final_adapter') or its config.json missing.
  Reason: Processor path ('./smolvlm-chartllama-full-tuned/final_processor') or its processor_config.json missing.
Using device: cuda
