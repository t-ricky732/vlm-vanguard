
--- Configuration Summary ---
Model ID: HuggingFaceTB/SmolVLM-Base
Training Type: full-tuned
Using LoRA: False
Using QLoRA: False
Output Directory: ./smolvlm-chartllama-full-tuned
Device: cuda
Dtype: torch.bfloat16
Seed: 42
---------------------------

PyTorch version: 2.6.0+cu124
CUDA available: True
CUDA version: 12.4
GPU: NVIDIA A100-SXM4-40GB
GPU memory: 42.47 GB
Using compute dtype: torch.bfloat16
Loading processor...
/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning:
The secret `HF_TOKEN` does not exist in your Colab secrets.
To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.
You will be able to reuse this secret in all of your notebooks.
Please note that authentication is recommended but still optional to access public models or datasets.
  warnings.warn(
Processor loaded.

Starting raw data loading and preprocessing...
Loading raw ChartLlama data...

An error occurred during data loading/processing: name 'load_chartllama_data' is not defined
Traceback (most recent call last):
  File "<ipython-input-17-329289428d7c>", line 37, in <cell line: 0>
    raw_dataset = load_chartllama_data(DATA_DIR, sample_limit=SAMPLE_LIMIT)
                  ^^^^^^^^^^^^^^^^^^^^
NameError: name 'load_chartllama_data' is not defined

ERROR: Dataset preparation failed. Cannot proceed.
Configuring model for Full Fine-Tuning (dtype: torch.bfloat16)...

Loading model 'HuggingFaceTB/SmolVLM-Base' with config: {'torch_dtype': torch.bfloat16}
Moving model to device: cuda
Model loaded in 24.20 seconds
Model device: cuda:0

Model configured for Full Fine-tuning.
Enabling gradient checkpointing for Full Fine-Tuning.

Setting Training Arguments for full-tuned...
Batch Size (Train): 2, Grad Accum: 8
Effective Batch Size: 16
Learning Rate: 2e-05
/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(

Skipping Trainer initialization due to dataset loading/processing errors.

Training skipped due to errors in data loading, preprocessing, or trainer initialization.

--- Preparing for Evaluation ---

Downloading ChartQA dataset...
Limiting evaluation to 100 samples.
Loaded ChartQA test split with 100 examples.

First ChartQA test sample structure:
{'image': <PIL.PngImagePlugin.PngImageFile image mode=RGBA size=850x600 at 0x7A80000C4D90>, 'query': 'How many food item is shown in the bar graph?', 'label': ['14'], 'human_or_machine': 0}

Using accuracy function: calculate_relaxed_accuracy

Evaluation setup complete.
