
--- Configuration Summary ---
Model ID: HuggingFaceTB/SmolVLM-Base
Training Type: full-tuned
Using LoRA: False
Using QLoRA: False
Output Directory: ./smolvlm-chartllama-full-tuned
Device: cuda
Dtype: torch.bfloat16
Seed: 42
---------------------------

PyTorch version: 2.6.0+cu124
CUDA available: True
CUDA version: 12.4
GPU: NVIDIA A100-SXM4-40GB
GPU memory: 42.47 GB
Using compute dtype: torch.bfloat16
Loading processor...
/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning:
The secret `HF_TOKEN` does not exist in your Colab secrets.
To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.
You will be able to reuse this secret in all of your notebooks.
Please note that authentication is recommended but still optional to access public models or datasets.
  warnings.warn(
Processor loaded.

Starting raw data loading and preprocessing...
Loading raw ChartLlama data...
Found 7 JSON files in chartllama_data.
Reading JSONs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:02<00:00,  2.35it/s]

Initial processing complete.
Loaded 980 valid samples.
Skipped 0 samples due to missing data or images.
Encountered 0 errors during file processing.

Created final dataset with 980 samples.
Dataset features: {'id': Value(dtype='string', id=None), 'image': Image(mode=None, decode=True, id=None), 'question': Value(dtype='string', id=None), 'answer': Value(dtype='string', id=None)}
Raw data loaded: 980 samples.
Raw dataset columns: ['id', 'image', 'question', 'answer']
Preprocessing raw data (batch_size=1)...
Columns to remove after mapping: ['image', 'question', 'answer', 'id']
Preprocessing map step complete.
Columns in dataset AFTER mapping: ['pixel_values', 'pixel_attention_mask', 'input_ids', 'attention_mask', 'labels']
Filtering processed data...
Filtering complete. Kept 980/980 samples.
Splitting dataset...
Splits created - Train: 784, Val: 98, Test: 98
Saving preprocessed dataset splits to: ./processed_data
Preprocessed dataset saved.

Dataset preparation complete. Proceeding to next steps (Model Loading/Trainer).
Final columns in train_dataset for Trainer: ['pixel_values', 'pixel_attention_mask', 'input_ids', 'attention_mask', 'labels']
Configuring model for Full Fine-Tuning (dtype: torch.bfloat16)...

Loading model 'HuggingFaceTB/SmolVLM-Base' with config: {'torch_dtype': torch.bfloat16}
Moving model to device: cuda
Model loaded in 13.45 seconds
Model device: cuda:0

Model configured for Full Fine-tuning.
Enabling gradient checkpointing for Full Fine-Tuning.

Setting Training Arguments for full-tuned...
Batch Size (Train): 2, Grad Accum: 8
Effective Batch Size: 16
Learning Rate: 2e-05
/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
Initializing Trainer...
Using data collator: <class 'transformers.data.data_collator.DataCollatorWithPadding'>
Trainer initialized.

Starting training...
[34m[1mwandb[0m: [33mWARNING[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.

An error occurred during training: Caught ValueError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py", line 777, in convert_to_tensors
    tensor = as_tensor(value)
             ^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py", line 739, in as_tensor
    return torch.tensor(value)
           ^^^^^^^^^^^^^^^^^^^
ValueError: expected sequence of length 1220 at dim 1 (got 1226)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/worker.py", line 349, in _worker_loop
    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
           ^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py", line 55, in fetch
    return self.collate_fn(data)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/transformers/data/data_collator.py", line 272, in __call__
    batch = pad_without_fast_tokenizer_warning(
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/transformers/data/data_collator.py", line 67, in pad_without_fast_tokenizer_warning
    padded = tokenizer.pad(*pad_args, **pad_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py", line 3407, in pad
    return BatchEncoding(batch_outputs, tensor_type=return_tensors)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py", line 241, in __init__
    self.convert_to_tensors(tensor_type=tensor_type, prepend_batch_axis=prepend_batch_axis)
  File "/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py", line 793, in convert_to_tensors
    raise ValueError(
ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`labels` in this case) have excessive nesting (inputs type `list` where type `int` is expected).

Attempted to save trainer state after error.

--- Preparing for Evaluation ---

Downloading ChartQA dataset...
Limiting evaluation to 100 samples.
Loaded ChartQA test split with 100 examples.

First ChartQA test sample structure:
{'image': <PIL.PngImagePlugin.PngImageFile image mode=RGBA size=850x600 at 0x79202FBBC350>, 'query': 'How many food item is shown in the bar graph?', 'label': ['14'], 'human_or_machine': 0}

Using accuracy function: calculate_relaxed_accuracy

Evaluation setup complete.

Evaluation setup complete.
--------------------------------------------------
FORCE DELETING OLD PROCESSED DATA - ENSURE PATH IS CORRECT
--------------------------------------------------
Target path for deletion: /content/drive/MyDrive/code/processed_data
Successfully deleted: /content/drive/MyDrive/code/processed_data
Waiting 2 seconds to ensure filesystem sync...
--------------------------------------------------
