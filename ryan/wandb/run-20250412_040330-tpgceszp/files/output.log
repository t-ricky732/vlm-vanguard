
--- Configuration Summary ---
Model ID: HuggingFaceTB/SmolVLM-Base
Training Type: full-tuned
Using LoRA: False
Using QLoRA: False
Output Directory: ./smolvlm-chartllama-full-tuned
Device: cuda
Dtype: torch.bfloat16
Seed: 42
---------------------------

PyTorch version: 2.6.0+cu124
CUDA available: True
CUDA version: 12.4
GPU: NVIDIA A100-SXM4-40GB
GPU memory: 42.47 GB
Using compute dtype: torch.bfloat16
Loading processor...
/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning:
The secret `HF_TOKEN` does not exist in your Colab secrets.
To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.
You will be able to reuse this secret in all of your notebooks.
Please note that authentication is recommended but still optional to access public models or datasets.
  warnings.warn(
Processor loaded.

Loading preprocessed dataset from: ./processed_data
Preprocessed dataset loaded successfully.
Loaded splits - Train: 784, Val: 98, Test: 98
Columns in loaded train_dataset: ['id', 'image', 'question', 'answer', 'pixel_values', 'pixel_attention_mask', 'input_ids', 'attention_mask', 'labels']

Dataset preparation complete. Proceeding to next steps (Model Loading/Trainer).
Final columns in train_dataset for Trainer: ['id', 'image', 'question', 'answer', 'pixel_values', 'pixel_attention_mask', 'input_ids', 'attention_mask', 'labels']
Configuring model for Full Fine-Tuning (dtype: torch.bfloat16)...

Loading model 'HuggingFaceTB/SmolVLM-Base' with config: {'torch_dtype': torch.bfloat16}
Moving model to device: cuda
Model loaded in 2.76 seconds
Model device: cuda:0

Model configured for Full Fine-tuning.
Enabling gradient checkpointing for Full Fine-Tuning.

Setting Training Arguments for full-tuned...
Batch Size (Train): 2, Grad Accum: 8
Effective Batch Size: 16
Learning Rate: 2e-05
/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
Initializing Trainer...
Trainer initialized.

Starting training...
[34m[1mwandb[0m: [33mWARNING[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.

An error occurred during training: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/worker.py", line 349, in _worker_loop
    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
           ^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py", line 55, in fetch
    return self.collate_fn(data)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/transformers/data/data_collator.py", line 93, in default_data_collator
    return torch_default_data_collator(features)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/transformers/data/data_collator.py", line 159, in torch_default_data_collator
    batch[k] = torch.tensor([f[k] for f in features])
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Could not infer dtype of PngImageFile

Attempted to save trainer state after error.

--- Preparing for Evaluation ---

Downloading ChartQA dataset...
Limiting evaluation to 100 samples.
Loaded ChartQA test split with 100 examples.

First ChartQA test sample structure:
{'image': <PIL.PngImagePlugin.PngImageFile image mode=RGBA size=850x600 at 0x7F9713D32E90>, 'query': 'How many food item is shown in the bar graph?', 'label': ['14'], 'human_or_machine': 0}

Using accuracy function: calculate_relaxed_accuracy

Evaluation setup complete.

Evaluation setup complete.
Current Working Directory: /content/drive/MyDrive/code
Checking for/saving processed data at absolute path: /content/drive/MyDrive/code/processed_data
Current Working Directory: /content/drive/MyDrive/code
Checking for/saving processed data at absolute path: /content/drive/MyDrive/code/processed_data
WARNING: Force-deleting directory: /content/drive/MyDrive/code/processed_data
Directory deleted successfully.

Starting raw data loading and preprocessing...
Loading raw ChartLlama data...
Found 7 JSON files in chartllama_data.
Reading JSONs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:03<00:00,  2.27it/s]

Initial processing complete.
Loaded 980 valid samples.
Skipped 0 samples due to missing data or images.
Encountered 0 errors during file processing.

Created final dataset with 980 samples.
Dataset features: {'id': Value(dtype='string', id=None), 'image': Image(mode=None, decode=True, id=None), 'question': Value(dtype='string', id=None), 'answer': Value(dtype='string', id=None)}
Raw data loaded: 980 samples.
Raw dataset columns: ['id', 'image', 'question', 'answer']
Preprocessing raw data (batch_size=1)...
Columns to remove after mapping: ['image', 'question', 'answer']
Preprocessing map step complete.
Columns in dataset AFTER mapping: ['id', 'pixel_values', 'pixel_attention_mask', 'input_ids', 'attention_mask', 'labels']
Filtering processed data...
Filtering complete. Kept 980/980 samples.
Splitting dataset...
Splits created - Train: 784, Val: 98, Test: 98
Saving preprocessed dataset splits to: ./processed_data

An error occurred during data loading/processing: name 'PROCESED_DATA_DIR' is not defined
Traceback (most recent call last):
  File "<ipython-input-26-8b1d70542666>", line 106, in <cell line: 0>
    os.makedirs(PROCESED_DATA_DIR, exist_ok=True) # Ensure directory exists
                ^^^^^^^^^^^^^^^^^
NameError: name 'PROCESED_DATA_DIR' is not defined

ERROR: Dataset preparation failed. Cannot proceed.
Configuring model for Full Fine-Tuning (dtype: torch.bfloat16)...

Loading model 'HuggingFaceTB/SmolVLM-Base' with config: {'torch_dtype': torch.bfloat16}
Moving model to device: cuda
Model loaded in 2.27 seconds
Model device: cuda:0

Model configured for Full Fine-tuning.
Enabling gradient checkpointing for Full Fine-Tuning.

Setting Training Arguments for full-tuned...
Batch Size (Train): 2, Grad Accum: 8
Effective Batch Size: 16
Learning Rate: 2e-05
/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(

Skipping Trainer initialization due to dataset loading/processing errors.

Training skipped due to errors in data loading, preprocessing, or trainer initialization.

Starting raw data loading and preprocessing...
Loading raw ChartLlama data...
Found 7 JSON files in chartllama_data.
Reading JSONs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:01<00:00,  6.50it/s]

Initial processing complete.
Loaded 980 valid samples.
Skipped 0 samples due to missing data or images.
Encountered 0 errors during file processing.

Created final dataset with 980 samples.
Dataset features: {'id': Value(dtype='string', id=None), 'image': Image(mode=None, decode=True, id=None), 'question': Value(dtype='string', id=None), 'answer': Value(dtype='string', id=None)}
Raw data loaded: 980 samples.
Raw dataset columns: ['id', 'image', 'question', 'answer']
Preprocessing raw data (batch_size=1)...
Columns to remove after mapping: ['image', 'question', 'answer']
Error: Required dataset variables not found in memory: ['train_dataset', 'val_dataset', 'test_dataset', 'PROCESSED_DATA_DIR']. Cannot save.
You will likely need to re-run the full preprocessing block after ensuring the typo is fixed there.
