PyTorch version: 2.6.0+cu124
CUDA available: True
CUDA version: 12.4
GPU: NVIDIA A100-SXM4-40GB
GPU memory: 42.47 GB
Using dtype: torch.bfloat16
Loading processor...
/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning:
The secret `HF_TOKEN` does not exist in your Colab secrets.
To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.
You will be able to reuse this secret in all of your notebooks.
Please note that authentication is recommended but still optional to access public models or datasets.
  warnings.warn(
Configuring model for standard LoRA (dtype: torch.bfloat16)...
Loading model 'HuggingFaceTB/SmolVLM-256M-Base'...
Model loaded in 1.42 seconds
Configured for Full Fine-tuning.
Preparing model for LoRA training...
trainable params: 5,769,216 || all params: 262,254,144 || trainable%: 2.1999
Checking data directory: /content/drive/MyDrive/code/chartllama_data
Does directory exist? True
Files in directory:
  - candlestick_chart_100examples_simplified_qa.json
  - funnel_chart_100examples_simplified_qa.json
  - gantt_chart_100examples_simplified_qa.json
  - polar_chart_100examples_simplified_qa.json
  - heatmap_chart_100examples_simplified_qa.json
  - box_chart_100examples_simplified_qa.json
  - .gitattributes
  - ours.zip
  - scatter_chart_100examples_simplified_qa.json
  - extracted_images
  - .cache
Loading dataset using custom ChartDataset...
Reading JSONs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [05:08<00:00, 44.01s/it]
Total samples: 980
Training samples: 784
Validation samples: 98
Test samples: 98
--- Displaying Item Index: 654 ---
Original ID: ours_simplified_qa_37_6
Chart Type: heatmap
Question: How many smartphone features have a usage below 50 in week 3?
<image>...
Answer: 2
Image Path (from JSON): ours/heatmap_chart/png/heatmap_chart_100examples_37.png
--------------------------------------
/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
Initializing Trainer...
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Starting training...
[34m[1mwandb[0m: [33mWARNING[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
Initializing Trainer...
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Starting training...
Preparing model for LoRA training...
Applying prepare_model_for_kbit_training for QLoRA...
Preparing model for LoRA training...
Applying prepare_model_for_kbit_training for QLoRA...
Configuring model for QLoRA (8-bit quantization)...
Loading model 'HuggingFaceTB/SmolVLM-256M-Base'...
`low_cpu_mem_usage` was None, now default to True since model is quantized.
Model loaded in 1.57 seconds
Preparing model for LoRA training...
Applying prepare_model_for_kbit_training for QLoRA...
trainable params: 5,769,216 || all params: 262,254,144 || trainable%: 2.1999
/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
Initializing Trainer...
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Starting training...
/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Training completed in 57.73 minutes
Saving training results and final model/adapter...
***** train metrics *****
  epoch                    =        3.0
  total_flos               =  3537531GF
  train_loss               =     4.0303
  train_runtime            = 0:57:43.12
  train_samples_per_second =      0.679
  train_steps_per_second   =      0.042
Model adapter and processor saved to ./smolvlm-chartllama-256-lora-tuned

--- Loading Base Model for Evaluation ---
Base model 'HuggingFaceTB/SmolVLM-256M-Base' loaded successfully on cuda.

--- Loading Fine-tuned Model for Evaluation ---
/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(

--- Loading Fine-tuned Model for Evaluation ---
Checking for model weights in: ./smolvlm-chartllama-256-full-tuned/final_adapter
Checking for processor in: ./smolvlm-chartllama-256-full-tuned/final_processor
Loading processor from: ./smolvlm-chartllama-256-full-tuned/final_processor
Loading model from: ./smolvlm-chartllama-256-full-tuned/final_adapter
Fine-tuned model and processor loaded successfully from subdirs on cuda.

--- Loading Fine-tuned Model for Evaluation ---

--- Loading Fine-tuned Model for Evaluation ---
Checking for model weights in: ./smolvlm-chartllama-256-lora-tuned/final_adapter
Checking for processor in: ./smolvlm-chartllama-256-lora-tuned/final_processor
Error: Could not find required model/processor directories or config files.
  Reason: Model path ('./smolvlm-chartllama-256-lora-tuned/final_adapter') or its config.json missing.

--- Loading LoRA Fine-tuned Model for Evaluation ---
Adapter config found in subdirectory: ./smolvlm-chartllama-256-lora-tuned/final_adapter
Checking for adapter config in: ./smolvlm-chartllama-256-lora-tuned/final_adapter
Checking for processor config in: ./smolvlm-chartllama-256-lora-tuned/final_processor

Loading processor from: ./smolvlm-chartllama-256-lora-tuned/final_processor

Loading BASE model 'HuggingFaceTB/SmolVLM-256M-Base'...
Configuring base model for standard LoRA loading...
Base model loaded.
Loading LoRA adapter from: ./smolvlm-chartllama-256-lora-tuned/final_adapter
LoRA adapter loaded and applied. Final model ready on cuda:0.
Downloading ChartQA dataset...
Loaded ChartQA test split with 2500 examples.

First ChartQA test sample structure:
{'image': <PIL.PngImagePlugin.PngImageFile image mode=RGBA size=850x600 at 0x7AEA7FD87890>, 'query': 'How many food item is shown in the bar graph?', 'label': ['14'], 'human_or_machine': 0}
Using accuracy function: calculate_relaxed_accuracy

--- Loading Base Model for Evaluation ---
Base model 'HuggingFaceTB/SmolVLM-256M-Base' loaded successfully on cuda.

--- Loading Fine-tuned Model for Evaluation ---
Checking for model weights in: ./smolvlm-chartllama-256-full-tuned/final_adapter
Checking for processor in: ./smolvlm-chartllama-256-full-tuned/final_processor
Loading processor from: ./smolvlm-chartllama-256-full-tuned/final_processor
Loading model from: ./smolvlm-chartllama-256-full-tuned/final_adapter
Fine-tuned model and processor loaded successfully from subdirs on cuda.

--- Loading LoRA Fine-tuned Model for Evaluation ---
Adapter config found in subdirectory: ./smolvlm-chartllama-256-lora-tuned/final_adapter
Checking for adapter config in: ./smolvlm-chartllama-256-lora-tuned/final_adapter
Checking for processor config in: ./smolvlm-chartllama-256-lora-tuned/final_processor

Loading processor from: ./smolvlm-chartllama-256-lora-tuned/final_processor

Loading BASE model 'HuggingFaceTB/SmolVLM-256M-Base'...
Configuring base model for standard LoRA loading...
Base model loaded.
Loading LoRA adapter from: ./smolvlm-chartllama-256-lora-tuned/final_adapter
LoRA adapter loaded and applied. Final model ready on cuda:0.

Evaluation skipped. Check if ChartQA dataset and ALL THREE models/processors (base, fft, lora) loaded correctly.

Evaluation skipped. Check if ChartQA dataset and ALL THREE models/processors (base, fft, lora) loaded correctly.

No results to report. Evaluation may have been skipped or encountered errors.
Reason: Base model results missing.
Reason: FFT model results missing.
Reason: LoRA model results missing.
--- Verifying Loaded Components ---

ChartQA Dataset:
  - Variable exists. Type: <class 'datasets.arrow_dataset.Dataset'>
  - Is None: False
  - Length: 2500

Base Model:
  - Variable 'base_model' exists. Type: <class 'transformers.models.idefics3.modeling_idefics3.Idefics3ForConditionalGeneration'>
  - Is None: False
  - Variable 'base_processor' exists. Type: <class 'transformers.models.idefics3.processing_idefics3.Idefics3Processor'>
  - Is None: False

FFT Model:
  - Variable 'fft_model' exists. Type: <class 'peft.peft_model.PeftModelForCausalLM'>
  - Is None: False
  - Variable 'fft_processor' exists. Type: <class 'transformers.models.idefics3.processing_idefics3.Idefics3Processor'>
  - Is None: False

LoRA Model:
  - Variable 'lora_model' NOT defined.
  - Variable 'lora_processor' NOT defined.

--- Verification Complete ---
--- Verifying Loaded Components ---

ChartQA Dataset:
  - Variable exists. Type: <class 'datasets.arrow_dataset.Dataset'>
  - Is None: False
  - Length: 2500

Base Model:
  - Variable 'base_model' exists. Type: <class 'transformers.models.idefics3.modeling_idefics3.Idefics3ForConditionalGeneration'>
  - Is None: False
  - Variable 'base_processor' exists. Type: <class 'transformers.models.idefics3.processing_idefics3.Idefics3Processor'>
  - Is None: False

FFT Model:
  - Variable 'fft_model' exists. Type: <class 'peft.peft_model.PeftModelForCausalLM'>
  - Is None: False
  - Variable 'fft_processor' exists. Type: <class 'transformers.models.idefics3.processing_idefics3.Idefics3Processor'>
  - Is None: False

LoRA Model:
  - Variable 'lora_model' NOT defined.
  - Variable 'lora_processor' NOT defined.

--- Verification Complete ---

--- Loading LoRA Fine-tuned Model for Evaluation ---
Adapter config found in subdirectory: ./smolvlm-chartllama-256-lora-tuned/final_adapter
Checking for adapter config in: ./smolvlm-chartllama-256-lora-tuned/final_adapter
Checking for processor config in: ./smolvlm-chartllama-256-lora-tuned/final_processor

Loading processor from: ./smolvlm-chartllama-256-lora-tuned/final_processor

Loading BASE model 'HuggingFaceTB/SmolVLM-256M-Base'...
Configuring base model for standard LoRA loading...
Base model loaded.
Loading LoRA adapter from: ./smolvlm-chartllama-256-lora-tuned/final_adapter
LoRA adapter loaded and applied. Final model ready on cuda:0.

--- Loading LoRA Fine-tuned Model for Evaluation ---
Adapter config found in subdirectory: ./smolvlm-chartllama-256-lora-tuned/final_adapter
Checking for adapter config in: ./smolvlm-chartllama-256-lora-tuned/final_adapter
Checking for processor config in: ./smolvlm-chartllama-256-lora-tuned/final_processor

Loading processor from: ./smolvlm-chartllama-256-lora-tuned/final_processor

Loading BASE model 'HuggingFaceTB/SmolVLM-256M-Base'...
Configuring base model for standard LoRA loading...
Base model loaded.
Loading LoRA adapter from: ./smolvlm-chartllama-256-lora-tuned/final_adapter
LoRA adapter loaded and applied. Final model ready on cuda:0.
--- Verifying Loaded Components ---

ChartQA Dataset:
  - Variable exists. Type: <class 'datasets.arrow_dataset.Dataset'>
  - Is None: False
  - Length: 2500

Base Model:
  - Variable 'base_model' exists. Type: <class 'transformers.models.idefics3.modeling_idefics3.Idefics3ForConditionalGeneration'>
  - Is None: False
  - Variable 'base_processor' exists. Type: <class 'transformers.models.idefics3.processing_idefics3.Idefics3Processor'>
  - Is None: False

FFT Model:
  - Variable 'fft_model' exists. Type: <class 'peft.peft_model.PeftModelForCausalLM'>
  - Is None: False
  - Variable 'fft_processor' exists. Type: <class 'transformers.models.idefics3.processing_idefics3.Idefics3Processor'>
  - Is None: False

LoRA Model:
  - Variable 'lora_model' exists. Type: <class 'peft.peft_model.PeftModelForCausalLM'>
  - Is None: False
  - Variable 'lora_processor' exists. Type: <class 'transformers.models.idefics3.processing_idefics3.Idefics3Processor'>
  - Is None: False

--- Verification Complete ---

--- Starting Evaluation on 100 ChartQA Samples (Base vs FFT vs LoRA) ---
/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(

--- Evaluation Results ---
Evaluated on: 100 samples
Metric: Relaxed Accuracy (calculate_relaxed_accuracy)

Base Model (HuggingFaceTB/SmolVLM-256M-Base): 0.0300
FFT Model (smolvlm-chartllama-256-full-tuned): 0.0900
LoRA Model (smolvlm-chartllama-256-lora-tuned): 0.0900

--- Example Predictions (First 5 Evaluated) ---

Sample Index: 0
  Q: How many food item is shown in the bar graph?
  GT: 14
  Base Pred: 1000 (Incorrect)
  FFT Pred:  1000 (Incorrect)
  LoRA Pred: 1000 (Incorrect)

Sample Index: 1
  Q: What is the difference in value between Lamb and Corn?
  GT: 0.57
  Base Pred: 1000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.00 (Incorrect)
  FFT Pred:  1000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000 (Incorrect)
  LoRA Pred: 1000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000 (Incorrect)

Sample Index: 2
  Q: How many bars are shown in the chart?
  GT: 3
  Base Pred: 0.1% 0.2% 0.3% 0.4% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0. (Incorrect)
  FFT Pred:  0.21% 0.38% 0.4% 0.3% 0.4% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% (Incorrect)
  LoRA Pred: 0.21% 0.38% 0.4% 0.3% 0.4% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% (Incorrect)

Sample Index: 3
  Q: Is the sum value of Madagascar more then Fiji?
  GT: No
  Base Pred: 0.1% 0.2% 0.3% 0.4% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0. (Incorrect)
  FFT Pred:  0.21% 0.38% 0.4% 0.3% 0.4% 0.3% 0.4% 0.3% 0.4% 0.3% 0.4% 0.3% 0.4% 0.3% 0.4% 0.3% 0.4% 0.3% 0.4% 0.3% 0.4% 0.3% 0.4% 0.3% 0.4% (Incorrect)
  LoRA Pred: 0.21% 0.38% 0.4% 0.3% 0.4% 0.3% 0.4% 0.3% 0.4% 0.3% 0.4% 0.3% 0.4% 0.3% 0.4% 0.3% 0.4% 0.3% 0.4% 0.3% 0.4% 0.3% 0.4% 0.3% 0.4% (Incorrect)

Sample Index: 4
  Q: What's the value of the lowest bar?
  GT: 23
  Base Pred: 0.000000 (Incorrect)
  FFT Pred:  0.00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000 (Incorrect)
  LoRA Pred: 0.00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000 (Incorrect)

--- Evaluation DataFrame (First 10 rows) ---

Releasing models from memory...
  - base_model deleted.
  - base_processor deleted.
  - fft_model deleted.
  - fft_processor deleted.
  - lora_model deleted.
  - lora_processor deleted.
  - chartqa_test_dataset deleted.
  - base_model_for_lora deleted.
  - Emptying CUDA cache...
Cleanup complete.
Downloading ChartQA dataset...
Loaded ChartQA test split with 2500 examples.

First ChartQA test sample structure:
{'image': <PIL.PngImagePlugin.PngImageFile image mode=RGBA size=850x600 at 0x7AEA7C0ECCD0>, 'query': 'How many food item is shown in the bar graph?', 'label': ['14'], 'human_or_machine': 0}
Using accuracy function: calculate_relaxed_accuracy

--- Loading Base Model for Evaluation ---
Base model 'HuggingFaceTB/SmolVLM-256M-Base' loaded successfully on cuda.

--- Loading Fine-tuned Model for Evaluation ---
Checking for model weights in: ./smolvlm-chartllama-256-full-tuned/final_adapter
Checking for processor in: ./smolvlm-chartllama-256-full-tuned/final_processor
Loading processor from: ./smolvlm-chartllama-256-full-tuned/final_processor
Loading model from: ./smolvlm-chartllama-256-full-tuned/final_adapter
Fine-tuned model and processor loaded successfully from subdirs on cuda.

--- Loading LoRA Fine-tuned Model for Evaluation ---
Adapter config found in subdirectory: ./smolvlm-chartllama-256-lora-tuned/final_adapter
Checking for adapter config in: ./smolvlm-chartllama-256-lora-tuned/final_adapter
Checking for processor config in: ./smolvlm-chartllama-256-lora-tuned/final_processor

Loading processor from: ./smolvlm-chartllama-256-lora-tuned/final_processor

Loading BASE model 'HuggingFaceTB/SmolVLM-256M-Base'...
Configuring base model for standard LoRA loading...
Base model loaded.
Loading LoRA adapter from: ./smolvlm-chartllama-256-lora-tuned/final_adapter
LoRA adapter loaded and applied. Final model ready on cuda:0.

--- Starting Evaluation on 100 ChartQA Samples (Base vs FFT vs LoRA) ---
/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(

--- Evaluation Results ---
Evaluated on: 100 samples
Metric: Relaxed Accuracy (calculate_relaxed_accuracy)

Base Model (HuggingFaceTB/SmolVLM-256M-Base): 0.0300
FFT Model (smolvlm-chartllama-256-full-tuned): 0.0500
LoRA Model (smolvlm-chartllama-256-lora-tuned): 0.0900

--- Example Predictions (First 5 Evaluated) ---

--------------------- Sample Index: 0 ---------------------
  Q: How many food item is shown in the bar graph?
  GT: 14
  Base Pred: 1000 (Incorrect)
  FFT Pred:  1000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000 (Incorrect)
  LoRA Pred: 1000 (Incorrect)

--------------------- Sample Index: 1 ---------------------
  Q: What is the difference in value between Lamb and Corn?
  GT: 0.57
  Base Pred: 1000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.00 (Incorrect)
  FFT Pred:  103.13 103.13 103.13 103.13 103.13 103.13 103.13 103.13 103.13 103.13 103.13 103.13 103.13 103.13 103.13 103.13 103.13 103.13 1 (Incorrect)
  LoRA Pred: 1000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000 (Incorrect)

--------------------- Sample Index: 2 ---------------------
  Q: How many bars are shown in the chart?
  GT: 3
  Base Pred: 0.1% 0.2% 0.3% 0.4% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0. (Incorrect)
  FFT Pred:  0.21% (Incorrect)
  LoRA Pred: 0.21% 0.38% 0.4% 0.3% 0.4% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% (Incorrect)

--------------------- Sample Index: 3 ---------------------
  Q: Is the sum value of Madagascar more then Fiji?
  GT: No
  Base Pred: 0.1% 0.2% 0.3% 0.4% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0. (Incorrect)
  FFT Pred:  0.4% (Incorrect)
  LoRA Pred: 0.21% 0.38% 0.4% 0.3% 0.4% 0.3% 0.4% 0.3% 0.4% 0.3% 0.4% 0.3% 0.4% 0.3% 0.4% 0.3% 0.4% 0.3% 0.4% 0.3% 0.4% 0.3% 0.4% 0.3% 0.4% (Incorrect)

--------------------- Sample Index: 4 ---------------------
  Q: What's the value of the lowest bar?
  GT: 23
  Base Pred: 0.000000 (Incorrect)
  FFT Pred:  100.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000. (Incorrect)
  LoRA Pred: 0.00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000 (Incorrect)

----------------------------------------------------------

--- Evaluation DataFrame (First 10 rows) ---
/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
Initializing Trainer...
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Starting training...
/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
