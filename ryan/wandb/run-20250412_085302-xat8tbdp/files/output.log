WandB initialized.
--- Environment Setup Complete ---

--- Defining Data Loading Function ---

--- Loading and Splitting Data ---
Found 7 JSON files in chartllama_data.
Loaded 980 raw examples. Skipped 0 invalid samples.

Raw data loaded and split.
Train samples: 882
Eval samples: 98

Sample train data point (raw):
{'id': 'ours_simplified_qa_59_0', 'question': 'What is the theme of the chart?', 'answer': 'Variation in the Consumer Price Index (CPI)', 'image_path': 'chartllama_data/extracted_images/ours/candlestick_chart/png/candlestick_chart_100examples_59.png'}

--- Loading Model and Processor ---
/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning:
The secret `HF_TOKEN` does not exist in your Colab secrets.
To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.
You will be able to reuse this secret in all of your notebooks.
Please note that authentication is recommended but still optional to access public models or datasets.
  warnings.warn(
Processor loaded.
Using standard attention implementation: eager
Configuring model for QLoRA (4-bit)...
Loading model 'HuggingFaceTB/SmolVLM-Base'...
Model loaded. Device: cuda:0. Footprint: 1.43 GB

Applying PEFT/LoRA configuration...
Preparing model for K-bit training (QLoRA)...
trainable params: 10,946,584 || all params: 2,257,219,464 || trainable%: 0.4850

--- Defining Formatting and Collator ---
Custom Collator defined.

--- Configuring Training Arguments ---
/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
SFTConfig set.

--- Initializing SFTTrainer ---
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
SFTTrainer initialized.

--- Starting Training ---
Calling trainer.train()...
[34m[1mwandb[0m: [33mWARNING[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.
  warnings.warn("Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.")
/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.
  warnings.warn("Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.")
/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.
  warnings.warn("Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.")
Training finished in 72.63 minutes.

Saving final model/adapter...
/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.
  warnings.warn("Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.")
Model/Adapter saved to ./smolvlm-chartllama-sft-refactored-qlora-tuned/final_checkpoint
Saving processor to ./smolvlm-chartllama-sft-refactored-qlora-tuned/final_processor...
Processor saved.
***** train metrics *****
  total_flos               = 50880175GF
  train_loss               =     0.3287
  train_runtime            = 1:12:36.73
  train_samples_per_second =      0.607
  train_steps_per_second   =      0.038

ERROR during training: [Errno 2] No such file or directory: './smolvlm-chartllama-sft-refactored-qlora-tuned/trainer_state.json'
Traceback (most recent call last):
  File "<ipython-input-20-1f1ba8a5b0c9>", line 30, in <cell line: 0>
    trainer.save_state()
  File "/usr/local/lib/python3.11/dist-packages/transformers/trainer_pt_utils.py", line 1102, in save_state
    self.state.save_to_json(path)
  File "/usr/local/lib/python3.11/dist-packages/transformers/trainer_callback.py", line 147, in save_to_json
    with open(json_path, "w", encoding="utf-8") as f:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: './smolvlm-chartllama-sft-refactored-qlora-tuned/trainer_state.json'

--- Setting up Evaluation ---
Loading processor from ./smolvlm-chartllama-sft-refactored-qlora-tuned/final_processor...
Error loading model/processor for evaluation: Repo id must be in the form 'repo_name' or 'namespace/repo_name': './smolvlm-chartllama-sft-refactored-qlora-tuned/final_processor'. Use `repo_type` argument if needed.
Traceback (most recent call last):
  File "/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py", line 424, in cached_files
    hf_hub_download(
  File "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': './smolvlm-chartllama-sft-refactored-qlora-tuned/final_processor'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<ipython-input-21-4d45c5a1b61c>", line 32, in <cell line: 0>
    eval_processor = AutoProcessor.from_pretrained(SAVED_PROCESSOR_PATH, trust_remote_code=True)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/transformers/models/auto/processing_auto.py", line 273, in from_pretrained
    processor_config_file = cached_file(pretrained_model_name_or_path, PROCESSOR_NAME, **cached_file_kwargs)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py", line 266, in cached_file
    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py", line 470, in cached_files
    resolved_files = [
                     ^
  File "/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py", line 471, in <listcomp>
    _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision) for filename in full_filenames
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py", line 134, in _get_cache_file_to_return
    resolved_file = try_to_load_from_cache(path_or_repo_id, full_filename, cache_dir=cache_dir, revision=revision)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': './smolvlm-chartllama-sft-refactored-qlora-tuned/final_processor'. Use `repo_type` argument if needed.
--- Evaluation Setup Complete ---

--- Verifying Saved Files ---
Checking contents of OUTPUT_DIR: ./smolvlm-chartllama-sft-refactored-qlora-tuned
total 1
-rw------- 1 root root 748 Apr 12 08:48 trainer_state.json

Checking contents of FINAL_PROCESSOR_DIR: ./smolvlm-chartllama-sft-refactored-qlora-tuned/final_processor
ERROR: Directory './smolvlm-chartllama-sft-refactored-qlora-tuned/final_processor' does not exist or is not a directory.
--- Verification Complete ---
Config Set: Training='qlora-tuned', Model='HuggingFaceTB/SmolVLM-Base'
Local Output Dir (for training): '/content/smolvlm-chartllama-sft-refactored-qlora-tuned'
Final Drive Output Dir (for copying): '/content/drive/MyDrive/code/smolvlm-chartllama-sft-refactored-qlora-tuned'
Device='cuda', Compute Dtype='torch.bfloat16', Use LoRA='True', Use QLoRA='True'
