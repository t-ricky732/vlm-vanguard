PyTorch version: 2.6.0+cu124
CUDA available: True
CUDA version: 12.4
GPU: NVIDIA A100-SXM4-40GB
GPU memory: 42.47 GB
Using dtype: torch.bfloat16
Checking data directory: /content/drive/MyDrive/code/chartllama_data
Does directory exist? True
Files in directory:
  - candlestick_chart_100examples_simplified_qa.json
  - funnel_chart_100examples_simplified_qa.json
  - gantt_chart_100examples_simplified_qa.json
  - polar_chart_100examples_simplified_qa.json
  - heatmap_chart_100examples_simplified_qa.json
  - box_chart_100examples_simplified_qa.json
  - .gitattributes
  - ours.zip
  - scatter_chart_100examples_simplified_qa.json
  - extracted_images
  - .cache
Loading processor...
/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning:
The secret `HF_TOKEN` does not exist in your Colab secrets.
To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.
You will be able to reuse this secret in all of your notebooks.
Please note that authentication is recommended but still optional to access public models or datasets.
  warnings.warn(
Loading dataset using custom ChartDataset...
Reading JSONs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [02:32<00:00, 21.77s/it]
Total samples: 980
Training samples: 784
Validation samples: 98
Test samples: 98
Configuring model for standard LoRA (dtype: torch.bfloat16)...
Loading model 'HuggingFaceTB/SmolVLM-500M-Base'...
Model loaded in 8.15 seconds
Configured for Full Fine-tuning.
/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
Initializing Trainer...
Starting training...
[34m[1mwandb[0m: [33mWARNING[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
Training completed in 38.21 minutes
Saving training results and final model/adapter...
***** train metrics *****
  epoch                    =        3.0
  total_flos               =  6972194GF
  train_loss               =     2.1856
  train_runtime            = 0:38:12.18
  train_samples_per_second =      1.026
  train_steps_per_second   =      0.064
Model adapter and processor saved to ./smolvlm-chartllama-500-full-tuned
Loading processor...
Loading dataset using custom ChartDataset...
Reading JSONs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:14<00:00,  2.02s/it]
Total samples: 980
Training samples: 784
Validation samples: 98
Test samples: 98
Configuring model for QLoRA (8-bit quantization)...
Loading model 'HuggingFaceTB/SmolVLM-500M-Base'...
`low_cpu_mem_usage` was None, now default to True since model is quantized.
Model loaded in 2.11 seconds
Preparing model for LoRA training...
Applying prepare_model_for_kbit_training for QLoRA...
trainable params: 9,568,256 || all params: 517,050,560 || trainable%: 1.8505
/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
Initializing Trainer...
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Starting training...
/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Starting training...
Loading processor...
Loading dataset using custom ChartDataset...
Reading JSONs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:14<00:00,  2.08s/it]
Total samples: 980
Training samples: 784
Validation samples: 98
Test samples: 98
Configuring model for QLoRA (8-bit quantization)...
Loading model 'HuggingFaceTB/SmolVLM-500M-Base'...
`low_cpu_mem_usage` was None, now default to True since model is quantized.
Model loaded in 1.28 seconds
Preparing model for LoRA training...
Applying prepare_model_for_kbit_training for QLoRA...
trainable params: 9,568,256 || all params: 517,050,560 || trainable%: 1.8505
/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
Initializing Trainer...
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Starting training...
/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Training completed in 57.68 minutes
Saving training results and final model/adapter...
***** train metrics *****
  epoch                    =        3.0
  total_flos               =  7117413GF
  train_loss               =     3.2894
  train_runtime            = 0:57:40.55
  train_samples_per_second =       0.68
  train_steps_per_second   =      0.042
Model adapter and processor saved to ./smolvlm-chartllama-500-lora-tuned
Downloading ChartQA dataset...
Loaded ChartQA test split with 2500 examples.

First ChartQA test sample structure:
{'image': <PIL.PngImagePlugin.PngImageFile image mode=RGBA size=850x600 at 0x7C6201464710>, 'query': 'How many food item is shown in the bar graph?', 'label': ['14'], 'human_or_machine': 0}
Using accuracy function: calculate_relaxed_accuracy

--- Loading Base Model for Evaluation ---
Base model 'HuggingFaceTB/SmolVLM-500M-Base' loaded successfully on cuda.
/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
Downloading ChartQA dataset...
Loaded ChartQA test split with 2500 examples.

First ChartQA test sample structure:
{'image': <PIL.PngImagePlugin.PngImageFile image mode=RGBA size=850x600 at 0x7C6235491FD0>, 'query': 'How many food item is shown in the bar graph?', 'label': ['14'], 'human_or_machine': 0}
Using accuracy function: calculate_relaxed_accuracy

--- Loading Base Model for Evaluation ---
Base model 'HuggingFaceTB/SmolVLM-500M-Base' loaded successfully on cuda.

--- Loading Fine-tuned Model for Evaluation ---
Checking for model weights in: ./smolvlm-chartllama-500-full-tuned/final_adapter
Checking for processor in: ./smolvlm-chartllama-500-full-tuned/final_processor
Loading processor from: ./smolvlm-chartllama-500-full-tuned/final_processor
Loading model from: ./smolvlm-chartllama-500-full-tuned/final_adapter
Fine-tuned model and processor loaded successfully from subdirs on cuda.

--- Loading LoRA Fine-tuned Model for Evaluation ---
Adapter config found in subdirectory: ./smolvlm-chartllama-lora-tuned/final_adapter
Checking for adapter config in: ./smolvlm-chartllama-lora-tuned/final_adapter
Checking for processor config in: ./smolvlm-chartllama-lora-tuned/final_processor

Loading processor from: ./smolvlm-chartllama-lora-tuned/final_processor

Loading BASE model 'HuggingFaceTB/SmolVLM-500M-Base'...
Configuring base model for standard LoRA loading...
Base model loaded.
Loading LoRA adapter from: ./smolvlm-chartllama-lora-tuned/final_adapter
Error loading LoRA fine-tuned model/processor: Error(s) in loading state_dict for PeftModelForCausalLM:
	size mismatch for base_model.model.model.vision_model.encoder.layers.0.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.0.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.0.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.0.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.0.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.1.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.1.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.1.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.1.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.1.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.1.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.2.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.2.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.2.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.2.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.2.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.2.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.3.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.3.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.3.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.3.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.3.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.3.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.4.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.4.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.4.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.4.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.4.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.4.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.5.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.5.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.5.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.5.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.5.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.5.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.6.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.6.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.6.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.6.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.6.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.6.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.7.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.7.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.7.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.7.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.7.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.7.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.8.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.8.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.8.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.8.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.8.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.8.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.9.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.9.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.9.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.9.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.9.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.9.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.10.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.10.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.10.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.10.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.10.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.10.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.11.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.11.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.11.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.11.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.11.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.11.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.text_model.layers.0.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.0.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.0.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.0.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.0.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.0.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.0.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.0.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.0.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.0.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.0.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.0.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.0.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 8192]) from checkpoint, the shape in current model is torch.Size([16, 2560]).
	size mismatch for base_model.model.model.text_model.layers.0.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.1.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.1.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.1.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.1.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.1.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.1.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.1.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.1.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.1.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.1.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.1.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.1.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.1.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 8192]) from checkpoint, the shape in current model is torch.Size([16, 2560]).
	size mismatch for base_model.model.model.text_model.layers.1.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.2.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.2.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.2.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.2.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.2.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.2.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.2.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.2.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.2.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.2.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.2.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.2.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.2.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 8192]) from checkpoint, the shape in current model is torch.Size([16, 2560]).
	size mismatch for base_model.model.model.text_model.layers.2.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.3.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.3.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.3.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.3.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.3.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.3.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.3.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.3.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.3.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.3.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.3.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.3.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.3.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 8192]) from checkpoint, the shape in current model is torch.Size([16, 2560]).
	size mismatch for base_model.model.model.text_model.layers.3.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.4.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.4.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.4.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.4.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.4.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.4.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.4.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.4.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.4.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.4.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.4.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.4.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.4.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 8192]) from checkpoint, the shape in current model is torch.Size([16, 2560]).
	size mismatch for base_model.model.model.text_model.layers.4.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.5.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.5.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.5.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.5.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.5.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.5.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.5.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.5.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.5.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.5.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.5.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.5.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.5.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 8192]) from checkpoint, the shape in current model is torch.Size([16, 2560]).
	size mismatch for base_model.model.model.text_model.layers.5.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.6.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.6.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.6.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.6.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.6.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.6.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.6.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.6.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.6.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.6.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.6.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.6.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.6.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 8192]) from checkpoint, the shape in current model is torch.Size([16, 2560]).
	size mismatch for base_model.model.model.text_model.layers.6.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.7.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.7.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.7.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.7.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.7.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.7.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.7.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.7.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.7.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.7.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.7.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.7.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.7.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 8192]) from checkpoint, the shape in current model is torch.Size([16, 2560]).
	size mismatch for base_model.model.model.text_model.layers.7.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.8.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.8.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.8.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.8.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.8.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.8.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.8.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.8.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.8.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.8.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.8.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.8.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.8.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 8192]) from checkpoint, the shape in current model is torch.Size([16, 2560]).
	size mismatch for base_model.model.model.text_model.layers.8.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.9.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.9.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.9.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.9.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.9.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.9.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.9.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.9.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.9.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.9.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.9.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.9.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.9.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 8192]) from checkpoint, the shape in current model is torch.Size([16, 2560]).
	size mismatch for base_model.model.model.text_model.layers.9.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.10.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.10.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.10.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.10.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.10.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.10.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.10.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.10.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.10.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.10.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.10.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.10.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.10.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 8192]) from checkpoint, the shape in current model is torch.Size([16, 2560]).
	size mismatch for base_model.model.model.text_model.layers.10.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.11.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.11.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.11.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.11.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.11.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.11.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.11.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.11.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.11.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.11.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.11.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.11.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.11.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 8192]) from checkpoint, the shape in current model is torch.Size([16, 2560]).
	size mismatch for base_model.model.model.text_model.layers.11.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.12.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.12.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.12.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.12.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.12.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.12.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.12.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.12.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.12.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.12.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.12.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.12.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.12.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 8192]) from checkpoint, the shape in current model is torch.Size([16, 2560]).
	size mismatch for base_model.model.model.text_model.layers.12.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.13.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.13.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.13.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.13.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.13.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.13.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.13.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.13.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.13.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.13.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.13.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.13.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.13.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 8192]) from checkpoint, the shape in current model is torch.Size([16, 2560]).
	size mismatch for base_model.model.model.text_model.layers.13.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.14.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.14.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.14.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.14.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.14.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.14.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.14.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.14.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.14.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.14.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.14.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.14.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.14.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 8192]) from checkpoint, the shape in current model is torch.Size([16, 2560]).
	size mismatch for base_model.model.model.text_model.layers.14.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.15.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.15.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.15.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.15.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.15.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.15.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.15.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.15.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.15.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.15.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.15.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.15.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.15.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 8192]) from checkpoint, the shape in current model is torch.Size([16, 2560]).
	size mismatch for base_model.model.model.text_model.layers.15.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.16.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.16.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.16.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.16.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.16.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.16.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.16.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.16.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.16.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.16.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.16.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.16.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.16.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 8192]) from checkpoint, the shape in current model is torch.Size([16, 2560]).
	size mismatch for base_model.model.model.text_model.layers.16.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.17.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.17.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.17.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.17.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.17.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.17.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.17.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.17.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.17.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.17.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.17.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.17.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.17.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 8192]) from checkpoint, the shape in current model is torch.Size([16, 2560]).
	size mismatch for base_model.model.model.text_model.layers.17.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.18.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.18.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.18.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.18.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.18.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.18.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.18.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.18.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.18.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.18.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.18.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.18.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.18.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 8192]) from checkpoint, the shape in current model is torch.Size([16, 2560]).
	size mismatch for base_model.model.model.text_model.layers.18.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.19.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.19.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.19.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.19.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.19.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.19.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.19.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.19.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.19.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.19.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.19.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.19.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.19.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 8192]) from checkpoint, the shape in current model is torch.Size([16, 2560]).
	size mismatch for base_model.model.model.text_model.layers.19.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.20.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.20.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.20.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.20.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.20.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.20.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.20.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.20.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.20.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.20.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.20.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.20.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.20.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 8192]) from checkpoint, the shape in current model is torch.Size([16, 2560]).
	size mismatch for base_model.model.model.text_model.layers.20.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.21.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.21.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.21.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.21.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.21.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.21.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.21.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.21.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.21.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.21.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.21.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.21.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.21.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 8192]) from checkpoint, the shape in current model is torch.Size([16, 2560]).
	size mismatch for base_model.model.model.text_model.layers.21.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.22.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.22.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.22.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.22.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.22.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.22.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.22.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.22.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.22.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.22.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.22.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.22.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.22.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 8192]) from checkpoint, the shape in current model is torch.Size([16, 2560]).
	size mismatch for base_model.model.model.text_model.layers.22.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.23.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.23.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.23.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.23.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.23.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.23.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.23.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.23.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.23.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.23.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.23.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.23.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.23.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 8192]) from checkpoint, the shape in current model is torch.Size([16, 2560]).
	size mismatch for base_model.model.model.text_model.layers.23.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).

--- Loading LoRA Fine-tuned Model for Evaluation ---
Adapter config found in subdirectory: ./smolvlm-chartllama-500-lora-tuned/final_adapter
Checking for adapter config in: ./smolvlm-chartllama-500-lora-tuned/final_adapter
Checking for processor config in: ./smolvlm-chartllama-500-lora-tuned/final_processor

Loading processor from: ./smolvlm-chartllama-500-lora-tuned/final_processor

Loading BASE model 'HuggingFaceTB/SmolVLM-500M-Base'...
Configuring base model for standard LoRA loading...
Base model loaded.
Loading LoRA adapter from: ./smolvlm-chartllama-500-lora-tuned/final_adapter
LoRA adapter loaded and applied. Final model ready on cuda:0.

--- Loading Fine-tuned Model for Evaluation ---
Checking for model weights in: ./smolvlm-chartllama-500-full-tuned/final_adapter
Checking for processor in: ./smolvlm-chartllama-500-full-tuned/final_processor
Loading processor from: ./smolvlm-chartllama-500-full-tuned/final_processor
Loading model from: ./smolvlm-chartllama-500-full-tuned/final_adapter
Fine-tuned model and processor loaded successfully from subdirs on cuda.

--- Loading LoRA Fine-tuned Model for Evaluation ---
Adapter config found in subdirectory: ./smolvlm-chartllama-500-lora-tuned/final_adapter
Checking for adapter config in: ./smolvlm-chartllama-500-lora-tuned/final_adapter
Checking for processor config in: ./smolvlm-chartllama-500-lora-tuned/final_processor

Loading processor from: ./smolvlm-chartllama-500-lora-tuned/final_processor

Loading BASE model 'HuggingFaceTB/SmolVLM-500M-Base'...
Configuring base model for standard LoRA loading...
Base model loaded.
Loading LoRA adapter from: ./smolvlm-chartllama-500-lora-tuned/final_adapter
LoRA adapter loaded and applied. Final model ready on cuda:0.

--- Starting Evaluation on 100 ChartQA Samples (Base vs FFT vs LoRA) ---
/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(

--- Evaluation Results ---
Evaluated on: 100 samples
Metric: Relaxed Accuracy (calculate_relaxed_accuracy)

Base Model (HuggingFaceTB/SmolVLM-500M-Base): 0.0200
FFT Model (smolvlm-chartllama-500-full-tuned): 0.0800
LoRA Model (smolvlm-chartllama-500-lora-tuned): 0.0700

--- Example Predictions (First 5 Evaluated) ---

--------------------- Sample Index: 0 ---------------------
  Q: How many food item is shown in the bar graph?
  GT: 14
  Base Pred: 100 (Incorrect)
  FFT Pred:  102.46 (Incorrect)
  LoRA Pred: 100 (Incorrect)

--------------------- Sample Index: 1 ---------------------
  Q: What is the difference in value between Lamb and Corn?
  GT: 0.57
  Base Pred: What is the difference in value between Barley and Rye? (Incorrect)
  FFT Pred:  103.13 (Incorrect)
  LoRA Pred: 103.13 102.46 87.37 85.27 83.73 82.2 68.48 64.71 64.71 57.6 55.36 42.48 25.56 18.81 40 60 80 100 (Incorrect)

--------------------- Sample Index: 2 ---------------------
  Q: How many bars are shown in the chart?
  GT: 3
  Base Pred: 0.48% 0.38% 0.21% 0.1% 0.0% 0.3% 0.4% 0.3% 0.2% 0.1% 0.0% (Incorrect)
  FFT Pred:  0.48% (Incorrect)
  LoRA Pred: 100 (Incorrect)

--------------------- Sample Index: 3 ---------------------
  Q: Is the sum value of Madagascar more then Fiji?
  GT: No
  Base Pred: Yes. (Incorrect)
  FFT Pred:  Yes (Incorrect)
  LoRA Pred: No. (Incorrect)

--------------------- Sample Index: 4 ---------------------
  Q: What's the value of the lowest bar?
  GT: 23
  Base Pred: The lowest bar is the highest bar. (Incorrect)
  FFT Pred:  29% (Incorrect)
  LoRA Pred: The value of the lowest bar is 29%. (Incorrect)

----------------------------------------------------------

--- Evaluation DataFrame (First 10 rows) ---
Using accuracy function: calculate_relaxed_accuracy_normalized
Downloading ChartQA dataset...
Loaded ChartQA test split with 2500 examples.

First ChartQA test sample structure:
{'image': <PIL.PngImagePlugin.PngImageFile image mode=RGBA size=850x600 at 0x7C62010F3490>, 'query': 'How many food item is shown in the bar graph?', 'label': ['14'], 'human_or_machine': 0}
Using accuracy function: calculate_relaxed_accuracy_normalized

--- Loading Base Model and SHARED Processor ---
Error loading base model or shared processor: Ellipsis is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

--- Loading Base Model and SHARED Processor ---
Base model 'HuggingFaceTB/SmolVLM-500M-Base' and SHARED processor loaded.
Fine-tuned model loaded. Using SHARED processor.

--- Loading Fine-tuned Model for Evaluation ---
Checking for model weights in: ./smolvlm-chartllama-500-full-tuned/final_adapter
Checking for processor in: ./smolvlm-chartllama-500-full-tuned/final_processor
Loading processor from: ./smolvlm-chartllama-500-full-tuned/final_processor
Loading model from: ./smolvlm-chartllama-500-full-tuned/final_adapter
Fine-tuned model and processor loaded successfully from subdirs on cuda.

--- Loading LoRA Fine-tuned Model for Evaluation ---
Adapter config found in subdirectory: ./smolvlm-chartllama-500-lora-tuned/final_adapter
Checking for adapter config in: ./smolvlm-chartllama-500-lora-tuned/final_adapter
Checking for processor config in: ./smolvlm-chartllama-500-lora-tuned/final_processor

Loading processor from: ./smolvlm-chartllama-500-lora-tuned/final_processor

Loading BASE model 'HuggingFaceTB/SmolVLM-500M-Base'...
Configuring base model for standard LoRA loading...
Base model loaded.
Loading LoRA adapter from: ./smolvlm-chartllama-500-lora-tuned/final_adapter
LoRA adapter loaded and applied. Final model ready on cuda:0.

--- Starting Evaluation on 100 ChartQA Samples (Base vs FFT vs LoRA) ---
/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2208: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.
  warnings.warn(
Error during model inference: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)
/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
Error during model inference: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)
Error during model inference: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)
Error during model inference: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)
Error during model inference: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)
Error during model inference: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)
Error during model inference: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)
Error during model inference: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)
Error during model inference: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)
Error during model inference: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)
Error during model inference: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)
Error during model inference: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)
Error during model inference: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)
Error during model inference: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)
Error during model inference: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)
Error during model inference: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)
Error during model inference: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)
Error during model inference: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)
Error during model inference: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)
Error during model inference: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)
Error during model inference: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)
Using accuracy function: calculate_relaxed_accuracy_normalized

--- Loading Base Model and SHARED Processor ---
Base model 'HuggingFaceTB/SmolVLM-500M-Base' and SHARED processor loaded.

--- Loading Fine-tuned Model for Evaluation ---
Checking for model weights in: ./smolvlm-chartllama-500-full-tuned/final_adapter
Checking for processor in: ./smolvlm-chartllama-500-full-tuned/final_processor
Loading processor from: ./smolvlm-chartllama-500-full-tuned/final_processor
Loading model from: ./smolvlm-chartllama-500-full-tuned/final_adapter
Fine-tuned model and processor loaded successfully from subdirs on cuda.

--- Loading LoRA Fine-tuned Model for Evaluation ---
Adapter config found in subdirectory: ./smolvlm-chartllama-500-lora-tuned/final_adapter
Checking for adapter config in: ./smolvlm-chartllama-500-lora-tuned/final_adapter
Checking for processor config in: ./smolvlm-chartllama-500-lora-tuned/final_processor

Loading processor from: ./smolvlm-chartllama-500-lora-tuned/final_processor

Loading BASE model 'HuggingFaceTB/SmolVLM-500M-Base'...
Configuring base model for standard LoRA loading...
Base model loaded.
Loading LoRA adapter from: ./smolvlm-chartllama-500-lora-tuned/final_adapter
LoRA adapter loaded and applied. Final model ready on cuda:0.

--- Starting Evaluation on 100 ChartQA Samples (Base vs FFT vs LoRA) ---
Error during model inference: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)
Error during model inference: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)
Error during model inference: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)
Error during model inference: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)
Error during model inference: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)
Error during model inference: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)
Using accuracy function: calculate_relaxed_accuracy

--- Loading Base Model and SHARED Processor ---
Base model 'HuggingFaceTB/SmolVLM-500M-Base' and SHARED processor loaded.

--- Loading Fine-tuned Model for Evaluation ---
Checking for model weights in: ./smolvlm-chartllama-500-full-tuned/final_adapter
Checking for processor in: ./smolvlm-chartllama-500-full-tuned/final_processor
Loading processor from: ./smolvlm-chartllama-500-full-tuned/final_processor
Loading model from: ./smolvlm-chartllama-500-full-tuned/final_adapter
Fine-tuned model and processor loaded successfully from subdirs on cuda.

--- Loading Base Model for Evaluation ---
Base model 'HuggingFaceTB/SmolVLM-500M-Base' loaded successfully on cuda.

--- Loading Fine-tuned Model for Evaluation ---
Checking for model weights in: ./smolvlm-chartllama-500-full-tuned/final_adapter
Checking for processor in: ./smolvlm-chartllama-500-full-tuned/final_processor
Loading processor from: ./smolvlm-chartllama-500-full-tuned/final_processor
Loading model from: ./smolvlm-chartllama-500-full-tuned/final_adapter
Fine-tuned model and processor loaded successfully from subdirs on cuda.

--- Loading LoRA Fine-tuned Model for Evaluation ---
Adapter config found in subdirectory: ./smolvlm-chartllama-500-lora-tuned/final_adapter
Checking for adapter config in: ./smolvlm-chartllama-500-lora-tuned/final_adapter
Checking for processor config in: ./smolvlm-chartllama-500-lora-tuned/final_processor

Loading processor from: ./smolvlm-chartllama-500-lora-tuned/final_processor

Loading BASE model 'HuggingFaceTB/SmolVLM-500M-Base'...
Configuring base model for standard LoRA loading...
Base model loaded.
Loading LoRA adapter from: ./smolvlm-chartllama-500-lora-tuned/final_adapter
LoRA adapter loaded and applied. Final model ready on cuda:0.

--- Starting Evaluation on 100 ChartQA Samples (Base vs FFT vs LoRA) ---

--- Loading Base Model for Evaluation ---
Base model 'HuggingFaceTB/SmolVLM-500M-Base' loaded successfully on cuda.

--- Loading Fine-tuned Model for Evaluation ---
Checking for model weights in: ./smolvlm-chartllama-500-full-tuned/final_adapter
Checking for processor in: ./smolvlm-chartllama-500-full-tuned/final_processor
Loading processor from: ./smolvlm-chartllama-500-full-tuned/final_processor
Loading model from: ./smolvlm-chartllama-500-full-tuned/final_adapter
Fine-tuned model and processor loaded successfully from subdirs on cuda.

--- Loading LoRA Fine-tuned Model for Evaluation ---
Error: adapter_config.json not found in ./smolvlm-chartllama-500-lora-tuned or ./smolvlm-chartllama-500-lora-tuned/final_adapter
Checking for adapter config in: None
Checking for processor config in: ./smolvlm-chartllama-500-lora-tuned/final_processor
Error: Could not find required adapter/processor directories or config files.

--- Loading LoRA Fine-tuned Model for Evaluation ---
Error: adapter_config.json not found in ./smolvlm-chartllama-500-lora-tuned or ./smolvlm-chartllama-500-lora-tuned/final_adapter
Checking for adapter config in: None
Checking for processor config in: ./smolvlm-chartllama-500-lora-tuned/final_processor
Error: Could not find required adapter/processor directories or config files.

--- Loading LoRA Fine-tuned Model for Evaluation ---
Error: adapter_config.json not found in ./smolvlm-chartllama-500-lora-tuned or ./smolvlm-chartllama-500-lora-tuned/final_adapter
Checking for adapter config in: None
Checking for processor config in: ./smolvlm-chartllama-500-lora-tuned/final_processor
Error: Could not find required adapter/processor directories or config files.

--- Loading Base Model for Evaluation ---
Base model 'HuggingFaceTB/SmolVLM-500M-Base' loaded successfully on cuda.

--- Loading Fine-tuned Model for Evaluation ---
Checking for model weights in: ./smolvlm-chartllama-500-full-tuned/final_adapter
Checking for processor in: ./smolvlm-chartllama-500-full-tuned/final_processor
Loading processor from: ./smolvlm-chartllama-500-full-tuned/final_processor
Loading model from: ./smolvlm-chartllama-500-full-tuned/final_adapter
Fine-tuned model and processor loaded successfully from subdirs on cuda.

--- Loading LoRA Fine-tuned Model for Evaluation ---
Error: adapter_config.json not found in ./smolvlm-chartllama-500-lora-tuned or ./smolvlm-chartllama-500-lora-tuned/final_adapter
Checking for adapter config in: None
Checking for processor config in: ./smolvlm-chartllama-500-lora-tuned/final_processor
Error: Could not find required adapter/processor directories or config files.
