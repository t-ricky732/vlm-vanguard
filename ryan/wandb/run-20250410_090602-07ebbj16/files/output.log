PyTorch version: 2.6.0+cu124
CUDA available: True
CUDA version: 12.4
GPU: NVIDIA A100-SXM4-40GB
GPU memory: 42.47 GB
Using dtype: torch.bfloat16
Checking data directory: /content/drive/MyDrive/code/chartllama_data
Does directory exist? True
Files in directory:
  - candlestick_chart_100examples_simplified_qa.json
  - funnel_chart_100examples_simplified_qa.json
  - gantt_chart_100examples_simplified_qa.json
  - polar_chart_100examples_simplified_qa.json
  - heatmap_chart_100examples_simplified_qa.json
  - box_chart_100examples_simplified_qa.json
  - .gitattributes
  - ours.zip
  - scatter_chart_100examples_simplified_qa.json
  - extracted_images
  - .cache
Loading processor...
Loading processor...
Loading dataset using custom ChartDataset...
Reading JSONs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:14<00:00,  2.09s/it]
Total samples: 980
Training samples: 784
Validation samples: 98
Test samples: 98
Configuring model for standard LoRA (dtype: torch.bfloat16)...
Loading model 'HuggingFaceTB/SmolVLM-500M-Base'...
Model loaded in 0.45 seconds
Configured for Full Fine-tuning.
/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
Initializing Trainer...
Starting training...
[34m[1mwandb[0m: [33mWARNING[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
Training completed in 36.77 minutes
Saving training results and final model/adapter...
***** train metrics *****
  epoch                    =        3.0
  total_flos               =  6972194GF
  train_loss               =     2.1765
  train_runtime            = 0:36:45.93
  train_samples_per_second =      1.066
  train_steps_per_second   =      0.067
Model adapter and processor saved to ./smolvlm-chartllama-500-full-tuned
Loading processor...
Loading dataset using custom ChartDataset...
Reading JSONs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:14<00:00,  2.03s/it]
Total samples: 980
Training samples: 784
Validation samples: 98
Test samples: 98
Configuring model for QLoRA (8-bit quantization)...
Loading model 'HuggingFaceTB/SmolVLM-500M-Base'...
`low_cpu_mem_usage` was None, now default to True since model is quantized.
Model loaded in 1.21 seconds
Preparing model for LoRA training...
Applying prepare_model_for_kbit_training for QLoRA...
trainable params: 9,568,256 || all params: 517,050,560 || trainable%: 1.8505
/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
Initializing Trainer...
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Starting training...
/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Training completed in 57.51 minutes
Saving training results and final model/adapter...
***** train metrics *****
  epoch                    =        3.0
  total_flos               =  7117413GF
  train_loss               =     3.2993
  train_runtime            = 0:57:30.15
  train_samples_per_second =      0.682
  train_steps_per_second   =      0.043
Model adapter and processor saved to ./smolvlm-chartllama-500-lora-tuned
Downloading ChartQA dataset...
Loaded ChartQA test split with 2500 examples.

First ChartQA test sample structure:
{'image': <PIL.PngImagePlugin.PngImageFile image mode=RGBA size=850x600 at 0x7B2082AD08D0>, 'query': 'How many food item is shown in the bar graph?', 'label': ['14'], 'human_or_machine': 0}
Using accuracy function: calculate_relaxed_accuracy

--- Loading Base Model for Evaluation ---
Base model 'HuggingFaceTB/SmolVLM-500M-Base' loaded successfully on cuda.

--- Loading Fine-tuned Model for Evaluation ---
Checking for model weights in: ./smolvlm-chartllama-500-full-tuned/final_adapter
Checking for processor in: ./smolvlm-chartllama-500-full-tuned/final_processor
Error: Could not find required model/processor directories or config files.
  Reason: Model path ('./smolvlm-chartllama-500-full-tuned/final_adapter') or its config.json missing.
  Reason: Processor path ('./smolvlm-chartllama-500-full-tuned/final_processor') or its processor_config.json missing.

--- Loading LoRA Fine-tuned Model for Evaluation ---
Adapter config found in subdirectory: ./smolvlm-chartllama-lora-tuned/final_adapter
Checking for adapter config in: ./smolvlm-chartllama-lora-tuned/final_adapter
Checking for processor config in: ./smolvlm-chartllama-lora-tuned/final_processor

Loading processor from: ./smolvlm-chartllama-lora-tuned/final_processor

Loading BASE model 'HuggingFaceTB/SmolVLM-500M-Base'...
Configuring base model for standard LoRA loading...
Base model loaded.
Loading LoRA adapter from: ./smolvlm-chartllama-lora-tuned/final_adapter
Error loading LoRA fine-tuned model/processor: Error(s) in loading state_dict for PeftModelForCausalLM:
	size mismatch for base_model.model.model.vision_model.encoder.layers.0.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.0.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.0.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.0.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.0.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.1.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.1.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.1.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.1.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.1.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.1.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.2.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.2.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.2.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.2.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.2.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.2.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.3.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.3.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.3.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.3.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.3.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.3.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.4.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.4.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.4.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.4.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.4.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.4.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.5.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.5.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.5.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.5.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.5.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.5.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.6.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.6.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.6.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.6.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.6.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.6.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.7.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.7.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.7.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.7.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.7.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.7.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.8.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.8.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.8.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.8.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.8.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.8.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.9.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.9.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.9.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.9.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.9.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.9.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.10.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.10.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.10.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.10.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.10.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.10.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.11.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.11.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.11.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.11.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.11.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 1152]) from checkpoint, the shape in current model is torch.Size([16, 768]).
	size mismatch for base_model.model.model.vision_model.encoder.layers.11.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([1152, 16]) from checkpoint, the shape in current model is torch.Size([768, 16]).
	size mismatch for base_model.model.model.text_model.layers.0.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.0.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.0.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.0.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.0.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.0.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.0.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.0.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.0.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.0.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.0.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.0.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.0.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 8192]) from checkpoint, the shape in current model is torch.Size([16, 2560]).
	size mismatch for base_model.model.model.text_model.layers.0.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.1.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.1.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.1.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.1.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.1.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.1.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.1.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.1.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.1.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.1.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.1.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.1.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.1.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 8192]) from checkpoint, the shape in current model is torch.Size([16, 2560]).
	size mismatch for base_model.model.model.text_model.layers.1.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.2.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.2.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.2.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.2.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.2.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.2.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.2.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.2.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.2.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.2.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.2.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.2.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.2.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 8192]) from checkpoint, the shape in current model is torch.Size([16, 2560]).
	size mismatch for base_model.model.model.text_model.layers.2.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.3.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.3.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.3.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.3.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.3.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.3.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.3.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.3.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.3.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.3.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.3.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.3.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.3.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 8192]) from checkpoint, the shape in current model is torch.Size([16, 2560]).
	size mismatch for base_model.model.model.text_model.layers.3.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.4.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.4.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.4.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.4.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.4.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.4.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.4.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.4.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.4.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.4.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.4.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.4.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.4.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 8192]) from checkpoint, the shape in current model is torch.Size([16, 2560]).
	size mismatch for base_model.model.model.text_model.layers.4.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.5.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.5.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.5.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.5.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.5.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.5.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.5.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.5.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.5.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.5.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.5.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.5.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.5.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 8192]) from checkpoint, the shape in current model is torch.Size([16, 2560]).
	size mismatch for base_model.model.model.text_model.layers.5.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.6.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.6.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.6.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.6.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.6.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.6.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.6.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.6.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.6.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.6.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.6.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.6.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.6.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 8192]) from checkpoint, the shape in current model is torch.Size([16, 2560]).
	size mismatch for base_model.model.model.text_model.layers.6.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.7.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.7.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.7.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.7.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.7.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.7.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.7.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.7.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.7.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.7.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.7.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.7.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.7.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 8192]) from checkpoint, the shape in current model is torch.Size([16, 2560]).
	size mismatch for base_model.model.model.text_model.layers.7.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.8.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.8.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.8.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.8.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.8.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.8.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.8.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.8.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.8.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.8.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.8.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.8.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.8.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 8192]) from checkpoint, the shape in current model is torch.Size([16, 2560]).
	size mismatch for base_model.model.model.text_model.layers.8.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.9.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.9.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.9.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.9.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.9.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.9.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.9.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.9.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.9.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.9.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.9.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.9.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.9.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 8192]) from checkpoint, the shape in current model is torch.Size([16, 2560]).
	size mismatch for base_model.model.model.text_model.layers.9.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.10.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.10.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.10.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.10.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.10.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.10.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.10.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.10.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.10.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.10.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.10.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.10.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.10.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 8192]) from checkpoint, the shape in current model is torch.Size([16, 2560]).
	size mismatch for base_model.model.model.text_model.layers.10.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.11.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.11.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.11.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.11.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.11.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.11.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.11.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.11.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.11.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.11.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.11.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.11.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.11.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 8192]) from checkpoint, the shape in current model is torch.Size([16, 2560]).
	size mismatch for base_model.model.model.text_model.layers.11.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.12.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.12.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.12.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.12.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.12.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.12.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.12.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.12.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.12.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.12.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.12.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.12.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.12.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 8192]) from checkpoint, the shape in current model is torch.Size([16, 2560]).
	size mismatch for base_model.model.model.text_model.layers.12.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.13.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.13.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.13.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.13.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.13.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.13.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.13.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.13.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.13.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.13.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.13.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.13.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.13.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 8192]) from checkpoint, the shape in current model is torch.Size([16, 2560]).
	size mismatch for base_model.model.model.text_model.layers.13.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.14.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.14.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.14.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.14.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.14.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.14.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.14.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.14.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.14.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.14.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.14.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.14.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.14.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 8192]) from checkpoint, the shape in current model is torch.Size([16, 2560]).
	size mismatch for base_model.model.model.text_model.layers.14.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.15.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.15.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.15.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.15.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.15.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.15.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.15.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.15.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.15.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.15.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.15.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.15.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.15.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 8192]) from checkpoint, the shape in current model is torch.Size([16, 2560]).
	size mismatch for base_model.model.model.text_model.layers.15.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.16.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.16.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.16.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.16.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.16.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.16.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.16.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.16.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.16.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.16.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.16.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.16.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.16.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 8192]) from checkpoint, the shape in current model is torch.Size([16, 2560]).
	size mismatch for base_model.model.model.text_model.layers.16.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.17.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.17.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.17.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.17.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.17.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.17.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.17.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.17.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.17.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.17.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.17.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.17.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.17.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 8192]) from checkpoint, the shape in current model is torch.Size([16, 2560]).
	size mismatch for base_model.model.model.text_model.layers.17.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.18.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.18.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.18.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.18.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.18.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.18.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.18.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.18.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.18.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.18.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.18.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.18.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.18.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 8192]) from checkpoint, the shape in current model is torch.Size([16, 2560]).
	size mismatch for base_model.model.model.text_model.layers.18.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.19.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.19.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.19.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.19.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.19.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.19.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.19.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.19.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.19.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.19.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.19.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.19.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.19.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 8192]) from checkpoint, the shape in current model is torch.Size([16, 2560]).
	size mismatch for base_model.model.model.text_model.layers.19.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.20.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.20.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.20.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.20.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.20.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.20.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.20.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.20.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.20.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.20.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.20.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.20.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.20.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 8192]) from checkpoint, the shape in current model is torch.Size([16, 2560]).
	size mismatch for base_model.model.model.text_model.layers.20.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.21.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.21.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.21.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.21.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.21.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.21.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.21.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.21.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.21.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.21.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.21.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.21.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.21.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 8192]) from checkpoint, the shape in current model is torch.Size([16, 2560]).
	size mismatch for base_model.model.model.text_model.layers.21.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.22.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.22.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.22.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.22.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.22.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.22.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.22.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.22.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.22.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.22.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.22.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.22.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.22.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 8192]) from checkpoint, the shape in current model is torch.Size([16, 2560]).
	size mismatch for base_model.model.model.text_model.layers.22.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.23.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.23.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.23.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.23.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.23.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.23.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([320, 16]).
	size mismatch for base_model.model.model.text_model.layers.23.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.23.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).
	size mismatch for base_model.model.model.text_model.layers.23.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.23.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.23.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2048]) from checkpoint, the shape in current model is torch.Size([16, 960]).
	size mismatch for base_model.model.model.text_model.layers.23.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 16]) from checkpoint, the shape in current model is torch.Size([2560, 16]).
	size mismatch for base_model.model.model.text_model.layers.23.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 8192]) from checkpoint, the shape in current model is torch.Size([16, 2560]).
	size mismatch for base_model.model.model.text_model.layers.23.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([960, 16]).

Evaluation skipped. Check if ChartQA dataset and ALL THREE models/processors (base, fft, lora) loaded correctly.

No results to report. Evaluation may have been skipped or encountered errors.
Reason: Base model results missing.
Reason: FFT model results missing.
Reason: LoRA model results missing.
Downloading ChartQA dataset...
Loaded ChartQA test split with 2500 examples.

First ChartQA test sample structure:
{'image': <PIL.PngImagePlugin.PngImageFile image mode=RGBA size=850x600 at 0x7B20594C27D0>, 'query': 'How many food item is shown in the bar graph?', 'label': ['14'], 'human_or_machine': 0}
Using accuracy function: calculate_relaxed_accuracy

--- Loading Base Model for Evaluation ---
Base model 'HuggingFaceTB/SmolVLM-500M-Base' loaded successfully on cuda.

--- Loading Fine-tuned Model for Evaluation ---
Checking for model weights in: ./smolvlm-chartllama-500-full-tuned/final_adapter
Checking for processor in: ./smolvlm-chartllama-500-full-tuned/final_processor
Error: Could not find required model/processor directories or config files.
  Reason: Model path ('./smolvlm-chartllama-500-full-tuned/final_adapter') or its config.json missing.
  Reason: Processor path ('./smolvlm-chartllama-500-full-tuned/final_processor') or its processor_config.json missing.
Loading processor...

--- Loading Base Model for Evaluation ---
Base model 'HuggingFaceTB/SmolVLM-256M-Base' loaded successfully on cuda.

--- Loading Fine-tuned Model for Evaluation ---
Checking for model weights in: ./smolvlm-chartllama-256-full-tuned/final_adapter
Checking for processor in: ./smolvlm-chartllama-256-full-tuned/final_processor
Loading processor from: ./smolvlm-chartllama-256-full-tuned/final_processor
Loading model from: ./smolvlm-chartllama-256-full-tuned/final_adapter
Fine-tuned model and processor loaded successfully from subdirs on cuda.

--- Loading LoRA Fine-tuned Model for Evaluation ---
Adapter config found in subdirectory: ./smolvlm-chartllama-256-lora-tuned/final_adapter
Checking for adapter config in: ./smolvlm-chartllama-256-lora-tuned/final_adapter
Checking for processor config in: ./smolvlm-chartllama-256-lora-tuned/final_processor

Loading processor from: ./smolvlm-chartllama-256-lora-tuned/final_processor

Loading BASE model 'HuggingFaceTB/SmolVLM-256M-Base'...
Configuring base model for standard LoRA loading...
Base model loaded.
Loading LoRA adapter from: ./smolvlm-chartllama-256-lora-tuned/final_adapter
LoRA adapter loaded and applied. Final model ready on cuda:0.

--- Starting Evaluation on 100 ChartQA Samples (Base vs FFT vs LoRA) ---
/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(

--- Evaluation Results ---
Evaluated on: 100 samples
Metric: Relaxed Accuracy (calculate_relaxed_accuracy)

Base Model (HuggingFaceTB/SmolVLM-256M-Base): 0.0300
FFT Model (smolvlm-chartllama-256-full-tuned): 0.0600
LoRA Model (smolvlm-chartllama-256-lora-tuned): 0.0900

--- Example Predictions (First 5 Evaluated) ---

--------------------- Sample Index: 0 ---------------------
  Q: How many food item is shown in the bar graph?
  GT: 14
  Base Pred: 1000 (Incorrect)
  FFT Pred:  1000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000 (Incorrect)
  LoRA Pred: 1000 (Incorrect)

--------------------- Sample Index: 1 ---------------------
  Q: What is the difference in value between Lamb and Corn?
  GT: 0.57
  Base Pred: 1000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.00 (Incorrect)
  FFT Pred:  100.000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000 (Incorrect)
  LoRA Pred: 1000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000 (Incorrect)

--------------------- Sample Index: 2 ---------------------
  Q: How many bars are shown in the chart?
  GT: 3
  Base Pred: 0.1% 0.2% 0.3% 0.4% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0. (Incorrect)
  FFT Pred:  0.21% (Incorrect)
  LoRA Pred: 0.21% 0.38% 0.4% 0.3% 0.4% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% (Incorrect)

--------------------- Sample Index: 3 ---------------------
  Q: Is the sum value of Madagascar more then Fiji?
  GT: No
  Base Pred: 0.1% 0.2% 0.3% 0.4% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0.3% 0. (Incorrect)
  FFT Pred:  Yes. (Incorrect)
  LoRA Pred: 0.21% 0.38% 0.4% 0.3% 0.4% 0.3% 0.4% 0.3% 0.4% 0.3% 0.4% 0.3% 0.4% 0.3% 0.4% 0.3% 0.4% 0.3% 0.4% 0.3% 0.4% 0.3% 0.4% 0.3% 0.4% (Incorrect)

--------------------- Sample Index: 4 ---------------------
  Q: What's the value of the lowest bar?
  GT: 23
  Base Pred: 0.000000 (Incorrect)
  FFT Pred:  100.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000. (Incorrect)
  LoRA Pred: 0.00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000 (Incorrect)

----------------------------------------------------------

--- Evaluation DataFrame (First 10 rows) ---
