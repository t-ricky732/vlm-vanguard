PyTorch version: 2.6.0+cu124
CUDA available: True
CUDA version: 12.4
GPU: NVIDIA A100-SXM4-40GB
GPU memory: 42.47 GB
Using dtype: torch.bfloat16
Loading processor...
/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning:
The secret `HF_TOKEN` does not exist in your Colab secrets.
To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.
You will be able to reuse this secret in all of your notebooks.
Please note that authentication is recommended but still optional to access public models or datasets.
  warnings.warn(
Configuring model for standard LoRA (dtype: torch.bfloat16)...
Loading model 'HuggingFaceTB/SmolVLM-256M-Base'...
Model loaded in 1.32 seconds
Preparing model for LoRA training...
trainable params: 5,769,216 || all params: 262,254,144 || trainable%: 2.1999
Checking data directory: /content/drive/MyDrive/code/chartllama_data
Does directory exist? True
Files in directory:
  - candlestick_chart_100examples_simplified_qa.json
  - funnel_chart_100examples_simplified_qa.json
  - gantt_chart_100examples_simplified_qa.json
  - polar_chart_100examples_simplified_qa.json
  - heatmap_chart_100examples_simplified_qa.json
  - box_chart_100examples_simplified_qa.json
  - .gitattributes
  - ours.zip
  - scatter_chart_100examples_simplified_qa.json
  - extracted_images
  - .cache
Loading dataset using custom ChartDataset...
Reading JSONs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:15<00:00,  2.23s/it]
Total samples: 980
Training samples: 784
Validation samples: 98
Test samples: 98
--- Displaying Item Index: 654 ---
Original ID: ours_simplified_qa_37_6
Chart Type: heatmap
Question: How many smartphone features have a usage below 50 in week 3?
<image>...
Answer: 2
Image Path (from JSON): ours/heatmap_chart/png/heatmap_chart_100examples_37.png
--------------------------------------
/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
Initializing Trainer...
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Starting training...
[34m[1mwandb[0m: [33mWARNING[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
Initializing Trainer...
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Starting training...
/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
Saving training results and final model/adapter...
Debugging forward pass...
Loss found in model output.
  Loss value: 6.219822883605957
  Loss requires_grad: False
  Loss grad_fn: None
trainable params: 5,769,216 || all params: 262,254,144 || trainable%: 2.1999
Starting training...
Configured for Full Fine-tuning.
/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
Initializing Trainer...
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Starting training...
/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
Loading processor...
Configuring model for standard LoRA (dtype: torch.bfloat16)...
Loading processor...
Configuring model for standard LoRA (dtype: torch.bfloat16)...
Loading model 'HuggingFaceTB/SmolVLM-256M-Base'...
Model loaded in 0.97 seconds
Configured for Full Fine-tuning.
/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
Initializing Trainer...
Starting training...
