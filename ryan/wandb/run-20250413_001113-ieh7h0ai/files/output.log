WandB initialized.
--- WandB Setup Complete ---

--- Defining Data Loading Function ---
Data loading function defined.

--- Loading and Splitting Data ---
Loading data from: /content/drive/MyDrive/code/chartllama_data
Found 7 JSON files in /content/drive/MyDrive/code/chartllama_data

Loaded 980 raw examples. Skipped 0 invalid/missing samples.

Raw data loaded and split.
Train samples: 882
Eval samples (Validation): 98

Sample train data point (raw):
{'id': 'ours_simplified_qa_59_0', 'question': 'What is the theme of the chart?', 'answer': 'Variation in the Consumer Price Index (CPI)', 'image_path': '/content/drive/MyDrive/code/chartllama_data/ours/candlestick_chart/png/candlestick_chart_100examples_59.png'}

--- Loading Model and Processor for Training ---
Loading processor for model: HuggingFaceTB/SmolVLM-Base
/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning:
The secret `HF_TOKEN` does not exist in your Colab secrets.
To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.
You will be able to reuse this secret in all of your notebooks.
Please note that authentication is recommended but still optional to access public models or datasets.
  warnings.warn(
Processor loaded.

Configuring model loading...
Configuring model for Full Fine-Tuning (dtype: torch.bfloat16).
Loading config for HuggingFaceTB/SmolVLM-Base...
Config does not have 'attn_implementation' attribute.
Loading model 'HuggingFaceTB/SmolVLM-Base' with configuration...

Model loaded.
  - Device Map: {'': 0}
  - Attention Implementation: N/A
  - Use Cache: False
Enabling gradient checkpointing for Full Fine-Tuning.

--- Model and Processor Setup Complete ---

--- Defining Formatting Function & Data Collator ---
Collator Initialized: Found image token ID: 49153

Custom Collator instantiated.

--- Configuring Training Arguments ---
SFTConfig set.

--- Initializing SFTTrainer ---
All components ready, initializing trainer...
SFTTrainer initialized successfully.

--- Checking if Training Should Be Skipped ---

Final output files not found or incomplete on Drive.
  - Model Check: False (Checked dir: /content/drive/MyDrive/code/smolvlm-chartllama-sft-refactored-SmolVLM-Base-full-tuned/final_model)
  - Processor Check: True (Checked dir: /content/drive/MyDrive/code/smolvlm-chartllama-sft-refactored-SmolVLM-Base-full-tuned/final_processor)
--- Proceeding with Training ---
Starting training run...
[34m[1mwandb[0m: [33mWARNING[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.

--- Checking if Training Should Be Skipped ---

Final output files not found or incomplete on Drive.
  - Model Check: False (Checked dir: /content/drive/MyDrive/code/smolvlm-chartllama-sft-refactored-SmolVLM-Base-full-tuned/final_model)
  - Processor Check: True (Checked dir: /content/drive/MyDrive/code/smolvlm-chartllama-sft-refactored-SmolVLM-Base-full-tuned/final_processor)
--- Proceeding with Training ---
Starting training run...

ERROR during training or saving: torch.utils.checkpoint: A different number of tensors was saved during the original forward and recomputation.
Number of tensors saved during forward: 200
Number of tensors saved during recomputation: 41
Traceback (most recent call last):
  File "<ipython-input-24-7dd83944b1d4>", line 50, in <cell line: 0>
    train_result = trainer.train()
                   ^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/transformers/trainer.py", line 2245, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/transformers/trainer.py", line 2556, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/transformers/trainer.py", line 3764, in training_step
    self.accelerator.backward(loss, **kwargs)
  File "/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py", line 2359, in backward
    loss.backward(**kwargs)
  File "/usr/local/lib/python3.11/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py", line 1129, in unpack_hook
    frame.check_recomputed_tensors_match(gid)
  File "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py", line 865, in check_recomputed_tensors_match
    raise CheckpointError(
torch.utils.checkpoint.CheckpointError: torch.utils.checkpoint: A different number of tensors was saved during the original forward and recomputation.
Number of tensors saved during forward: 200
Number of tensors saved during recomputation: 41
Attempting to save trainer state after error...

Cleaning up trainer object...

--- Training Step Execution Finished ---
