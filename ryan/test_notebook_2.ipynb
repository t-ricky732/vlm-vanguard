{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVKZjV8QLkg-"
      },
      "source": [
        "# Fine-tuning SmolVLM on ChartLlama Dataset\n",
        "\n",
        "This notebook demonstrates how to fine-tune the SmolVLM model on the ChartLlama dataset using parameter-efficient fine-tuning (LoRA)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYU5W95UL-6o",
        "outputId": "0d6316f5-b3f4-4f92-b099-80bde6a94b54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5_dD1fjUMVNd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('/content/drive/MyDrive/code')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install -U bitsandbytes"
      ],
      "metadata": {
        "id": "pcdMTIUFvfEK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "270ca778-18e5-4782-e77c-22e167fab19a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.3.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.5)\n",
            "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2024.12.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "vKFvh8NnLsGM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import gc\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import io\n",
        "import json\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from IPython.display import display, Image as IPyImage # Added IPyImage for display\n",
        "\n",
        "from transformers import (\n",
        "    Idefics3Processor,\n",
        "    Idefics3ForConditionalGeneration,\n",
        "    BitsAndBytesConfig,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    EarlyStoppingCallback,\n",
        "    default_data_collator # Use the default collator\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, PeftModel, prepare_model_for_kbit_training\n",
        "from torch.utils.data import random_split # Keep random_split if needed for custom splits\n",
        "import wandb\n",
        "from accelerate.utils import set_seed\n",
        "from accelerate import Accelerator\n",
        "from tqdm.notebook import tqdm\n",
        "# Corrected import: Explicitly import Dataset along with others\n",
        "from datasets import load_dataset, DatasetDict, concatenate_datasets, Image as ds_Image, Dataset\n",
        "\n",
        "import os\n",
        "from datasets import load_from_disk, DatasetDict, concatenate_datasets\n",
        "import gc\n",
        "import traceback\n",
        "import io\n",
        "import torch # Needed for empty tensor return\n",
        "\n",
        "from chartllama_load import load_chartllama_data, create_chat_prompt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CHOOSE TRAINING METHOD ---\n",
        "USE_LORA = False    # Set to True for LoRA or QLoRA, False for Full Fine-Tuning\n",
        "USE_QLORA = False   # Set to True for QLoRA (requires USE_LORA=True)"
      ],
      "metadata": {
        "id": "avTs6VmlXswB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "XvMTFZi2RW00"
      },
      "outputs": [],
      "source": [
        "# --- Basic Configuration ---\n",
        "WANDB_PROJECT = \"smolvlm-chartllama\" # Project name for Weights & Biases (set to None to disable)\n",
        "MODEL_ID = \"HuggingFaceTB/SmolVLM-Base\"\n",
        "DATA_DIR = Path(\"./chartllama_data\") # Directory containing ChartLlama JSON files and image folder\n",
        "CHARTQA_CACHE_DIR = \"./chartqa_cache\" # Cache directory for ChartQA dataset\n",
        "SEED = 42\n",
        "SAMPLE_LIMIT = None # Limit number of samples for faster testing (e.g., 100). None for all data.\n",
        "EVAL_SAMPLE_LIMIT = 100 # Limit number of ChartQA samples for evaluation\n",
        "SYSTEM_MESSAGE = \"You are a helpful assistant that analyzes charts and answers questions about them.\" # Optional system prompt\n",
        "PROCESSED_DATA_DIR = \"./processed_data\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Output Directories ---\n",
        "BASE_OUTPUT_DIR = \"./smolvlm-chartllama\"\n",
        "if not USE_LORA:\n",
        "    TRAINING_TYPE = \"full-tuned\"\n",
        "    print(\"Configuring for Full Fine-Tuning.\")\n",
        "elif USE_QLORA:\n",
        "    TRAINING_TYPE = \"qlora-tuned\"\n",
        "    print(\"Configuring for QLoRA.\")\n",
        "    if not USE_LORA:\n",
        "        print(\"Warning: USE_QLORA=True requires USE_LORA=True. Setting USE_LORA=True.\")\n",
        "        USE_LORA = True\n",
        "else:\n",
        "    TRAINING_TYPE = \"lora-tuned\"\n",
        "    print(\"Configuring for standard LoRA.\")\n",
        "\n",
        "OUTPUT_DIR = f\"{BASE_OUTPUT_DIR}-{TRAINING_TYPE}\"\n",
        "FINAL_ADAPTER_DIR = os.path.join(OUTPUT_DIR, \"final_adapter\")\n",
        "FINAL_PROCESSOR_DIR = os.path.join(OUTPUT_DIR, \"final_processor\") # Consistent processor saving\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2HAAYFyr8LI",
        "outputId": "c1e44b8f-a659-4df3-ee59-7845ab424dac"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuring for Full Fine-Tuning.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Hardware & Precision ---\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# BF16 is preferred on Ampere+ GPUs for stability and speed\n",
        "DTYPE = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "print(f\"Using computational dtype: {DTYPE}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8-ukE7pX3eu",
        "outputId": "a358e2ea-c552-4418-c02b-00aeb7db2923"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Using computational dtype: torch.bfloat16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- LoRA Specific Configuration ---\n",
        "if USE_LORA:\n",
        "    LORA_R = 16\n",
        "    LORA_ALPHA = 32\n",
        "    LORA_DROPOUT = 0.1\n",
        "    # Common target modules for Llama-like architectures used in Idefics3\n",
        "    LORA_TARGET_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]"
      ],
      "metadata": {
        "id": "LatbRXw_X4jP"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Setup ---\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "set_seed(SEED)\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
      ],
      "metadata": {
        "id": "B1dd5SyvX6vb"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Weights & Biases Setup ---\n",
        "if WANDB_PROJECT:\n",
        "    wandb.init(project=WANDB_PROJECT, config={\n",
        "        \"model_id\": MODEL_ID,\n",
        "        \"training_type\": TRAINING_TYPE,\n",
        "        \"use_lora\": USE_LORA,\n",
        "        \"use_qlora\": USE_QLORA,\n",
        "        \"dtype\": str(DTYPE),\n",
        "        \"seed\": SEED,\n",
        "        \"lora_r\": LORA_R if USE_LORA else None,\n",
        "        \"lora_alpha\": LORA_ALPHA if USE_LORA else None,\n",
        "        \"sample_limit\": SAMPLE_LIMIT,\n",
        "        \"output_dir\": OUTPUT_DIR,\n",
        "    }, job_type=\"fine-tuning\")\n",
        "else:\n",
        "    os.environ[\"WANDB_DISABLED\"] = \"true\" # Disable wandb if project name is None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "fZHyZtDNX-v6",
        "outputId": "4597937f-1c0c-48fb-b001-23ef9cf44f35"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mryan-seet467\u001b[0m (\u001b[33mryan-seet467-georgia-institute-of-technology\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/drive/MyDrive/code/wandb/run-20250412_024749-7a28h73f</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/ryan-seet467-georgia-institute-of-technology/smolvlm-chartllama/runs/7a28h73f' target=\"_blank\">dark-fire-51</a></strong> to <a href='https://wandb.ai/ryan-seet467-georgia-institute-of-technology/smolvlm-chartllama' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/ryan-seet467-georgia-institute-of-technology/smolvlm-chartllama' target=\"_blank\">https://wandb.ai/ryan-seet467-georgia-institute-of-technology/smolvlm-chartllama</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/ryan-seet467-georgia-institute-of-technology/smolvlm-chartllama/runs/7a28h73f' target=\"_blank\">https://wandb.ai/ryan-seet467-georgia-institute-of-technology/smolvlm-chartllama/runs/7a28h73f</a>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Rc3mwQ_Nk1jI"
      },
      "outputs": [],
      "source": [
        "# https://wandb.ai/authorize\n",
        "# 0469802d14d997b8dad4d23a7ba212e0a8d8f197"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6kiYbwYSRd9-",
        "outputId": "901ade8e-14aa-402b-95c1-b7d29fc43ee1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Configuration Summary ---\n",
            "Model ID: HuggingFaceTB/SmolVLM-Base\n",
            "Training Type: full-tuned\n",
            "Using LoRA: False\n",
            "Using QLoRA: False\n",
            "Output Directory: ./smolvlm-chartllama-full-tuned\n",
            "Device: cuda\n",
            "Dtype: torch.bfloat16\n",
            "Seed: 42\n",
            "---------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(f\"\\n--- Configuration Summary ---\")\n",
        "print(f\"Model ID: {MODEL_ID}\")\n",
        "print(f\"Training Type: {TRAINING_TYPE}\")\n",
        "print(f\"Using LoRA: {USE_LORA}\")\n",
        "print(f\"Using QLoRA: {USE_QLORA}\")\n",
        "print(f\"Output Directory: {OUTPUT_DIR}\")\n",
        "print(f\"Device: {DEVICE}\")\n",
        "print(f\"Dtype: {DTYPE}\")\n",
        "print(f\"Seed: {SEED}\")\n",
        "if USE_LORA:\n",
        "    print(f\"LoRA R: {LORA_R}, Alpha: {LORA_ALPHA}, Dropout: {LORA_DROPOUT}\")\n",
        "print(\"---------------------------\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVgoB9BuLkhC"
      },
      "source": [
        "## Check Hardware and Clear Memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R10RgsYVLkhC",
        "outputId": "ace7c239-aa4e-4374-cd09-9db4d620ad46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.6.0+cu124\n",
            "CUDA available: True\n",
            "CUDA version: 12.4\n",
            "GPU: NVIDIA A100-SXM4-40GB\n",
            "GPU memory: 42.47 GB\n",
            "Using compute dtype: torch.bfloat16\n"
          ]
        }
      ],
      "source": [
        "# --- Check Hardware ---\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    print(f\"GPU: {gpu_name}\")\n",
        "    print(f\"GPU memory: {gpu_memory_gb:.2f} GB\")\n",
        "    print(f\"Using compute dtype: {DTYPE}\")\n",
        "    if USE_LORA and not USE_QLORA and gpu_memory_gb < 20: # Rough estimate for standard LoRA\n",
        "         print(\"Warning: Standard LoRA without quantization might require significant VRAM (>20GB). Consider using USE_QLORA=True if you encounter memory issues.\")\n",
        "    elif not USE_LORA and gpu_memory_gb < 40: # Rough estimate for FFT\n",
        "         print(\"Warning: Full Fine-Tuning requires substantial VRAM (often >40GB for models of this size). Monitor memory usage closely.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Processor (Before Data Loading)\n"
      ],
      "metadata": {
        "id": "DzAOPzywYNhg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Load Processor ---\n",
        "# Load the processor first, as it's needed for preprocessing\n",
        "print(\"Loading processor...\")\n",
        "processor = Idefics3Processor.from_pretrained(MODEL_ID)\n",
        "if processor.tokenizer.pad_token is None:\n",
        "    processor.tokenizer.pad_token = processor.tokenizer.eos_token # Set pad token if not defined\n",
        "print(\"Processor loaded.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6pg40QlYQr2",
        "outputId": "c2b3a6a1-8990-4bb2-b527-d18b6f2334c6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading processor...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processor loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Loading and Preprocessing (Replaces custom Dataset and Collate)"
      ],
      "metadata": {
        "id": "yq-8ArMTYUM3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def preprocess_data_batched(batch, processor, system_message=\"\"):\n",
        "    \"\"\"\n",
        "    Preprocesses a BATCH of data using the Idefics3Processor.\n",
        "    Handles potential missing images or template errors within the batch.\n",
        "    \"\"\"\n",
        "    batch_size = len(batch[\"id\"]) # Assumes 'id' key exists and determines batch size\n",
        "    all_prompts_text = []\n",
        "    all_images = []\n",
        "    valid_indices_for_batch = [] # Keep track of samples processed correctly in this batch\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        sample_id = batch.get(\"id\", [\"Unknown\"]*batch_size)[i] # Get sample ID safely\n",
        "        try:\n",
        "            # --- Image Handling ---\n",
        "            image = batch[\"image\"][i]\n",
        "            if image is None:\n",
        "                # print(f\"Debug: Skipping sample {sample_id} - no image.\") # Keep commented unless debugging\n",
        "                continue\n",
        "\n",
        "            # Basic PIL conversion check (add more robust conversion if needed based on your data)\n",
        "            if not isinstance(image, Image.Image):\n",
        "                if isinstance(image, dict) and 'bytes' in image and image['bytes']:\n",
        "                     image = Image.open(io.BytesIO(image['bytes'])).convert(\"RGB\")\n",
        "                else:\n",
        "                     # Add other necessary conversions here if needed\n",
        "                     # print(f\"Debug: Could not convert image type {type(image)} for {sample_id}\")\n",
        "                     raise TypeError(f\"Unhandled image type: {type(image)}\")\n",
        "\n",
        "            if image.mode != 'RGB':\n",
        "                image = image.convert(\"RGB\")\n",
        "\n",
        "            # --- Text Handling ---\n",
        "            # Reconstruct single sample dict for create_chat_prompt\n",
        "            single_sample = {key: values[i] for key, values in batch.items() if isinstance(values, list) and i < len(values)}\n",
        "            chat_messages = create_chat_prompt(single_sample, system_message)\n",
        "            prompt_text = processor.apply_chat_template(chat_messages, add_generation_prompt=False)\n",
        "\n",
        "            # --- Add valid data ---\n",
        "            all_prompts_text.append(prompt_text)\n",
        "            all_images.append(image)\n",
        "            valid_indices_for_batch.append(i) # Mark as valid for this batch\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Error processing sample {sample_id} in batch: {e}. Skipping sample.\")\n",
        "            # Optionally uncomment traceback for deep debugging:\n",
        "            # traceback.print_exc()\n",
        "\n",
        "    # --- Processor Call ---\n",
        "    if not all_prompts_text:\n",
        "        # Return structure matching expected output but with empty tensors/lists\n",
        "        # Ensure keys match what the Trainer/model expects!\n",
        "        return {'input_ids': torch.tensor([]), 'attention_mask': torch.tensor([]), 'pixel_values': torch.tensor([]), 'labels': torch.tensor([])}\n",
        "\n",
        "    try:\n",
        "        inputs = processor(\n",
        "            text=all_prompts_text,\n",
        "            images=all_images,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=\"longest\",\n",
        "            truncation=True,\n",
        "            max_length=getattr(getattr(processor, 'tokenizer', None), 'model_max_length', 2048)\n",
        "        )\n",
        "        # Set labels\n",
        "        inputs['labels'] = inputs['input_ids'].clone()\n",
        "        return inputs\n",
        "\n",
        "    except Exception as proc_err:\n",
        "        print(f\"Error during processor call for batch (containing IDs around {sample_id}): {proc_err}\")\n",
        "        # Return empty structure on processor error\n",
        "        return {'input_ids': torch.tensor([]), 'attention_mask': torch.tensor([]), 'pixel_values': torch.tensor([]), 'labels': torch.tensor([])}\n",
        "\n",
        "# --- Corrected Batched Filter Function ---\n",
        "def batched_filter_fn(batch):\n",
        "    required_keys = ['input_ids', 'attention_mask', 'pixel_values', 'labels']\n",
        "    key_for_length = None\n",
        "    for k in required_keys: # Find a reliable key to determine batch size\n",
        "        if k in batch and isinstance(batch[k], list) and batch[k]:\n",
        "            key_for_length = k\n",
        "            break\n",
        "\n",
        "    if key_for_length is None: # Handle empty or malformed batch dictionary\n",
        "         list_keys = [k for k,v in batch.items() if isinstance(v, list)]\n",
        "         if not list_keys or not batch[list_keys[0]]: return []\n",
        "         batch_size = len(batch[list_keys[0]])\n",
        "         # print(f\"Debug: Using backup key {list_keys[0]} for filter batch size.\")\n",
        "         return [False] * batch_size # Filter out if primary keys are missing/empty\n",
        "\n",
        "    batch_size = len(batch[key_for_length])\n",
        "    keep_mask = [True] * batch_size\n",
        "\n",
        "    # Check structure once per batch\n",
        "    keys_present_and_correct_length = all(\n",
        "        k in batch and isinstance(batch[k], list) and len(batch[k]) == batch_size\n",
        "        for k in required_keys\n",
        "    )\n",
        "    if not keys_present_and_correct_length:\n",
        "        # print(f\"Debug: Batch structure mismatch in filter. Filtering out batch.\")\n",
        "        return [False] * batch_size\n",
        "\n",
        "    # Iterate samples within the batch\n",
        "    for i in range(batch_size):\n",
        "        # Check individual elements for validity\n",
        "        input_ids_ok = batch['input_ids'][i] is not None and hasattr(batch['input_ids'][i], '__len__') and len(batch['input_ids'][i]) > 0\n",
        "        attn_mask_ok = batch['attention_mask'][i] is not None\n",
        "        pixel_values_ok = batch['pixel_values'][i] is not None\n",
        "        labels_ok = batch['labels'][i] is not None\n",
        "\n",
        "        if not (input_ids_ok and attn_mask_ok and pixel_values_ok and labels_ok):\n",
        "            keep_mask[i] = False\n",
        "    return keep_mask\n"
      ],
      "metadata": {
        "id": "8RtB10q2YYR6"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load and Preprocess"
      ],
      "metadata": {
        "id": "frBQmrP2YuFn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Main Loading/Preprocessing/Saving Block ---\n",
        "\n",
        "processed_data_exists = os.path.isdir(PROCESSED_DATA_DIR)\n",
        "train_dataset, val_dataset, test_dataset = None, None, None # Initialize\n",
        "\n",
        "if processed_data_exists:\n",
        "    print(f\"\\nLoading preprocessed dataset from: {PROCESSED_DATA_DIR}\")\n",
        "    try:\n",
        "        processed_dataset_dict = load_from_disk(PROCESSED_DATA_DIR)\n",
        "        train_dataset = processed_dataset_dict[\"train\"]\n",
        "        val_dataset = processed_dataset_dict[\"validation\"]\n",
        "        test_dataset = processed_dataset_dict[\"test\"]\n",
        "        print(\"Preprocessed dataset loaded successfully.\")\n",
        "        # Basic validation after load\n",
        "        if not train_dataset or not val_dataset or not test_dataset:\n",
        "             raise ValueError(\"Loaded dataset splits are missing or empty.\")\n",
        "        print(f\"Loaded splits - Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading preprocessed data from {PROCESSED_DATA_DIR}: {e}\")\n",
        "        print(\"Will attempt to re-process from raw data.\")\n",
        "        processed_data_exists = False # Force re-processing\n",
        "\n",
        "if not processed_data_exists:\n",
        "    print(\"\\nStarting raw data loading and preprocessing...\")\n",
        "    try:\n",
        "        # Ensure dependencies are available\n",
        "        if 'processor' not in globals() or processor is None:\n",
        "             raise NameError(\"Processor not defined or loaded.\")\n",
        "        if 'SYSTEM_MESSAGE' not in globals():\n",
        "             raise NameError(\"SYSTEM_MESSAGE not defined.\")\n",
        "        # Assuming chartllama_load is imported and works\n",
        "        # from chartllama_load import load_chartllama_data\n",
        "\n",
        "        # 1. Load Raw Data\n",
        "        print(\"Loading raw ChartLlama data...\")\n",
        "        raw_dataset = load_chartllama_data(DATA_DIR, sample_limit=SAMPLE_LIMIT)\n",
        "        print(f\"Raw data loaded: {len(raw_dataset)} samples.\")\n",
        "\n",
        "        # 2. Preprocess using .map()\n",
        "        print(\"Preprocessing raw data (batch_size=1)...\")\n",
        "        keep_cols = [\"image\", \"question\", \"answer\", \"id\"] # Columns needed by preprocess_data_batched\n",
        "        remove_cols = [col for col in raw_dataset.column_names if col not in keep_cols]\n",
        "        processed_dataset = raw_dataset.map(\n",
        "            lambda batch: preprocess_data_batched(batch, processor, SYSTEM_MESSAGE),\n",
        "            batched=True,\n",
        "            batch_size=1,             # Keep batch_size=1 for stability during map\n",
        "            remove_columns=remove_cols,\n",
        "            desc=\"Preprocessing data\"\n",
        "        )\n",
        "        print(\"Preprocessing map step complete.\")\n",
        "\n",
        "        # 3. Filter using batched filter function\n",
        "        print(\"Filtering processed data...\")\n",
        "        original_len = len(processed_dataset)\n",
        "        processed_dataset = processed_dataset.filter(\n",
        "            batched_filter_fn,\n",
        "            batched=True,\n",
        "            batch_size=10,            # Batching filter is efficient, use 10 or more\n",
        "            desc=\"Filtering data\"\n",
        "        )\n",
        "        filtered_len = len(processed_dataset)\n",
        "        print(f\"Filtering complete. Kept {filtered_len}/{original_len} samples.\")\n",
        "\n",
        "        if filtered_len == 0:\n",
        "            raise ValueError(\"Dataset is empty after filtering.\")\n",
        "\n",
        "        # 4. Split into Train/Validation/Test\n",
        "        print(\"Splitting dataset...\")\n",
        "        processed_dataset = processed_dataset.shuffle(seed=SEED) # Shuffle before split\n",
        "        if filtered_len < 10:\n",
        "             print(\"Warning: Less than 10 samples. Using all for train/val/test.\")\n",
        "             train_dataset = processed_dataset\n",
        "             val_dataset = processed_dataset\n",
        "             test_dataset = processed_dataset\n",
        "        else:\n",
        "            train_val_split = processed_dataset.train_test_split(test_size=0.2, seed=SEED)\n",
        "            train_dataset = train_val_split[\"train\"]\n",
        "            test_val_combined = train_val_split[\"test\"]\n",
        "            if len(test_val_combined) >= 2:\n",
        "                 val_test_split = test_val_combined.train_test_split(test_size=0.5, seed=SEED)\n",
        "                 val_dataset = val_test_split[\"train\"]\n",
        "                 test_dataset = val_test_split[\"test\"]\n",
        "            else:\n",
        "                 val_dataset = test_val_combined\n",
        "                 test_dataset = test_val_combined\n",
        "        print(f\"Splits created - Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")\n",
        "\n",
        "\n",
        "        # 5. Save the Processed Splits\n",
        "        print(f\"Saving preprocessed dataset splits to: {PROCESSED_DATA_DIR}\")\n",
        "        os.makedirs(PROCESSED_DATA_DIR, exist_ok=True)\n",
        "        final_splits = DatasetDict({\n",
        "            \"train\": train_dataset,\n",
        "            \"validation\": val_dataset,\n",
        "            \"test\": test_dataset\n",
        "        })\n",
        "        final_splits.save_to_disk(PROCESSED_DATA_DIR)\n",
        "        print(\"Preprocessed dataset saved.\")\n",
        "\n",
        "        # Clean up intermediate objects\n",
        "        del raw_dataset\n",
        "        del processed_dataset\n",
        "        del final_splits\n",
        "        gc.collect()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nAn error occurred during data loading/processing: {e}\")\n",
        "        traceback.print_exc()\n",
        "        # Ensure datasets are None if processing failed\n",
        "        train_dataset, val_dataset, test_dataset = None, None, None\n",
        "\n",
        "# --- Final Check ---\n",
        "if train_dataset is None or val_dataset is None or test_dataset is None:\n",
        "    print(\"\\nERROR: Dataset preparation failed. Cannot proceed.\")\n",
        "    # Handle error appropriately, e.g., exit() or raise Exception\n",
        "else:\n",
        "    print(\"\\nDataset preparation complete. Proceeding to next steps (Model Loading/Trainer).\")\n",
        "    # ... (rest of your notebook code) ..."
      ],
      "metadata": {
        "id": "hTS6drXJYwsK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 635,
          "referenced_widgets": [
            "b7d6af74504a4890bcaf8d906f04cf69",
            "6463887c2c7b4dd081c9232774b450b9",
            "d1c2e5acda75462ca60c9197db275cc0",
            "fe7064b1048b4310b0d18b69a7e8676e",
            "b2644bab6bbf4060aa4a3544da98023c",
            "d7c8f7758c834b3caaeac520ebf623c3",
            "6c72da713790472e8d6ceac8598144b6",
            "7edc6348437e47d3ae6e4170c8e0ddac",
            "d73f35e215aa487381895108889beef0",
            "ef99a18230374771857e57e5780ad50e",
            "f41d2cd4b6994e1584bc5617e345d253",
            "b77f2408fd6b4b76862302591798f611",
            "c8dbca17e0174ea0a9994649b820581f",
            "1f9936673b45432689108378d95a488e",
            "8b97a338cc9342709129ddedf19de1d5",
            "7b7c96ced4e541838dbeef1e01a1c7fd",
            "0c3b3b55315f4777817e243437172922",
            "20261957335a4d40a16da9ad90463a05",
            "0c51f1cd1abd4fe8ae0296e8ef93b5e1",
            "cd83ee61de2b4e29a014defffe1b09e8",
            "c711c370e8ee485094c24cc232492768",
            "044954eb042840dcbfd92fa84afdcd56",
            "ea5ccf8398284976af00cf48206e6385",
            "0e1c71bc84a741a9b6488af82f91e85d",
            "ea536376948c41cd80ce563eea459bb6",
            "f65caf4a3f5349128864ff81f4756480",
            "2e1b1588990141e6b990533061e8e67f",
            "8ff7565888214f5fa63171fc1424a8e1",
            "813f327290a64f60ad669e0c619625d1",
            "7dfcc9132032495db575577cbf02abf4",
            "b2a63a495bc14ccc9d8f39bab40e57d0",
            "ca67b730ee3246c8a271a84172fbf346",
            "43504f81b53e485e8fd6b3c7e9deb115",
            "eb5c7d04882b4c20b1a6d586d283f0a5",
            "cad073da3490440bbc117ec2d84ef68b",
            "20960bf261ec49e9a3c4c7a706414cb4",
            "1737b0be86e04d80af11333bf63455d6",
            "fc6625c278e64eba9405a48b32360892",
            "87b936fbb8d64a5283a7b8d68ec0b87f",
            "107f4de73f7d4f0684eb98b9737ca1b8",
            "9e66e8133f7940158d0cc72a1c946d43",
            "2a8e7274e05c45b99f2d45e093fda3d6",
            "e6c02541f3c546e89c57b703d711ec32",
            "a97d0266da914727977ae4b504fbf57f",
            "034e4b3db4394d1982513203d474e0a8",
            "2200974f57fe4ac0be5733979f412d76",
            "f6b321ccebf04392801efe151486db7b",
            "f499a9d17c74482b8209118a922358f6",
            "8fec0757b6874dc690ba2c34be544251",
            "8412957ddea64862a77997cb2aa56ef3",
            "bc295cc6c38b4d018bbe6050a010401e",
            "edaf995b070d4cf18c4fb80ab305e31b",
            "2f83861a24d64cf59f8e5687efe4835d",
            "a0feca7da74c4e9bb69b10c3e4d9dc95",
            "66528a0ba1db452ea3010313ed4621cc"
          ]
        },
        "outputId": "ba43b85d-05c4-477f-e44b-dd745e781345"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting raw data loading and preprocessing...\n",
            "Loading raw ChartLlama data...\n",
            "Found 7 JSON files in chartllama_data.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reading JSONs: 100%|██████████| 7/7 [00:01<00:00,  6.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Initial processing complete.\n",
            "Loaded 980 valid samples.\n",
            "Skipped 0 samples due to missing data or images.\n",
            "Encountered 0 errors during file processing.\n",
            "\n",
            "Created final dataset with 980 samples.\n",
            "Dataset features: {'id': Value(dtype='string', id=None), 'image': Image(mode=None, decode=True, id=None), 'question': Value(dtype='string', id=None), 'answer': Value(dtype='string', id=None)}\n",
            "Raw data loaded: 980 samples.\n",
            "Preprocessing raw data (batch_size=1)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Preprocessing data:   0%|          | 0/980 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b7d6af74504a4890bcaf8d906f04cf69"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing map step complete.\n",
            "Filtering processed data...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Filtering data:   0%|          | 0/980 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b77f2408fd6b4b76862302591798f611"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtering complete. Kept 980/980 samples.\n",
            "Splitting dataset...\n",
            "Splits created - Train: 784, Val: 98, Test: 98\n",
            "Saving preprocessed dataset splits to: ./processed_data\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Saving the dataset (0/62 shards):   0%|          | 0/784 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ea5ccf8398284976af00cf48206e6385"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Saving the dataset (0/8 shards):   0%|          | 0/98 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eb5c7d04882b4c20b1a6d586d283f0a5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Saving the dataset (0/8 shards):   0%|          | 0/98 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "034e4b3db4394d1982513203d474e0a8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessed dataset saved.\n",
            "\n",
            "Dataset preparation complete. Proceeding to next steps (Model Loading/Trainer).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMzlhtFiLkhE"
      },
      "source": [
        "## 7. Model Loading and Preparation\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configure Model Loading ---\n",
        "model_load_kwargs = {}\n",
        "quantization_config = None\n",
        "\n",
        "if USE_QLORA:\n",
        "    print(\"Configuring model for QLoRA (8-bit quantization)...\")\n",
        "    # Note: SmolVLM example uses 8-bit, 4-bit might also work but requires careful testing\n",
        "    quantization_config = BitsAndBytesConfig(\n",
        "        load_in_8bit=True,\n",
        "        # Optional: These can sometimes improve 8-bit performance/stability\n",
        "        # bnb_8bit_use_double_quant=True,\n",
        "        # bnb_8bit_quant_type=\"nf8\",\n",
        "        # bnb_8bit_compute_dtype=DTYPE # Compute in higher precision\n",
        "    )\n",
        "    model_load_kwargs[\"quantization_config\"] = quantization_config\n",
        "    model_load_kwargs[\"device_map\"] = \"auto\" # Recommended for quantization\n",
        "    print(\"Using device_map='auto' for QLoRA loading.\")\n",
        "elif not USE_LORA: # Full Fine-Tuning\n",
        "     print(f\"Configuring model for Full Fine-Tuning (dtype: {DTYPE})...\")\n",
        "     model_load_kwargs[\"torch_dtype\"] = DTYPE\n",
        "     # device_map=\"auto\" can sometimes cause issues with FFT DataParallel/DDP,\n",
        "     # manually moving to DEVICE later might be safer if issues arise.\n",
        "     # model_load_kwargs[\"device_map\"] = \"auto\"\n",
        "else: # Standard LoRA\n",
        "    print(f\"Configuring model for standard LoRA (dtype: {DTYPE})...\")\n",
        "    model_load_kwargs[\"torch_dtype\"] = DTYPE\n",
        "    # device_map=\"auto\" can be useful here too, but manual placement also works\n",
        "    # model_load_kwargs[\"device_map\"] = \"auto\"\n",
        "\n",
        "\n",
        "# --- Load Model ---\n",
        "print(f\"\\nLoading model '{MODEL_ID}' with config: {model_load_kwargs}\")\n",
        "start_time = time.time()\n",
        "model = Idefics3ForConditionalGeneration.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    **model_load_kwargs,\n",
        "     # attn_implementation=\"flash_attention_2\" # Optional: Use if available and desired for speed\n",
        "     trust_remote_code=True # Might be needed for Idefics3 architecture specifics\n",
        ")\n",
        "\n",
        "# Manually move model to device if device_map wasn't used\n",
        "if \"device_map\" not in model_load_kwargs:\n",
        "     print(f\"Moving model to device: {DEVICE}\")\n",
        "     model.to(DEVICE)\n",
        "\n",
        "print(f\"Model loaded in {time.time() - start_time:.2f} seconds\")\n",
        "print(f\"Model device: {model.device}\")\n",
        "\n",
        "\n",
        "# --- Prepare Model for Fine-tuning ---\n",
        "if USE_LORA:\n",
        "    print(\"\\nPreparing model for LoRA/QLoRA training...\")\n",
        "    if USE_QLORA:\n",
        "        print(\"Applying prepare_model_for_kbit_training for QLoRA...\")\n",
        "        # Gradient checkpointing is implicitly enabled by this function\n",
        "        model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)\n",
        "\n",
        "    # Configure LoRA\n",
        "    print(\"Applying LoRA configuration...\")\n",
        "    lora_config = LoraConfig(\n",
        "        r=LORA_R,\n",
        "        lora_alpha=LORA_ALPHA,\n",
        "        target_modules=LORA_TARGET_MODULES,\n",
        "        lora_dropout=LORA_DROPOUT,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "        # modules_to_save = [\"lm_head\", \"embed_tokens\"] # Optional: Train output layer? Needs testing.\n",
        "    )\n",
        "    model = get_peft_model(model, lora_config)\n",
        "    model.print_trainable_parameters()\n",
        "else:\n",
        "      print(\"\\nModel configured for Full Fine-tuning.\")\n",
        "      # Enable gradient checkpointing manually if not using LoRA helpers\n",
        "      if hasattr(model, \"gradient_checkpointing_enable\"):\n",
        "           print(\"Enabling gradient checkpointing for Full Fine-Tuning.\")\n",
        "           model.gradient_checkpointing_enable()\n",
        "\n",
        "\n",
        "# Disable cache for training\n",
        "model.config.use_cache = False"
      ],
      "metadata": {
        "id": "NzpxbzypdCDR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cfa4d5a-199c-429b-eead-0f9f5444d2c0"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuring model for Full Fine-Tuning (dtype: torch.bfloat16)...\n",
            "\n",
            "Loading model 'HuggingFaceTB/SmolVLM-Base' with config: {'torch_dtype': torch.bfloat16}\n",
            "Moving model to device: cuda\n",
            "Model loaded in 3.81 seconds\n",
            "Model device: cuda:0\n",
            "\n",
            "Model configured for Full Fine-tuning.\n",
            "Enabling gradient checkpointing for Full Fine-Tuning.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Arguments"
      ],
      "metadata": {
        "id": "2B7I-EswdSAi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configure Training Arguments ---\n",
        "# Adjust hyperparameters based on whether it's FFT, LoRA, or QLoRA and available VRAM\n",
        "\n",
        "per_device_train_batch_size = 2 # Decrease if OOM\n",
        "per_device_eval_batch_size = 4  # Decrease if OOM during eval\n",
        "gradient_accumulation_steps = 8  # Increase to simulate larger batch size if VRAM is limited\n",
        "learning_rate = 2e-5 if not USE_LORA else 1e-4 # LoRA often benefits from higher LR\n",
        "num_train_epochs = 3\n",
        "weight_decay = 0.01\n",
        "warmup_ratio = 0.1\n",
        "lr_scheduler_type = \"cosine\"\n",
        "logging_steps = 25\n",
        "save_steps = 500 # Or adjust based on epoch strategy\n",
        "eval_steps = 500 # Or adjust based on epoch strategy\n",
        "\n",
        "print(f\"\\nSetting Training Arguments for {TRAINING_TYPE}...\")\n",
        "print(f\"Batch Size (Train): {per_device_train_batch_size}, Grad Accum: {gradient_accumulation_steps}\")\n",
        "print(f\"Effective Batch Size: {per_device_train_batch_size * gradient_accumulation_steps * torch.cuda.device_count() if torch.cuda.is_available() else per_device_train_batch_size * gradient_accumulation_steps}\")\n",
        "print(f\"Learning Rate: {learning_rate}\")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    per_device_eval_batch_size=per_device_eval_batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    learning_rate=learning_rate,\n",
        "\n",
        "    weight_decay=weight_decay,\n",
        "    warmup_ratio=warmup_ratio,\n",
        "    lr_scheduler_type=lr_scheduler_type,\n",
        "\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=logging_steps,\n",
        "\n",
        "    save_strategy=\"epoch\",         # Save checkpoints every epoch\n",
        "    evaluation_strategy=\"epoch\",   # Evaluate every epoch\n",
        "    # save_strategy=\"steps\",\n",
        "    # evaluation_strategy=\"steps\",\n",
        "    # save_steps=save_steps,\n",
        "    # eval_steps=eval_steps,\n",
        "\n",
        "    load_best_model_at_end=True,      # Load the best model found during training\n",
        "    metric_for_best_model=\"eval_loss\",# Use eval loss to determine the best model\n",
        "    greater_is_better=False,          # Lower eval loss is better\n",
        "    save_total_limit=2,               # Keep only the best and the latest checkpoint\n",
        "\n",
        "    fp16=(DTYPE == torch.float16),    # Use FP16 if selected\n",
        "    bf16=(DTYPE == torch.bfloat16),   # Use BF16 if selected\n",
        "\n",
        "    # Gradient checkpointing is enabled via prepare_model_for_kbit_training for QLoRA\n",
        "    # or manually enabled for FFT/standard LoRA in the model prep cell.\n",
        "    # Set False here if manually enabled, True otherwise (though manual is often clearer).\n",
        "    gradient_checkpointing=False if USE_QLORA or not USE_LORA else True,\n",
        "    gradient_checkpointing_kwargs={\"use_reentrant\": False}, # Recommended setting\n",
        "\n",
        "    seed=SEED,\n",
        "    optim=\"adamw_torch\",              # Recommended optimizer\n",
        "    report_to=\"wandb\" if WANDB_PROJECT else \"none\", # Report to W&B if configured\n",
        "    remove_unused_columns=False,      # Important: processor adds columns needed by model\n",
        "    dataloader_num_workers=2,         # Adjust based on system\n",
        "    dataloader_pin_memory=True,\n",
        "\n",
        "    # Potential args for speeding up / memory saving (use with caution)\n",
        "    # dataloader_drop_last=True,      # Can speed up if last batch is small\n",
        "    # group_by_length=True,           # Can make padding more efficient, requires length column\n",
        ")"
      ],
      "metadata": {
        "id": "eC2qkFX1dTuV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b448f305-d822-4939-ac17-6d78b616054f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Setting Training Arguments for full-tuned...\n",
            "Batch Size (Train): 2, Grad Accum: 8\n",
            "Effective Batch Size: 16\n",
            "Learning Rate: 2e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wSx9b2_8YMfH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialize trasiner using default collator"
      ],
      "metadata": {
        "id": "sSm5SLjYdal8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Initialize Trainer ---\n",
        "if train_dataset and val_dataset:\n",
        "    print(\"Initializing Trainer...\")\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        processor=processor, # Pass processor for potential saving/logging\n",
        "        data_collator=default_data_collator, # Use the default collator\n",
        "        # Removed callbacks for simplicity, can add EarlyStoppingCallback back if needed\n",
        "        # callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
        "    )\n",
        "    print(\"Trainer initialized.\")\n",
        "else:\n",
        "    print(\"\\nSkipping Trainer initialization due to dataset loading/processing errors.\")\n",
        "    trainer = None"
      ],
      "metadata": {
        "id": "M2PwG1aide5w",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "outputId": "c2fec31c-a26a-4c00-ced2-979fc74b0002"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing Trainer...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Trainer.__init__() got an unexpected keyword argument 'processor'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-d1dc9d7dec1c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtrain_dataset\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Initializing Trainer...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     trainer = Trainer(\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Trainer.__init__() got an unexpected keyword argument 'processor'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Training ---\n",
        "if trainer:\n",
        "    print(\"\\nStarting training...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Clear cache before training\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    try:\n",
        "        train_result = trainer.train()\n",
        "\n",
        "        training_time = (time.time() - start_time) / 60\n",
        "        print(f\"\\nTraining completed in {training_time:.2f} minutes\")\n",
        "\n",
        "        # --- Save Results and Final Model/Adapter ---\n",
        "        print(\"\\nSaving training results, final model/adapter, and processor...\")\n",
        "        trainer.log_metrics(\"train\", train_result.metrics)\n",
        "        trainer.save_metrics(\"train\", train_result.metrics)\n",
        "        trainer.save_state()\n",
        "\n",
        "        # Saving logic depends on the training type\n",
        "        if USE_LORA:\n",
        "            print(f\"Saving LoRA adapter weights to: {FINAL_ADAPTER_DIR}\")\n",
        "            model.save_pretrained(FINAL_ADAPTER_DIR) # Saves only the adapter\n",
        "        else: # Full Fine-Tuning\n",
        "            print(f\"Saving full fine-tuned model to: {OUTPUT_DIR}\")\n",
        "            # Trainer saves the best model automatically based on `load_best_model_at_end`\n",
        "            # If you want to explicitly save the *final* state regardless of best:\n",
        "            # model.save_pretrained(os.path.join(OUTPUT_DIR, \"final_model_state\"))\n",
        "            # Note: Be mindful that Trainer might save the *best* model to OUTPUT_DIR itself.\n",
        "            # Saving the adapter here might be redundant or overwrite if paths are the same.\n",
        "            # It's generally safer to rely on Trainer's saving for FFT best model.\n",
        "            # We'll save the processor separately for consistency.\n",
        "\n",
        "        # Save the processor in both cases\n",
        "        print(f\"Saving processor to: {FINAL_PROCESSOR_DIR}\")\n",
        "        # Ensure the directory exists before saving\n",
        "        os.makedirs(FINAL_PROCESSOR_DIR, exist_ok=True)\n",
        "        if processor:\n",
        "             processor.save_pretrained(FINAL_PROCESSOR_DIR)\n",
        "        else:\n",
        "             print(\"Warning: Processor object not found, cannot save.\")\n",
        "\n",
        "        print(f\"\\nModel {'adapter' if USE_LORA else 'weights (best checkpoint)'} and processor saved.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nAn error occurred during training: {e}\")\n",
        "        # Optionally try to save state even on error\n",
        "        try:\n",
        "            trainer.save_state()\n",
        "            print(\"Attempted to save trainer state after error.\")\n",
        "        except:\n",
        "            print(\"Could not save trainer state after error.\")\n",
        "else:\n",
        "    print(\"\\nTraining skipped due to errors in data loading, preprocessing, or trainer initialization.\")"
      ],
      "metadata": {
        "id": "h-7T4IVqd3vh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation Setup (Load ChartQA, Define Accuracy)"
      ],
      "metadata": {
        "id": "TyG-MLsEeRrX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Evaluation Setup ---\n",
        "\n",
        "print(\"\\n--- Preparing for Evaluation ---\")\n",
        "\n",
        "# Clear some memory before loading evaluation models/datasets\n",
        "gc.collect()\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# 1. Download ChartQA Dataset\n",
        "print(\"\\nDownloading ChartQA dataset...\")\n",
        "chartqa_test_dataset = None\n",
        "try:\n",
        "    # Use trust_remote_code=True if required by the dataset script\n",
        "    chartqa_test_dataset = load_dataset(\"HuggingFaceM4/ChartQA\", split=\"test\", cache_dir=CHARTQA_CACHE_DIR, trust_remote_code=True)\n",
        "    # Limit evaluation samples if needed\n",
        "    if EVAL_SAMPLE_LIMIT and EVAL_SAMPLE_LIMIT < len(chartqa_test_dataset):\n",
        "        print(f\"Limiting evaluation to {EVAL_SAMPLE_LIMIT} samples.\")\n",
        "        chartqa_test_dataset = chartqa_test_dataset.select(range(EVAL_SAMPLE_LIMIT))\n",
        "    print(f\"Loaded ChartQA test split with {len(chartqa_test_dataset)} examples.\")\n",
        "    print(\"\\nFirst ChartQA test sample structure:\")\n",
        "    print(chartqa_test_dataset[0])\n",
        "except Exception as e:\n",
        "    print(f\"Error downloading/loading ChartQA dataset: {e}\")\n",
        "\n",
        "\n",
        "# 2. Define Relaxed Accuracy Function (Modified for list label)\n",
        "def calculate_relaxed_accuracy(prediction, ground_truths):\n",
        "    \"\"\"\n",
        "    Checks if the prediction string is contained within any of the ground truth strings (case-insensitive).\n",
        "    Handles cases where ground_truths is a list (like in ChartQA).\n",
        "    \"\"\"\n",
        "    if not prediction or not ground_truths:\n",
        "        return False\n",
        "\n",
        "    # Ensure ground_truths is treated as a list, even if it's a single string initially\n",
        "    if isinstance(ground_truths, str):\n",
        "        ground_truths = [ground_truths]\n",
        "    elif not isinstance(ground_truths, list):\n",
        "         # Try converting to string if it's neither list nor string (e.g., number)\n",
        "         try:\n",
        "              ground_truths = [str(ground_truths)]\n",
        "         except:\n",
        "              print(f\"Warning: Could not convert ground_truth '{ground_truths}' to list/string.\")\n",
        "              return False # Cannot compare if format is unknown\n",
        "\n",
        "    prediction_lower = prediction.lower().strip()\n",
        "    if not prediction_lower: # Handle empty prediction string\n",
        "        return False\n",
        "\n",
        "    for gt in ground_truths:\n",
        "        gt_str = str(gt) if gt is not None else \"\"\n",
        "        gt_lower = gt_str.lower().strip()\n",
        "        if not gt_lower:\n",
        "            continue\n",
        "        # Check if prediction is a substring of ground truth\n",
        "        if prediction_lower in gt_lower:\n",
        "            return True\n",
        "        # Optional: Check if ground truth is a substring of prediction (more relaxed)\n",
        "        # if gt_lower in prediction_lower:\n",
        "        #     return True\n",
        "\n",
        "    return False\n",
        "\n",
        "EVAL_ACCURACY_FUNC = calculate_relaxed_accuracy\n",
        "print(f\"\\nUsing accuracy function: {EVAL_ACCURACY_FUNC.__name__}\")\n"
      ],
      "metadata": {
        "id": "U02AcDFTeW38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Helper function for inference (Updated prompt format)\n",
        "@torch.no_grad()\n",
        "def generate_eval_answer(model, processor, image, question, device, max_new_tokens=128):\n",
        "    \"\"\"Generates an answer for evaluation using the chat template.\"\"\"\n",
        "    if image is None or question is None:\n",
        "        print(\"Warning: Skipping inference due to None image or question.\")\n",
        "        return \"Error: Missing image or question\"\n",
        "\n",
        "    # Ensure image is PIL\n",
        "    if not isinstance(image, Image.Image):\n",
        "         try:\n",
        "              # Handle datasets.Image format\n",
        "              if isinstance(image, dict) and 'bytes' in image and image['bytes']:\n",
        "                   image = Image.open(io.BytesIO(image['bytes'])).convert(\"RGB\")\n",
        "              elif isinstance(image, dict) and 'path' in image and image['path']:\n",
        "                   image = Image.open(image['path']).convert(\"RGB\")\n",
        "              elif isinstance(image, bytes):\n",
        "                   image = Image.open(io.BytesIO(image)).convert(\"RGB\")\n",
        "              else:\n",
        "                   # Attempt conversion as a last resort\n",
        "                   image = Image.fromarray(np.array(image)).convert(\"RGB\")\n",
        "         except Exception as e:\n",
        "              print(f\"Warning: Could not process/convert image of type {type(image)}. Error: {e}. Skipping inference.\")\n",
        "              return \"Error: Image processing failed\"\n",
        "\n",
        "    # Ensure RGB\n",
        "    if image.mode != 'RGB':\n",
        "        image = image.convert(\"RGB\")\n",
        "\n",
        "    # Format prompt using the chat template structure for user turn\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"image\"},\n",
        "                {\"type\": \"text\", \"text\": question}\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        "    # Apply template, adding the generation prompt for the assistant's turn\n",
        "    prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
        "\n",
        "    try:\n",
        "        # Process inputs\n",
        "        inputs = processor(text=[prompt], images=[image], return_tensors=\"pt\", padding=True).to(device)\n",
        "\n",
        "        # Generate\n",
        "        # Set pad_token_id to eos_token_id for open-ended generation\n",
        "        generation_args = {\n",
        "            \"max_new_tokens\": max_new_tokens,\n",
        "            \"pad_token_id\": processor.tokenizer.eos_token_id,\n",
        "            \"eos_token_id\": processor.tokenizer.eos_token_id,\n",
        "            \"do_sample\": False, # Greedy decoding for deterministic eval\n",
        "            \"num_beams\": 1,\n",
        "        }\n",
        "\n",
        "        generated_ids = model.generate(**inputs, **generation_args)\n",
        "\n",
        "        # Decode only generated tokens\n",
        "        input_len = inputs['input_ids'].shape[1]\n",
        "        generated_texts = processor.batch_decode(generated_ids[:, input_len:], skip_special_tokens=True)\n",
        "\n",
        "        # Basic cleanup\n",
        "        generated_text = generated_texts[0].strip()\n",
        "        return generated_text\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during model inference for question '{question[:50]}...': {e}\")\n",
        "        # import traceback\n",
        "        # traceback.print_exc() # Uncomment for detailed traceback\n",
        "        return \"Error: Inference failed\"\n",
        "\n",
        "print(\"\\nEvaluation setup complete.\")"
      ],
      "metadata": {
        "id": "oFu6Mv7VeZ0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "06zkLcuRecCx"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b7d6af74504a4890bcaf8d906f04cf69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6463887c2c7b4dd081c9232774b450b9",
              "IPY_MODEL_d1c2e5acda75462ca60c9197db275cc0",
              "IPY_MODEL_fe7064b1048b4310b0d18b69a7e8676e"
            ],
            "layout": "IPY_MODEL_b2644bab6bbf4060aa4a3544da98023c"
          }
        },
        "6463887c2c7b4dd081c9232774b450b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d7c8f7758c834b3caaeac520ebf623c3",
            "placeholder": "​",
            "style": "IPY_MODEL_6c72da713790472e8d6ceac8598144b6",
            "value": "Preprocessing data: 100%"
          }
        },
        "d1c2e5acda75462ca60c9197db275cc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7edc6348437e47d3ae6e4170c8e0ddac",
            "max": 980,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d73f35e215aa487381895108889beef0",
            "value": 980
          }
        },
        "fe7064b1048b4310b0d18b69a7e8676e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef99a18230374771857e57e5780ad50e",
            "placeholder": "​",
            "style": "IPY_MODEL_f41d2cd4b6994e1584bc5617e345d253",
            "value": " 980/980 [06:25&lt;00:00,  3.11 examples/s]"
          }
        },
        "b2644bab6bbf4060aa4a3544da98023c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7c8f7758c834b3caaeac520ebf623c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c72da713790472e8d6ceac8598144b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7edc6348437e47d3ae6e4170c8e0ddac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d73f35e215aa487381895108889beef0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ef99a18230374771857e57e5780ad50e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f41d2cd4b6994e1584bc5617e345d253": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b77f2408fd6b4b76862302591798f611": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c8dbca17e0174ea0a9994649b820581f",
              "IPY_MODEL_1f9936673b45432689108378d95a488e",
              "IPY_MODEL_8b97a338cc9342709129ddedf19de1d5"
            ],
            "layout": "IPY_MODEL_7b7c96ced4e541838dbeef1e01a1c7fd"
          }
        },
        "c8dbca17e0174ea0a9994649b820581f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c3b3b55315f4777817e243437172922",
            "placeholder": "​",
            "style": "IPY_MODEL_20261957335a4d40a16da9ad90463a05",
            "value": "Filtering data: 100%"
          }
        },
        "1f9936673b45432689108378d95a488e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c51f1cd1abd4fe8ae0296e8ef93b5e1",
            "max": 980,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cd83ee61de2b4e29a014defffe1b09e8",
            "value": 980
          }
        },
        "8b97a338cc9342709129ddedf19de1d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c711c370e8ee485094c24cc232492768",
            "placeholder": "​",
            "style": "IPY_MODEL_044954eb042840dcbfd92fa84afdcd56",
            "value": " 980/980 [43:18&lt;00:00,  2.59s/ examples]"
          }
        },
        "7b7c96ced4e541838dbeef1e01a1c7fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c3b3b55315f4777817e243437172922": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20261957335a4d40a16da9ad90463a05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0c51f1cd1abd4fe8ae0296e8ef93b5e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd83ee61de2b4e29a014defffe1b09e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c711c370e8ee485094c24cc232492768": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "044954eb042840dcbfd92fa84afdcd56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ea5ccf8398284976af00cf48206e6385": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0e1c71bc84a741a9b6488af82f91e85d",
              "IPY_MODEL_ea536376948c41cd80ce563eea459bb6",
              "IPY_MODEL_f65caf4a3f5349128864ff81f4756480"
            ],
            "layout": "IPY_MODEL_2e1b1588990141e6b990533061e8e67f"
          }
        },
        "0e1c71bc84a741a9b6488af82f91e85d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ff7565888214f5fa63171fc1424a8e1",
            "placeholder": "​",
            "style": "IPY_MODEL_813f327290a64f60ad669e0c619625d1",
            "value": "Saving the dataset (62/62 shards): 100%"
          }
        },
        "ea536376948c41cd80ce563eea459bb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7dfcc9132032495db575577cbf02abf4",
            "max": 784,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b2a63a495bc14ccc9d8f39bab40e57d0",
            "value": 784
          }
        },
        "f65caf4a3f5349128864ff81f4756480": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca67b730ee3246c8a271a84172fbf346",
            "placeholder": "​",
            "style": "IPY_MODEL_43504f81b53e485e8fd6b3c7e9deb115",
            "value": " 784/784 [02:01&lt;00:00,  6.33 examples/s]"
          }
        },
        "2e1b1588990141e6b990533061e8e67f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ff7565888214f5fa63171fc1424a8e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "813f327290a64f60ad669e0c619625d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7dfcc9132032495db575577cbf02abf4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2a63a495bc14ccc9d8f39bab40e57d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ca67b730ee3246c8a271a84172fbf346": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43504f81b53e485e8fd6b3c7e9deb115": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eb5c7d04882b4c20b1a6d586d283f0a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cad073da3490440bbc117ec2d84ef68b",
              "IPY_MODEL_20960bf261ec49e9a3c4c7a706414cb4",
              "IPY_MODEL_1737b0be86e04d80af11333bf63455d6"
            ],
            "layout": "IPY_MODEL_fc6625c278e64eba9405a48b32360892"
          }
        },
        "cad073da3490440bbc117ec2d84ef68b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_87b936fbb8d64a5283a7b8d68ec0b87f",
            "placeholder": "​",
            "style": "IPY_MODEL_107f4de73f7d4f0684eb98b9737ca1b8",
            "value": "Saving the dataset (8/8 shards): 100%"
          }
        },
        "20960bf261ec49e9a3c4c7a706414cb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9e66e8133f7940158d0cc72a1c946d43",
            "max": 98,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2a8e7274e05c45b99f2d45e093fda3d6",
            "value": 98
          }
        },
        "1737b0be86e04d80af11333bf63455d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e6c02541f3c546e89c57b703d711ec32",
            "placeholder": "​",
            "style": "IPY_MODEL_a97d0266da914727977ae4b504fbf57f",
            "value": " 98/98 [00:15&lt;00:00,  6.57 examples/s]"
          }
        },
        "fc6625c278e64eba9405a48b32360892": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87b936fbb8d64a5283a7b8d68ec0b87f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "107f4de73f7d4f0684eb98b9737ca1b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9e66e8133f7940158d0cc72a1c946d43": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a8e7274e05c45b99f2d45e093fda3d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e6c02541f3c546e89c57b703d711ec32": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a97d0266da914727977ae4b504fbf57f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "034e4b3db4394d1982513203d474e0a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2200974f57fe4ac0be5733979f412d76",
              "IPY_MODEL_f6b321ccebf04392801efe151486db7b",
              "IPY_MODEL_f499a9d17c74482b8209118a922358f6"
            ],
            "layout": "IPY_MODEL_8fec0757b6874dc690ba2c34be544251"
          }
        },
        "2200974f57fe4ac0be5733979f412d76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8412957ddea64862a77997cb2aa56ef3",
            "placeholder": "​",
            "style": "IPY_MODEL_bc295cc6c38b4d018bbe6050a010401e",
            "value": "Saving the dataset (8/8 shards): 100%"
          }
        },
        "f6b321ccebf04392801efe151486db7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_edaf995b070d4cf18c4fb80ab305e31b",
            "max": 98,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2f83861a24d64cf59f8e5687efe4835d",
            "value": 98
          }
        },
        "f499a9d17c74482b8209118a922358f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a0feca7da74c4e9bb69b10c3e4d9dc95",
            "placeholder": "​",
            "style": "IPY_MODEL_66528a0ba1db452ea3010313ed4621cc",
            "value": " 98/98 [00:14&lt;00:00,  7.31 examples/s]"
          }
        },
        "8fec0757b6874dc690ba2c34be544251": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8412957ddea64862a77997cb2aa56ef3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc295cc6c38b4d018bbe6050a010401e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "edaf995b070d4cf18c4fb80ab305e31b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f83861a24d64cf59f8e5687efe4835d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a0feca7da74c4e9bb69b10c3e4d9dc95": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66528a0ba1db452ea3010313ed4621cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}