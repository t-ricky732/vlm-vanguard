{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d83b802-887f-4e95-8540-136c17a629df",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99cd3dce-5ab8-42dd-9509-caa6a16d8769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully configured pip to use AWS CodeArtifact repository https://amazon-149122183214.d.codeartifact.us-west-2.amazonaws.com/pypi/shared/ \n",
      "Login expires in 12 hours at 2025-04-25 01:41:52+00:00\n"
     ]
    }
   ],
   "source": [
    "!aws codeartifact login --tool pip --repository shared --domain amazon --domain-owner 149122183214 --region us-west-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4f7260d-cfd1-45e5-aae5-57dd590be0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install  -U -q transformers trl datasets bitsandbytes peft accelerate triton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a005c53-c4f1-4ab2-9cf4-9107fe8d214b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e7633d9-3ddd-4471-81b7-68e89fb32f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q tensorboard\n",
    "!pip install -q word2number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c6bfa27-ac0b-41b6-9490-061483edf187",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "790d4da0-eef8-4012-bbaa-6a500361bd18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU allocated memory: 0.00 GB\n",
      "GPU reserved memory: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import time\n",
    "\n",
    "def clear_memory():\n",
    "    # Delete variables if they exist in the current global scope\n",
    "    if \"inputs\" in globals():\n",
    "        del globals()[\"inputs\"]\n",
    "    if \"model\" in globals():\n",
    "        del globals()[\"model\"]\n",
    "    if \"processor\" in globals():\n",
    "        del globals()[\"processor\"]\n",
    "    if \"trainer\" in globals():\n",
    "        del globals()[\"trainer\"]\n",
    "    if \"peft_model\" in globals():\n",
    "        del globals()[\"peft_model\"]\n",
    "    if \"bnb_config\" in globals():\n",
    "        del globals()[\"bnb_config\"]\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Garbage collection and clearing CUDA memory\n",
    "    gc.collect()\n",
    "    time.sleep(2)\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    time.sleep(2)\n",
    "    gc.collect()\n",
    "    time.sleep(2)\n",
    "\n",
    "    print(f\"GPU allocated memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "    print(f\"GPU reserved memory: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
    "\n",
    "\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e67363-8216-4855-8755-fcaf089d165f",
   "metadata": {},
   "source": [
    "## Base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3d7d987-a855-4df8-8364-a143a1bf61cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Resources & Precision ---\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DTYPE = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float16\n",
    "ATTN_IMPLEMENTATION = \"flash_attention_2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a38a358a-39e5-4e3a-83ae-7e3a4f56f2cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 13:42:11.819308: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745502131.834960    2142 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745502131.839780    2142 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-24 13:42:11.855455: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Hugging Face transformers\n",
    "from transformers import (\n",
    "    AutoProcessor,\n",
    "    Idefics3ForConditionalGeneration,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    AutoConfig\n",
    ")\n",
    "\n",
    "MODEL_ID = \"HuggingFaceTB/SmolVLM-500M-Base\" # specify model to use here\n",
    "\n",
    "base_model_config = AutoConfig.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "base_model_config.use_cache = True\n",
    "\n",
    "if hasattr(base_model_config, \"attn_implementation\"):\n",
    "    base_model_config.attn_implementation = ATTN_IMPLEMENTATION\n",
    "\n",
    "base_load_kwargs = {\"device_map\": \"auto\", \"torch_dtype\": DTYPE}\n",
    "\n",
    "base_model_eval = Idefics3ForConditionalGeneration.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    config=base_model_config,\n",
    "    trust_remote_code=True,\n",
    "    **base_load_kwargs\n",
    ")\n",
    "\n",
    "base_model_eval.eval()\n",
    "print(\"Base model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2883b2-4f66-42c5-948f-3701e089186b",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9deac1ee-b18c-449d-b57c-5ca7a045b052",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "WORKING_DIR = \"./\"\n",
    "OUTPUT_DIR_BASE_NAME = \"base_evaluation\"\n",
    "\n",
    "OUTPUT_DIR = os.path.join(WORKING_DIR, OUTPUT_DIR_BASE_NAME) # full path on Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b487bcf-7f92-4813-a272-a5bf09d4ebce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Evaluation Config ---\n",
    "CHARTQA_DATASET_ID = \"HuggingFaceM4/ChartQA\"\n",
    "EVAL_SPLIT = \"test\"\n",
    "EVAL_LIMIT = None # number of chartqa samples to use when evaluating ChartQA\n",
    "MAX_NEW_TOKENS_EVAL = 32\n",
    "EVAL_OUTPUT_FILE = os.path.join(OUTPUT_DIR, MODEL_ID, \"chartqa_evaluation_results_comparison.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3081b667-5f31-4c4d-a514-0fc4e45c97df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2500 samples for evaluation (full test set).\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "chartqa_test_iterable = load_dataset(CHARTQA_DATASET_ID, split=EVAL_SPLIT, streaming=False)\n",
    "chartqa_test_dataset = list(chartqa_test_iterable)\n",
    "print(f\"Loaded {len(chartqa_test_dataset)} samples for evaluation (full {EVAL_SPLIT} set).\")\n",
    "\n",
    "if chartqa_test_dataset:\n",
    "     if isinstance(chartqa_test_dataset[0], dict) and 'img_idx' not in chartqa_test_dataset[0]:\n",
    "          chartqa_test_dataset = [dict(sample, img_idx=i) for i, sample in enumerate(chartqa_test_dataset)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86f91aaf-3cdd-453b-9a69-fcce0536d1d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Defining Evaluation Helper Functions ---\n",
      "Evaluation helper functions defined.\n"
     ]
    }
   ],
   "source": [
    "# Define Evaluation helper functions\n",
    "print(\"\\n--- Defining Evaluation Helper Functions ---\")\n",
    "\n",
    "# System message for evaluation prompting (provided by SmolVLM)\n",
    "EVAL_SYSTEM_MESSAGE = \"\"\"You are a Vision Language Model specialized in interpreting visual data from chart images.\n",
    "Your task is to analyze the provided chart image and respond to queries with concise answers, usually a single word, number, or short phrase.\n",
    "The charts include a variety of types (e.g., line charts, bar charts) and contain colors, labels, and text.\n",
    "Focus on delivering accurate, succinct answers based on the visual information. Avoid additional explanation unless absolutely necessary.\"\"\"\n",
    "\n",
    "def create_inference_chat_messages(sample):\n",
    "    \"\"\"Creates the chat message structure for inference using ChartQA's 'query' field.\"\"\"\n",
    "    question = sample.get(\"query\")\n",
    "    if not question:\n",
    "        return None\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": EVAL_SYSTEM_MESSAGE}]},\n",
    "        {\"role\": \"user\", \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": question}]},\n",
    "    ]\n",
    "\n",
    "\n",
    "print(\"Evaluation helper functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "005599c5-7ac7-47d1-83c8-26ce3fdfe7f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Evaluation Generation & Saving ---\n"
     ]
    }
   ],
   "source": [
    "# --- Generate Predictions and Save Results ---\n",
    "\n",
    "# Image processing\n",
    "from PIL import Image as PILImage\n",
    "\n",
    "print(\"\\n--- Running Evaluation Generation & Saving ---\")\n",
    "\n",
    "# --- Define Concise Generation Function ---\n",
    "def generate_prediction_concise(model, processor, sample, max_tokens):\n",
    "    question, image = sample.get(\"query\"), sample.get(\"image\")\n",
    "    if not (question and isinstance(image, Image.Image)): return \"ERROR:Input\"\n",
    "    if image.mode != 'RGB':\n",
    "        try: image = image.convert('RGB')\n",
    "        except: return \"ERROR:Convert\"\n",
    "\n",
    "    messages = create_inference_chat_messages(sample)\n",
    "    if not messages: return \"ERROR:Messages\"\n",
    "\n",
    "    prediction = \"ERROR:Generate\"\n",
    "    inputs = None; generated_ids = None\n",
    "    try:\n",
    "        prompt = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "        device = next(iter(model.parameters()), torch.tensor([])).device\n",
    "        if device is None: device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        inputs = processor(text=[prompt], images=[image], return_tensors=\"pt\", padding=True).to(device)\n",
    "        generated_ids = model.generate(**inputs, max_new_tokens=max_tokens, do_sample=False,\n",
    "                                       pad_token_id=processor.tokenizer.pad_token_id,\n",
    "                                       eos_token_id=processor.tokenizer.eos_token_id)\n",
    "        gen_tokens = generated_ids[0, inputs[\"input_ids\"].shape[1]:]\n",
    "        prediction = processor.decode(gen_tokens, skip_special_tokens=True).strip() if gen_tokens.numel() > 0 else \"[NO_TOKENS]\"\n",
    "    except Exception as e:\n",
    "        print(f\"\\n⚠️ Exception occurred during prediction:\\nType: {e.__class__.__name__}\\nMessage: {e}\")\n",
    "        prediction = f\"ERROR:{e.__class__.__name__}\"\n",
    "    finally:\n",
    "        del inputs, generated_ids\n",
    "    return prediction\n",
    "# --- End Generation Function ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea4cfaee-747a-4b64-9281-d5517eb579ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading processor for model: HuggingFaceTB/SmolVLM-500M-Base\n",
      "Processor loaded.\n"
     ]
    }
   ],
   "source": [
    "# load Processor\n",
    "print(f\"Loading processor for model: {MODEL_ID}\")\n",
    "eval_processor = AutoProcessor.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "if eval_processor.tokenizer.pad_token_id is None:\n",
    "    eval_processor.tokenizer.pad_token = eval_processor.tokenizer.eos_token\n",
    "    if hasattr(eval_processor, 'pad_token') and eval_processor.pad_token is None:\n",
    "        eval_processor.pad_token = eval_processor.tokenizer.eos_token\n",
    "print(\"Processor loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74a1456-8ff7-45c0-b108-57695ce1c662",
   "metadata": {},
   "source": [
    "## Generate prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c130793a-0562-4d87-9d81-ee00eb331e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions for 1 model(s) on 2500 samples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e443cb478b504bcfbbb3fad8895c900c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Predictions:   0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation results (raw predictions) saved to: ./base_evaluation/HuggingFaceTB/SmolVLM-500M-Base/chartqa_evaluation_results_comparison.json\n",
      "\n",
      "--- Evaluation Generation & Saving Finished ---\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "# Determine models to evaluate\n",
    "models_to_eval = {}\n",
    "models_to_eval[\"base\"] = base_model_eval\n",
    "\n",
    "results_list = []\n",
    "# Check dataset and models before looping\n",
    "print(f\"Generating predictions for {len(models_to_eval)} model(s) on {len(chartqa_test_dataset)} samples...\")\n",
    "# --- Generate Predictions ---\n",
    "for sample in tqdm(chartqa_test_dataset, desc=\"Generating Predictions\"):\n",
    "    entry = {\"id\": sample.get(\"img_idx\", \"N/A\"),\n",
    "             \"question\": sample.get(\"query\"),\n",
    "             \"ground_truth\": str(sample.get(\"label\"))} # Convert GT to string here\n",
    "\n",
    "    # Basic validation of core sample data needed for processing\n",
    "    if not all([entry[\"id\"] != \"N/A\", entry[\"question\"], entry[\"ground_truth\"] is not None, isinstance(sample.get(\"image\"), Image.Image)]):\n",
    "        print(f\"Warning: Skipping sample {entry['id']} due to missing/invalid core data.\")\n",
    "        continue # Skip this sample\n",
    "\n",
    "    # Generate for applicable models\n",
    "    for name, model_obj in models_to_eval.items():\n",
    "        entry[f\"predicted_answer_{name}\"] = generate_prediction_concise(model_obj, eval_processor, sample, MAX_NEW_TOKENS_EVAL)\n",
    "    results_list.append(entry)\n",
    "# --- End Prediction Loop ---\n",
    "\n",
    "# --- Save results_df to JSON ---\n",
    "if results_list:\n",
    "    results_df = pd.DataFrame(results_list)\n",
    "    if 'EVAL_OUTPUT_FILE' in locals() and EVAL_OUTPUT_FILE:\n",
    "        try:\n",
    "            os.makedirs(os.path.dirname(EVAL_OUTPUT_FILE), exist_ok=True)\n",
    "            results_df.to_json(EVAL_OUTPUT_FILE, orient=\"records\", indent=2)\n",
    "            print(f\"\\nEvaluation results (raw predictions) saved to: {EVAL_OUTPUT_FILE}\")\n",
    "        except Exception as e_save:\n",
    "             print(f\"\\nERROR saving evaluation results to {EVAL_OUTPUT_FILE}: {e_save}\")\n",
    "    else:\n",
    "         print(\"\\nWarning: EVAL_OUTPUT_FILE not defined, results not saved.\")\n",
    "else:\n",
    "    print(\"No valid evaluation results were generated to save.\")\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\n--- Evaluation Generation & Saving Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "905910c9-cf29-4e9f-99b0-34b1416233dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Aggregating Evaluation Results ---\n",
      "Aggregated Results Preview:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>Pred_SmolVLM-500M-Original</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>How many food item is shown in the bar graph?</td>\n",
       "      <td>['14']</td>\n",
       "      <td>What is the purpose of the text?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>What is the difference in value between Lamb a...</td>\n",
       "      <td>['0.57']</td>\n",
       "      <td>What is the difference between a \"good\" and \"b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>How many bars are shown in the chart?</td>\n",
       "      <td>['3']</td>\n",
       "      <td>What is the purpose of the text?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Is the sum value of Madagascar more then Fiji?</td>\n",
       "      <td>['No']</td>\n",
       "      <td>What is the purpose of the text?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>What's the value of the lowest bar?</td>\n",
       "      <td>['23']</td>\n",
       "      <td>What is the purpose of the text?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                           question ground_truth  \\\n",
       "0   0      How many food item is shown in the bar graph?       ['14']   \n",
       "1   1  What is the difference in value between Lamb a...     ['0.57']   \n",
       "2   2              How many bars are shown in the chart?        ['3']   \n",
       "3   3     Is the sum value of Madagascar more then Fiji?       ['No']   \n",
       "4   4                What's the value of the lowest bar?       ['23']   \n",
       "\n",
       "                          Pred_SmolVLM-500M-Original  \n",
       "0                   What is the purpose of the text?  \n",
       "1  What is the difference between a \"good\" and \"b...  \n",
       "2                   What is the purpose of the text?  \n",
       "3                   What is the purpose of the text?  \n",
       "4                   What is the purpose of the text?  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated results saved to base_evaluation/HuggingFaceTB/SmolVLM-500M-Base/base_evaluation-ALL_RUNS_COMPARISON_WITH_BASE.csv\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from functools import reduce\n",
    "\n",
    "### Final results compilation\n",
    "print(\"--- Aggregating Evaluation Results ---\")\n",
    "\n",
    "BASE_OUTPUT_DIR = Path(WORKING_DIR)\n",
    "OUTPUT_FOLDER_PREFIX = OUTPUT_DIR_BASE_NAME\n",
    "EVAL_FILENAME = \"chartqa_evaluation_results_comparison.json\"\n",
    "\n",
    "search_pattern = f\"{OUTPUT_FOLDER_PREFIX}/{MODEL_ID}/{EVAL_FILENAME}\"\n",
    "result_files = list(BASE_OUTPUT_DIR.glob(search_pattern))\n",
    "\n",
    "# if not result_files:\n",
    "#     raise FileNotFoundError(\"No evaluation result files found.\")\n",
    "\n",
    "aggregated_dfs = []\n",
    "base_models_added = set()\n",
    "\n",
    "for file_path in result_files:\n",
    "    run_label = file_path.parent.name.replace(f\"{OUTPUT_FOLDER_PREFIX}-\", \"\")\n",
    "    parts = run_label.split('-')\n",
    "    base_model_name = '-'.join(parts[:-1]) if len(parts) >= 2 else \"UnknownBase\"\n",
    "    base_label = f\"{base_model_name}-Original\"\n",
    "\n",
    "    df = pd.read_json(file_path)\n",
    "\n",
    "    columns = ['id', 'question', 'ground_truth'] if not aggregated_dfs else ['id']\n",
    "    rename_dict = {}\n",
    "\n",
    "    if 'predicted_answer_finetuned' in df:\n",
    "        columns.append('predicted_answer_finetuned')\n",
    "        rename_dict['predicted_answer_finetuned'] = f\"Pred_{run_label}\"\n",
    "\n",
    "    if base_model_name not in base_models_added and 'predicted_answer_base' in df:\n",
    "        columns.append('predicted_answer_base')\n",
    "        rename_dict['predicted_answer_base'] = f\"Pred_{base_label}\"\n",
    "        base_models_added.add(base_model_name)\n",
    "\n",
    "    df_subset = df[columns].rename(columns=rename_dict)\n",
    "    aggregated_dfs.append(df_subset)\n",
    "\n",
    "if aggregated_dfs:\n",
    "    final_comparison_df = reduce(lambda left, right: pd.merge(left, right, on='id', how='outer'), aggregated_dfs)\n",
    "    final_comparison_df.ffill(inplace=True)\n",
    "    final_comparison_df.bfill(inplace=True)\n",
    "else:\n",
    "    print(\"⚠️ No evaluation results found to aggregate.\")\n",
    "    final_comparison_df = pd.DataFrame()\n",
    "\n",
    "\n",
    "print(\"Aggregated Results Preview:\")\n",
    "display(final_comparison_df.head())\n",
    "\n",
    "output_file = BASE_OUTPUT_DIR / OUTPUT_DIR_BASE_NAME / MODEL_ID /f\"{OUTPUT_FOLDER_PREFIX}-ALL_RUNS_COMPARISON_WITH_BASE.csv\"\n",
    "final_comparison_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Aggregated results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10316636-431a-44e3-a2c6-86aa954a1c1b",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d12659b6-d9d0-4621-b6af-b584cd60e631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SmolVLM-500M-Original EM=  0.00%  Relaxed=  0.00%  (3 num / 3 valid)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EM (%)</th>\n",
       "      <th>Relaxed Num (%)</th>\n",
       "      <th># Numeric</th>\n",
       "      <th># Valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SmolVLM-500M-Original</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       EM (%)  Relaxed Num (%)  # Numeric  # Valid\n",
       "SmolVLM-500M-Original     0.0              0.0        3.0      3.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>Pred_SmolVLM-500M-Original</th>\n",
       "      <th>GT_proc</th>\n",
       "      <th>Pred_SmolVLM-500M-Original_proc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>How many food item is shown in the bar graph?</td>\n",
       "      <td>['14']</td>\n",
       "      <td>What is the purpose of the text?</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>What is the difference in value between Lamb a...</td>\n",
       "      <td>['0.57']</td>\n",
       "      <td>What is the difference between a \"good\" and \"b...</td>\n",
       "      <td>0.57</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>How many bars are shown in the chart?</td>\n",
       "      <td>['3']</td>\n",
       "      <td>What is the purpose of the text?</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Is the sum value of Madagascar more then Fiji?</td>\n",
       "      <td>['No']</td>\n",
       "      <td>What is the purpose of the text?</td>\n",
       "      <td>no</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>What's the value of the lowest bar?</td>\n",
       "      <td>['23']</td>\n",
       "      <td>What is the purpose of the text?</td>\n",
       "      <td>23.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                           question ground_truth  \\\n",
       "0   0      How many food item is shown in the bar graph?       ['14']   \n",
       "1   1  What is the difference in value between Lamb a...     ['0.57']   \n",
       "2   2              How many bars are shown in the chart?        ['3']   \n",
       "3   3     Is the sum value of Madagascar more then Fiji?       ['No']   \n",
       "4   4                What's the value of the lowest bar?       ['23']   \n",
       "\n",
       "                          Pred_SmolVLM-500M-Original GT_proc  \\\n",
       "0                   What is the purpose of the text?    14.0   \n",
       "1  What is the difference between a \"good\" and \"b...    0.57   \n",
       "2                   What is the purpose of the text?     3.0   \n",
       "3                   What is the purpose of the text?      no   \n",
       "4                   What is the purpose of the text?    23.0   \n",
       "\n",
       "   Pred_SmolVLM-500M-Original_proc  \n",
       "0                              NaN  \n",
       "1                              NaN  \n",
       "2                              NaN  \n",
       "3                              NaN  \n",
       "4                              NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾  Saved: base_evaluation/HuggingFaceTB/SmolVLM-500M-Base/base_evaluation-ALL_RUNS_COMPARISON_WITH_PROCESSED.csv\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from word2number import w2n\n",
    "\n",
    "# --- Advanced accuracy metrics -------------------------------\n",
    "\n",
    "YES, NO      = {\"yes\",\"y\",\"true\",\"correct\"}, {\"no\",\"n\",\"false\",\"incorrect\"}\n",
    "TOL, EPS     = 0.05, 1e-9                  # ±5 %, tiny tolerance for 0\n",
    "PRED_PREFIX  = \"Pred_\"                     # rename if your columns differ\n",
    "\n",
    "# 2.  Helpers -------------------------------------------------------------------\n",
    "def to_scalar(v):\n",
    "    \"\"\"Clean & convert value → float | 'yes' | 'no' | None.\"\"\"\n",
    "    if pd.isna(v): return None\n",
    "    s = str(v).strip()\n",
    "    m = re.fullmatch(r\"\\[['\\\"]?(.*?)['\\\"]?\\]\", s)\n",
    "    if m: s = m.group(1)\n",
    "    s = s.lower().replace(\",\",\"\").replace(\"$\",\"\").replace(\"%\",\"\").strip()\n",
    "    if s in YES: return \"yes\"\n",
    "    if s in NO:  return \"no\"\n",
    "    for fn in (float, w2n.word_to_num):\n",
    "        try: return float(fn(s))\n",
    "        except Exception: pass\n",
    "    return None\n",
    "\n",
    "def relaxed(gt, pred):\n",
    "    return abs(gt - pred) <= (abs(gt) * TOL or EPS)\n",
    "\n",
    "# 3.  Basic checks --------------------------------------------------------------\n",
    "if not isinstance(globals().get(\"final_comparison_df\"), pd.DataFrame):\n",
    "    sys.exit(\"  `final_comparison_df` is missing.\")\n",
    "\n",
    "df = final_comparison_df.copy()\n",
    "df[\"GT_proc\"] = df[\"ground_truth\"].map(to_scalar)\n",
    "\n",
    "keep = df[\"GT_proc\"].isin([\"yes\",\"no\"]) | df[\"GT_proc\"].apply(lambda x: isinstance(x,(int,float)))\n",
    "df_filt = df[keep]\n",
    "if df_filt.empty:\n",
    "    sys.exit(\"  No yes/no/numeric ground‑truth rows after processing.\")\n",
    "\n",
    "# Metrics per run -----------------------------------------------------------\n",
    "results = {}\n",
    "for col in [c for c in df.columns if c.startswith(PRED_PREFIX)]:\n",
    "    proc = f\"{col}_proc\"\n",
    "    df[proc] = df[col].map(to_scalar)\n",
    "\n",
    "    valid   = df[[\"GT_proc\", proc]].dropna()\n",
    "    numeric = valid[valid[\"GT_proc\"].apply(lambda x:isinstance(x,(int,float))) &\n",
    "                    valid[proc].apply(lambda x:isinstance(x,(int,float)))]\n",
    "\n",
    "    em  = 100 * (valid[\"GT_proc\"] == valid[proc]).mean() if not valid.empty else 0\n",
    "    rel = 100 * numeric.apply(lambda r: relaxed(r[\"GT_proc\"], r[proc]), axis=1).mean() if not numeric.empty else 0\n",
    "\n",
    "    run = col.replace(PRED_PREFIX, \"\")\n",
    "    results[run] = {\"EM (%)\": round(em,2),\n",
    "                    \"Relaxed Num (%)\": round(rel,2),\n",
    "                    \"# Numeric\": len(numeric),\n",
    "                    \"# Valid\": len(valid)}\n",
    "\n",
    "    print(f\"{run:<15} EM={em:6.2f}%  Relaxed={rel:6.2f}%  ({len(numeric)} num / {len(valid)} valid)\")\n",
    "\n",
    "# Summary & preview ---------------------------------------------------------\n",
    "summary = pd.DataFrame(results).T.sort_values(\"EM (%)\", ascending=False)\n",
    "display(summary)\n",
    "display(df.head())\n",
    "\n",
    "# Save to CSV ---------------------------------------------------------------\n",
    "BASE_OUTPUT_DIR = Path(globals().get(\"BASE_OUTPUT_DIR\", \"./outputs\"))\n",
    "BASE_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "prefix = globals().get(\"OUTPUT_FOLDER_PREFIX\", \"RUNS\")\n",
    "out_file = BASE_OUTPUT_DIR / OUTPUT_DIR_BASE_NAME / MODEL_ID / f\"{prefix}-ALL_RUNS_COMPARISON_WITH_PROCESSED.csv\"\n",
    "summary_file = BASE_OUTPUT_DIR / OUTPUT_DIR_BASE_NAME / MODEL_ID / f\"{prefix}-result.csv\"\n",
    "df.to_csv(out_file, index=False)\n",
    "summary.to_csv(summary_file, index=False)\n",
    "print(\"💾  Saved:\", out_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d8c154-8219-4a70-b228-3510867b94e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
