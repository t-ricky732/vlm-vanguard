{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d83b802-887f-4e95-8540-136c17a629df",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99cd3dce-5ab8-42dd-9509-caa6a16d8769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully configured pip to use AWS CodeArtifact repository https://amazon-149122183214.d.codeartifact.us-west-2.amazonaws.com/pypi/shared/ \n",
      "Login expires in 12 hours at 2025-04-24 20:43:15+00:00\n"
     ]
    }
   ],
   "source": [
    "!aws codeartifact login --tool pip --repository shared --domain amazon --domain-owner 149122183214 --region us-west-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4f7260d-cfd1-45e5-aae5-57dd590be0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install  -U -q transformers==4.46.3 trl==0.12.1 datasets bitsandbytes peft accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a005c53-c4f1-4ab2-9cf4-9107fe8d214b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e7633d9-3ddd-4471-81b7-68e89fb32f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c6bfa27-ac0b-41b6-9490-061483edf187",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e67363-8216-4855-8755-fcaf089d165f",
   "metadata": {},
   "source": [
    "## Base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3d7d987-a855-4df8-8364-a143a1bf61cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Resources & Precision ---\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DTYPE = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float16\n",
    "ATTN_IMPLEMENTATION = \"flash_attention_2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a38a358a-39e5-4e3a-83ae-7e3a4f56f2cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 08:43:24.033468: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-24 08:43:24.049194: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-04-24 08:43:24.054127: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-24 08:43:24.065549: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Hugging Face transformers\n",
    "from transformers import (\n",
    "    AutoProcessor,\n",
    "    Idefics3ForConditionalGeneration,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    AutoConfig\n",
    ")\n",
    "\n",
    "\n",
    "MODEL_ID = \"HuggingFaceTB/SmolVLM-Base\" # specify model to use here\n",
    "\n",
    "base_model_config = AutoConfig.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "base_model_config.use_cache = True\n",
    "\n",
    "if hasattr(base_model_config, \"attn_implementation\"):\n",
    "    base_model_config.attn_implementation = ATTN_IMPLEMENTATION\n",
    "\n",
    "base_load_kwargs = {\"device_map\": \"auto\", \"torch_dtype\": DTYPE}\n",
    "\n",
    "base_model_eval = Idefics3ForConditionalGeneration.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    config=base_model_config,\n",
    "    trust_remote_code=True,\n",
    "    **base_load_kwargs\n",
    ")\n",
    "\n",
    "base_model_eval.eval()\n",
    "print(\"Base model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2883b2-4f66-42c5-948f-3701e089186b",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9deac1ee-b18c-449d-b57c-5ca7a045b052",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "WORKING_DIR = \"./\"\n",
    "OUTPUT_DIR_BASE_NAME = \"base_evaluation\"\n",
    "\n",
    "OUTPUT_DIR = os.path.join(WORKING_DIR, OUTPUT_DIR_BASE_NAME) # full path on Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b487bcf-7f92-4813-a272-a5bf09d4ebce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Evaluation Config ---\n",
    "CHARTQA_DATASET_ID = \"HuggingFaceM4/ChartQA\"\n",
    "EVAL_SPLIT = \"test\"\n",
    "EVAL_LIMIT = None # number of chartqa samples to use when evaluating ChartQA\n",
    "MAX_NEW_TOKENS_EVAL = 32\n",
    "EVAL_OUTPUT_FILE = os.path.join(OUTPUT_DIR, MODEL_ID, \"chartqa_evaluation_results_comparison.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3081b667-5f31-4c4d-a514-0fc4e45c97df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2500 samples for evaluation (full test set).\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "chartqa_test_iterable = load_dataset(CHARTQA_DATASET_ID, split=EVAL_SPLIT, streaming=False)\n",
    "chartqa_test_dataset = list(chartqa_test_iterable)\n",
    "print(f\"Loaded {len(chartqa_test_dataset)} samples for evaluation (full {EVAL_SPLIT} set).\")\n",
    "\n",
    "if chartqa_test_dataset:\n",
    "     if isinstance(chartqa_test_dataset[0], dict) and 'img_idx' not in chartqa_test_dataset[0]:\n",
    "          chartqa_test_dataset = [dict(sample, img_idx=i) for i, sample in enumerate(chartqa_test_dataset)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86f91aaf-3cdd-453b-9a69-fcce0536d1d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Defining Evaluation Helper Functions ---\n",
      "Evaluation helper functions defined.\n"
     ]
    }
   ],
   "source": [
    "# Define Evaluation helper functions\n",
    "print(\"\\n--- Defining Evaluation Helper Functions ---\")\n",
    "\n",
    "# System message for evaluation prompting (provided by SmolVLM)\n",
    "EVAL_SYSTEM_MESSAGE = \"\"\"You are a Vision Language Model specialized in interpreting visual data from chart images.\n",
    "Your task is to analyze the provided chart image and respond to queries with concise answers, usually a single word, number, or short phrase.\n",
    "The charts include a variety of types (e.g., line charts, bar charts) and contain colors, labels, and text.\n",
    "Focus on delivering accurate, succinct answers based on the visual information. Avoid additional explanation unless absolutely necessary.\"\"\"\n",
    "\n",
    "def create_inference_chat_messages(sample):\n",
    "    \"\"\"Creates the chat message structure for inference using ChartQA's 'query' field.\"\"\"\n",
    "    question = sample.get(\"query\")\n",
    "    if not question:\n",
    "        return None\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": EVAL_SYSTEM_MESSAGE}]},\n",
    "        {\"role\": \"user\", \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": question}]},\n",
    "    ]\n",
    "\n",
    "\n",
    "print(\"Evaluation helper functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "005599c5-7ac7-47d1-83c8-26ce3fdfe7f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Evaluation Generation & Saving ---\n"
     ]
    }
   ],
   "source": [
    "# --- Generate Predictions and Save Results ---\n",
    "\n",
    "print(\"\\n--- Running Evaluation Generation & Saving ---\")\n",
    "\n",
    "# --- Define Concise Generation Function ---\n",
    "def generate_prediction_concise(model, processor, sample, max_tokens):\n",
    "    question, image = sample.get(\"query\"), sample.get(\"image\")\n",
    "    if not (question and isinstance(image, Image.Image)): return \"ERROR:Input\"\n",
    "    if image.mode != 'RGB':\n",
    "        try: image = image.convert('RGB')\n",
    "        except: return \"ERROR:Convert\"\n",
    "\n",
    "    messages = create_inference_chat_messages(sample)\n",
    "    if not messages: return \"ERROR:Messages\"\n",
    "\n",
    "    prediction = \"ERROR:Generate\"\n",
    "    inputs = None; generated_ids = None\n",
    "    try:\n",
    "        prompt = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "        device = next(iter(model.parameters()), torch.tensor([])).device\n",
    "        if device is None: device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        inputs = processor(text=[prompt], images=[image], return_tensors=\"pt\", padding=True).to(device)\n",
    "        generated_ids = model.generate(**inputs, max_new_tokens=max_tokens, do_sample=False,\n",
    "                                       pad_token_id=processor.tokenizer.pad_token_id,\n",
    "                                       eos_token_id=processor.tokenizer.eos_token_id)\n",
    "        gen_tokens = generated_ids[0, inputs[\"input_ids\"].shape[1]:]\n",
    "        prediction = processor.decode(gen_tokens, skip_special_tokens=True).strip() if gen_tokens.numel() > 0 else \"[NO_TOKENS]\"\n",
    "    except Exception as e:\n",
    "        prediction = f\"ERROR:{e.__class__.__name__}\"\n",
    "    finally:\n",
    "        del inputs, generated_ids\n",
    "    return prediction\n",
    "# --- End Generation Function ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea4cfaee-747a-4b64-9281-d5517eb579ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading processor for model: HuggingFaceTB/SmolVLM-Base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some kwargs in processor config are unused and will not have any effect: image_seq_len. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processor loaded.\n"
     ]
    }
   ],
   "source": [
    "# load Processor\n",
    "print(f\"Loading processor for model: {MODEL_ID}\")\n",
    "eval_processor = AutoProcessor.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "if eval_processor.tokenizer.pad_token_id is None:\n",
    "    eval_processor.tokenizer.pad_token = eval_processor.tokenizer.eos_token\n",
    "    if hasattr(eval_processor, 'pad_token') and eval_processor.pad_token is None:\n",
    "        eval_processor.pad_token = eval_processor.tokenizer.eos_token\n",
    "print(\"Processor loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74a1456-8ff7-45c0-b108-57695ce1c662",
   "metadata": {},
   "source": [
    "## Generate prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c130793a-0562-4d87-9d81-ee00eb331e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions for 1 model(s) on 2500 samples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7322e72d62a4e3ba51b94199be6bfd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Predictions:   0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "# Determine models to evaluate\n",
    "models_to_eval = {}\n",
    "models_to_eval[\"base\"] = base_model_eval\n",
    "\n",
    "results_list = []\n",
    "# Check dataset and models before looping\n",
    "print(f\"Generating predictions for {len(models_to_eval)} model(s) on {len(chartqa_test_dataset)} samples...\")\n",
    "# --- Generate Predictions ---\n",
    "for sample in tqdm(chartqa_test_dataset, desc=\"Generating Predictions\"):\n",
    "    entry = {\"id\": sample.get(\"img_idx\", \"N/A\"),\n",
    "             \"question\": sample.get(\"query\"),\n",
    "             \"ground_truth\": str(sample.get(\"label\"))} # Convert GT to string here\n",
    "\n",
    "    # Basic validation of core sample data needed for processing\n",
    "    if not all([entry[\"id\"] != \"N/A\", entry[\"question\"], entry[\"ground_truth\"] is not None, isinstance(sample.get(\"image\"), Image.Image)]):\n",
    "        print(f\"Warning: Skipping sample {entry['id']} due to missing/invalid core data.\")\n",
    "        continue # Skip this sample\n",
    "\n",
    "    # Generate for applicable models\n",
    "    for name, model_obj in models_to_eval.items():\n",
    "        entry[f\"predicted_answer_{name}\"] = generate_prediction_concise(model_obj, eval_processor, sample, MAX_NEW_TOKENS_EVAL)\n",
    "    results_list.append(entry)\n",
    "# --- End Prediction Loop ---\n",
    "\n",
    "# --- Save results_df to JSON ---\n",
    "if results_list:\n",
    "    results_df = pd.DataFrame(results_list)\n",
    "    if 'EVAL_OUTPUT_FILE' in locals() and EVAL_OUTPUT_FILE:\n",
    "        try:\n",
    "            os.makedirs(os.path.dirname(EVAL_OUTPUT_FILE), exist_ok=True)\n",
    "            results_df.to_json(EVAL_OUTPUT_FILE, orient=\"records\", indent=2)\n",
    "            print(f\"\\nEvaluation results (raw predictions) saved to: {EVAL_OUTPUT_FILE}\")\n",
    "        except Exception as e_save:\n",
    "             print(f\"\\nERROR saving evaluation results to {EVAL_OUTPUT_FILE}: {e_save}\")\n",
    "    else:\n",
    "         print(\"\\nWarning: EVAL_OUTPUT_FILE not defined, results not saved.\")\n",
    "else:\n",
    "    print(\"No valid evaluation results were generated to save.\")\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\n--- Evaluation Generation & Saving Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905910c9-cf29-4e9f-99b0-34b1416233dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from functools import reduce\n",
    "\n",
    "### Final results compilation\n",
    "print(\"--- Aggregating Evaluation Results ---\")\n",
    "\n",
    "BASE_OUTPUT_DIR = Path(WORKING_DIR)\n",
    "OUTPUT_FOLDER_PREFIX = OUTPUT_DIR_BASE_NAME\n",
    "EVAL_FILENAME = \"chartqa_evaluation_results_comparison.json\"\n",
    "\n",
    "search_pattern = f\"{OUTPUT_FOLDER_PREFIX}/{MODEL_ID}/{EVAL_FILENAME}\"\n",
    "result_files = list(BASE_OUTPUT_DIR.glob(search_pattern))\n",
    "# if not result_files:\n",
    "#     raise FileNotFoundError(\"No evaluation result files found.\")\n",
    "\n",
    "aggregated_dfs = []\n",
    "base_models_added = set()\n",
    "\n",
    "for file_path in result_files:\n",
    "    run_label = file_path.parent.name.replace(f\"{OUTPUT_FOLDER_PREFIX}-\", \"\")\n",
    "    parts = run_label.split('-')\n",
    "    base_model_name = '-'.join(parts[:-1]) if len(parts) >= 2 else \"UnknownBase\"\n",
    "    base_label = f\"{base_model_name}-Original\"\n",
    "\n",
    "    df = pd.read_json(file_path)\n",
    "\n",
    "    columns = ['id', 'question', 'ground_truth'] if not aggregated_dfs else ['id']\n",
    "    rename_dict = {}\n",
    "\n",
    "    if 'predicted_answer_finetuned' in df:\n",
    "        columns.append('predicted_answer_finetuned')\n",
    "        rename_dict['predicted_answer_finetuned'] = f\"Pred_{run_label}\"\n",
    "\n",
    "    if base_model_name not in base_models_added and 'predicted_answer_base' in df:\n",
    "        columns.append('predicted_answer_base')\n",
    "        rename_dict['predicted_answer_base'] = f\"Pred_{base_label}\"\n",
    "        base_models_added.add(base_model_name)\n",
    "\n",
    "    df_subset = df[columns].rename(columns=rename_dict)\n",
    "    aggregated_dfs.append(df_subset)\n",
    "\n",
    "if aggregated_dfs:\n",
    "    final_comparison_df = reduce(lambda left, right: pd.merge(left, right, on='id', how='outer'), aggregated_dfs)\n",
    "    final_comparison_df.ffill(inplace=True)\n",
    "    final_comparison_df.bfill(inplace=True)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No evaluation results found to aggregate.\")\n",
    "    final_comparison_df = pd.DataFrame()\n",
    "\n",
    "\n",
    "print(\"Aggregated Results Preview:\")\n",
    "display(final_comparison_df.head())\n",
    "\n",
    "output_file = BASE_OUTPUT_DIR / OUTPUT_DIR_BASE_NAME / MODEL_ID /f\"{OUTPUT_FOLDER_PREFIX}-ALL_RUNS_COMPARISON_WITH_BASE.csv\"\n",
    "final_comparison_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Aggregated results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10316636-431a-44e3-a2c6-86aa954a1c1b",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12659b6-d9d0-4621-b6af-b584cd60e631",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from word2number import w2n\n",
    "\n",
    "# --- Advanced accuracy metrics -------------------------------\n",
    "\n",
    "YES, NO      = {\"yes\",\"y\",\"true\",\"correct\"}, {\"no\",\"n\",\"false\",\"incorrect\"}\n",
    "TOL, EPS     = 0.05, 1e-9                  # ¬±5¬†%, tiny tolerance for 0\n",
    "PRED_PREFIX  = \"Pred_\"                     # rename if your columns differ\n",
    "\n",
    "# 2.  Helpers -------------------------------------------------------------------\n",
    "def to_scalar(v):\n",
    "    \"\"\"Clean & convert value ‚Üí float | 'yes' | 'no' | None.\"\"\"\n",
    "    if pd.isna(v): return None\n",
    "    s = str(v).strip()\n",
    "    m = re.fullmatch(r\"\\[['\\\"]?(.*?)['\\\"]?\\]\", s)\n",
    "    if m: s = m.group(1)\n",
    "    s = s.lower().replace(\",\",\"\").replace(\"$\",\"\").replace(\"%\",\"\").strip()\n",
    "    if s in YES: return \"yes\"\n",
    "    if s in NO:  return \"no\"\n",
    "    for fn in (float, w2n.word_to_num):\n",
    "        try: return float(fn(s))\n",
    "        except Exception: pass\n",
    "    return None\n",
    "\n",
    "def relaxed(gt, pred):\n",
    "    return abs(gt - pred) <= (abs(gt) * TOL or EPS)\n",
    "\n",
    "# 3.  Basic checks --------------------------------------------------------------\n",
    "if not isinstance(globals().get(\"final_comparison_df\"), pd.DataFrame):\n",
    "    sys.exit(\"  `final_comparison_df` is missing.\")\n",
    "\n",
    "df = final_comparison_df.copy()\n",
    "df[\"GT_proc\"] = df[\"ground_truth\"].map(to_scalar)\n",
    "\n",
    "keep = df[\"GT_proc\"].isin([\"yes\",\"no\"]) | df[\"GT_proc\"].apply(lambda x: isinstance(x,(int,float)))\n",
    "df_filt = df[keep]\n",
    "if df_filt.empty:\n",
    "    sys.exit(\"  No yes/no/numeric ground‚Äëtruth rows after processing.\")\n",
    "\n",
    "# Metrics per run -----------------------------------------------------------\n",
    "results = {}\n",
    "for col in [c for c in df.columns if c.startswith(PRED_PREFIX)]:\n",
    "    proc = f\"{col}_proc\"\n",
    "    df[proc] = df[col].map(to_scalar)\n",
    "\n",
    "    valid   = df[[\"GT_proc\", proc]].dropna()\n",
    "    numeric = valid[valid[\"GT_proc\"].apply(lambda x:isinstance(x,(int,float))) &\n",
    "                    valid[proc].apply(lambda x:isinstance(x,(int,float)))]\n",
    "\n",
    "    em  = 100 * (valid[\"GT_proc\"] == valid[proc]).mean() if not valid.empty else 0\n",
    "    rel = 100 * numeric.apply(lambda r: relaxed(r[\"GT_proc\"], r[proc]), axis=1).mean() if not numeric.empty else 0\n",
    "\n",
    "    run = col.replace(PRED_PREFIX, \"\")\n",
    "    results[run] = {\"EM (%)\": round(em,2),\n",
    "                    \"Relaxed¬†Num¬†(%)\": round(rel,2),\n",
    "                    \"#¬†Numeric\": len(numeric),\n",
    "                    \"#¬†Valid\": len(valid)}\n",
    "\n",
    "    print(f\"{run:<15} EM={em:6.2f}%  Relaxed={rel:6.2f}%  ({len(numeric)} num / {len(valid)} valid)\")\n",
    "\n",
    "# Summary & preview ---------------------------------------------------------\n",
    "summary = pd.DataFrame(results).T.sort_values(\"EM (%)\", ascending=False)\n",
    "display(summary)\n",
    "display(df.head())\n",
    "\n",
    "# Save to CSV ---------------------------------------------------------------\n",
    "BASE_OUTPUT_DIR = Path(globals().get(\"BASE_OUTPUT_DIR\", \"./outputs\"))\n",
    "BASE_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "prefix = globals().get(\"OUTPUT_FOLDER_PREFIX\", \"RUNS\")\n",
    "out_file = BASE_OUTPUT_DIR / OUTPUT_DIR_BASE_NAME / MODEL_ID / f\"{prefix}-ALL_RUNS_COMPARISON_WITH_PROCESSED.csv\"\n",
    "df.to_csv(out_file, index=False)\n",
    "print(\"üíæ  Saved:\", out_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d8c154-8219-4a70-b228-3510867b94e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
