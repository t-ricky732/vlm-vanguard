{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d83b802-887f-4e95-8540-136c17a629df",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4f7260d-cfd1-45e5-aae5-57dd590be0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install  -U -q transformers==4.46.3 trl==0.12.1 datasets bitsandbytes peft accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a005c53-c4f1-4ab2-9cf4-9107fe8d214b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e7633d9-3ddd-4471-81b7-68e89fb32f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c6bfa27-ac0b-41b6-9490-061483edf187",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2883b2-4f66-42c5-948f-3701e089186b",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c90b6c03-783a-46a9-8c4e-16884447bd1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'What is the fast food consumption for the age group 65+?', 'answer': '55', 'image_path': 'chart_llama_dataset/ours/scatter_chart/png/scatter_chart_100examples_52.png', 'chart_type': 'scatter_chart'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "root = Path(\"./chart_llama_dataset\")\n",
    "image_root = root\n",
    "\n",
    "examples = []\n",
    "\n",
    "for json_file in root.glob(\"*_simplified_qa.json\"):\n",
    "    chart_type = json_file.stem.split(\"_chart\")[0] + \"_chart\"\n",
    "    with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        qa_list = json.load(f)\n",
    "\n",
    "    for qa in qa_list:\n",
    "        try:\n",
    "            question = qa[\"conversations\"][0][\"value\"].replace(\"<image>\", \"\").strip()\n",
    "            answer = qa[\"conversations\"][1][\"value\"].strip()\n",
    "            image_path = image_root / qa[\"image\"]  # 相対パスを解決\n",
    "            examples.append({\n",
    "                \"question\": question,\n",
    "                \"answer\": answer,\n",
    "                \"image_path\": str(image_path),\n",
    "                \"chart_type\": chart_type\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"スキップ: {qa.get('id', '')} due to error: {e}\")\n",
    "\n",
    "# Datasetに変換\n",
    "dataset = Dataset.from_list(examples)\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b763a3a-91ea-41fb-bd98-6ccb9e49df9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'What is the fast food consumption for the age group 65+?', 'answer': '55', 'image_path': <PIL.PngImagePlugin.PngImageFile image mode=RGBA size=640x480 at 0x7FCD55DD6BD0>, 'chart_type': 'scatter_chart'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import Image\n",
    "\n",
    "dataset = dataset.cast_column(\"image_path\", Image())\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e2aaf34-197b-4ec9-9ee0-d2e33cb80729",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_split = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "train_dataset = dataset_split[\"train\"]\n",
    "test_dataset = dataset_split[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "880bbc14-3183-46a5-9a31-c98930a5f6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are a helpful assistant that answers questions about charts.\"\n",
    "\n",
    "from PIL import Image as PILImage\n",
    "\n",
    "def format_data(example):\n",
    "    img_info = example[\"image_path\"]\n",
    "\n",
    "    if isinstance(img_info, dict) and \"path\" in img_info:\n",
    "        pil_img = PILImage.open(img_info[\"path\"])\n",
    "    elif isinstance(img_info, PILImage.Image):\n",
    "        pil_img = img_info\n",
    "    else:\n",
    "        raise ValueError(\"Unknown image format:\", img_info)\n",
    "\n",
    "    if pil_img.mode != \"RGB\":\n",
    "        pil_img = pil_img.convert(\"RGB\")\n",
    "\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": [{\"type\": \"text\", \"text\": system_message}],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"image\", \"image\": pil_img},\n",
    "                    {\"type\": \"text\", \"text\": example[\"question\"]},\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": [{\"type\": \"text\", \"text\": example[\"answer\"]}],\n",
    "            },\n",
    "        ]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb264caf-4b04-40fa-b030-fb24dc632449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to dataset\n",
    "train_formatted_dataset = [format_data(sample) for sample in train_dataset]\n",
    "test_formatted_dataset = [format_data(sample) for sample in test_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c548e2d6-fb4a-4aea-bbc5-1440ef64586b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [{'role': 'system',\n",
       "   'content': [{'type': 'text',\n",
       "     'text': 'You are a helpful assistant that answers questions about charts.'}]},\n",
       "  {'role': 'user',\n",
       "   'content': [{'type': 'image',\n",
       "     'image': <PIL.Image.Image image mode=RGB size=640x480>},\n",
       "    {'type': 'text',\n",
       "     'text': 'What is the water pollution level for Europe in 2020?'}]},\n",
       "  {'role': 'assistant', 'content': [{'type': 'text', 'text': '40'}]}]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_formatted_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c66bc8c-361c-4aec-86eb-896928eb1646",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c06dbf58-b181-440b-a317-e86d31128e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-10 10:36:26.795882: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-10 10:36:26.808094: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-04-10 10:36:26.811927: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-10 10:36:26.823542: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Idefics3ForConditionalGeneration, AutoProcessor\n",
    "\n",
    "model_id = \"HuggingFaceTB/SmolVLM-256M-Base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "130425a4-bbf0-4ade-b22b-8b51248c2620",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some kwargs in processor config are unused and will not have any effect: image_seq_len. \n"
     ]
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "# BitsAndBytesConfig int-4 config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = Idefics3ForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=bnb_config,\n",
    "    _attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c35ca9af-9ee7-438d-9ec1-cdab5a6e3e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,067,776 || all params: 259,552,704 || trainable%: 1.1819\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Configure LoRA\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=8,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"down_proj\", \"o_proj\", \"k_proj\", \"q_proj\", \"gate_proj\", \"up_proj\", \"v_proj\"],\n",
    "    use_dora=True,\n",
    "    init_lora_weights=\"gaussian\",\n",
    ")\n",
    "\n",
    "# Apply PEFT model adaptation\n",
    "peft_model = get_peft_model(model, peft_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91592a7e-d982-4efe-a17c-0409db9a99a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig\n",
    "\n",
    "# Configure training arguments using SFTConfig\n",
    "training_args = SFTConfig(\n",
    "    output_dir=\"smolvlm-base-chartllama\",\n",
    "    max_seq_length=1024,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=50,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=25,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=25,\n",
    "    save_total_limit=1,\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    bf16=True,\n",
    "    #push_to_hub=True,\n",
    "    #report_to=\"tensorboard\",\n",
    "    remove_unused_columns=False,\n",
    "    gradient_checkpointing=True,\n",
    "    dataset_text_field=\"\",\n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "697835b3-3e1b-412f-adf1-30e378787930",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image as PILImage\n",
    "\n",
    "MAX_SIZE = 384\n",
    "\n",
    "def resize_if_needed(img, max_size=MAX_SIZE):\n",
    "    w, h = img.size\n",
    "    if max(w, h) > max_size:\n",
    "        scale = max_size / max(w, h)\n",
    "        new_w, new_h = int(w * scale), int(h * scale)\n",
    "        img = img.resize((new_w, new_h), PILImage.LANCZOS)\n",
    "    return img\n",
    "\n",
    "def collate_fn(examples):\n",
    "    texts = [processor.apply_chat_template(ex[\"messages\"], tokenize=False) for ex in examples]\n",
    "\n",
    "    images = []\n",
    "    for ex in examples:\n",
    "        for msg in ex[\"messages\"]:\n",
    "            if msg[\"role\"] == \"user\":\n",
    "                for item in msg[\"content\"]:\n",
    "                    if item[\"type\"] == \"image\":\n",
    "                        img = item[\"image\"]\n",
    "                        if isinstance(img, PILImage.Image):  # 念のため確認\n",
    "                            img = resize_if_needed(img)\n",
    "                            if img.mode != \"RGB\":\n",
    "                                img = img.convert(\"RGB\")\n",
    "                            images.append([img])  # SmolVLM expects list of lists\n",
    "                        break\n",
    "\n",
    "\n",
    "    batch = processor(text=texts, images=images, return_tensors=\"pt\", padding=True, do_resize=False)\n",
    "\n",
    "    labels = batch[\"input_ids\"].clone()\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "    image_token_id = processor.tokenizer.additional_special_tokens_ids[\n",
    "        processor.tokenizer.additional_special_tokens.index(\"<image>\")\n",
    "    ]\n",
    "    labels[labels == image_token_id] = -100\n",
    "\n",
    "    batch[\"labels\"] = labels\n",
    "    return batch\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc0d49b8-49a9-4e9c-bc70-f814ebca4417",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/peft/tuners/tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_formatted_dataset,\n",
    "    eval_dataset=test_formatted_dataset,\n",
    "    data_collator=collate_fn,\n",
    "    peft_config=peft_config,\n",
    "    tokenizer=processor.tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aedb0eb5-9c0f-4891-833a-66232d4ca282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'longest_edge': 2048}\n",
      "{'longest_edge': 512}\n"
     ]
    }
   ],
   "source": [
    "print(processor.image_processor.size) \n",
    "print(processor.image_processor.max_image_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db147ec6-6fcf-43b6-a43a-8412d365bac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\n",
      "The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\n",
      "/opt/conda/lib/python3.11/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='239' max='245' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [239/245 14:03 < 00:21, 0.28 it/s, Epoch 4.86/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>5.677000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.814200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.887200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>1.128400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.059700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.999600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.948600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.934300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.11/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.11/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.11/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.11/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.11/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.11/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.11/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.11/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.11/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "trainer.train()\n",
    "end_time = time.time()\n",
    "print(\"Time taken:\", (end_time - start_time)/60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
